[
  {
    "objectID": "univariate.html",
    "href": "univariate.html",
    "title": "Univariate TS Models (ARIMA/SARIMA)",
    "section": "",
    "text": "On this page, we’re diving into an in-depth exploration of our time series data. We’ll start by examining its stationarity using ACF graphs and the Augmented Dickey-Fuller Test. If our data isn’t stationary, we’ll apply differencing to correct it. Next, we’ll analyze ACF and PACF plots to pinpoint the optimal parameters for our ARIMA model. Once we’ve built the model, we’ll conduct a comprehensive evaluation of the residuals. Additionally, we’ll compare our model with the results from auto.arima() to see how they stack up. Finally, we’ll forecast future values using our model and assess its accuracy against standard benchmark methods, ensuring our analysis is robust and reliable."
  },
  {
    "objectID": "univariate.html#arima-modeling",
    "href": "univariate.html#arima-modeling",
    "title": "Univariate TS Models (ARIMA/SARIMA)",
    "section": "ARIMA Modeling",
    "text": "ARIMA Modeling\n\nACF and PCF Plots\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 emissions\n\n\n\n\nView Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(tidyverse)  \nlibrary(lubridate)   \nlibrary(fpp2)          \nlibrary(zoo)   \nlibrary(forecast)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nprices<-read.csv(\"datasets/eda/Henry_Hub_Natural_Gas_Spot_Price.csv\")\nprices$Date <- as.Date(prices$Month, format = \"%m/%d/%y\")\nprices <- subset(prices, select = -Month)\nnames(prices)[1] <- \"prices\"\nprices <- prices[c(\"Date\", \"prices\")]\n\nprices_ts = ts(prices$prices, start = 2005, end = c(2022,12), frequency =12 )\n\npricesacf <- ggAcf(prices_ts)+ggtitle(\"ACF Plot for Natural Gas Prices (Henry Hub)\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\npricespacf <- ggPacf(prices_ts)+ggtitle(\"PACF Plot for Natural Gas Prices (Henry Hub)\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\ngrid.arrange(pricesacf, pricespacf, nrow=2)\n\n\n\n\n\nACF Plot has significant lags from 1-24. PACF Plot has significant lags at 1. Since we see autocorrelation in the plots, we can conclude this data is not stationary and will require differencing to make it stationary.\n\n\n\n\nView Code\nconsumption<-read.csv(\"datasets/eda/Consumption.csv\")\n# Convert \"Month\" column to Date type\nconsumption$Date <- as.Date(consumption$Month, format = \"%Y-%m-%d\")\n\nconsumption_ts = ts(consumption$Consumption, start = 2005,end = c(2022,12),frequency = 12)\n\nconsumptionacf <- ggAcf(consumption_ts)+ggtitle(\"ACF Plot for Natural Gas Consumption \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \nconsumptionpacf <- ggPacf(consumption_ts)+ggtitle(\"PACF Plot for Natural Gas Consumption \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \ngrid.arrange(consumptionacf, consumptionpacf, nrow=2)\n\n\n\n\n\nACF Plot has significant lags at 1, 2. PACF Plot has significant lags at 1 and 2. Since we see autocorrelation in the plots, we can conclude this data is not stationary and will require differencing to make it stationary.\n\n\n\n\nView Code\nproduction<-read.csv(\"datasets/eda/Production.csv\")\n\n# Convert \"Month\" column to Date type\nproduction$Date <- as.Date(production$Month, format = \"%Y-%m-%d\")\n\nproduction_ts = ts(production$Production, start = 2005,end = c(2022,12),frequency = 12)\n\nproductionacf <- ggAcf(production_ts,100)+ggtitle(\"ACF Plot for Natural Gas Production \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \nproductionpacf <- ggPacf(production_ts,100)+ggtitle(\"PACF Plot for Natural Gas Production \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \ngrid.arrange(productionacf, productionpacf, nrow=2)\n\n\n\n\n\nACF Plot has significant lags from 1 to 55. PACF Plot has significant lags at 1 and 2. Since we see autocorrelation in the plots, we can conclude this data is not stationary and will require differencing to make it stationary.\n\n\n\n\nView Code\nimports<-read.csv(\"datasets/eda/Imports.csv\")\n\n# Convert \"Month\" column to Date type\nimports$Date <- as.Date(imports$Month, format = \"%Y-%m-%d\")\n\nimports_ts = ts(imports$Imports, start = 1997,end = c(2022,12),frequency = 12)\n\nimportsacf <- ggAcf(imports_ts,100)+ggtitle(\"ACF Plot for Natural Gas Imports \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \nimportspacf <- ggPacf(imports_ts,100)+ggtitle(\"PACF Plot for Natural Gas Imports \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \ngrid.arrange(importsacf, importspacf, nrow=2)\n\n\n\n\n\nFrom the visualization above, it looks like there are significant lags at 1-60, with the first lag being the most pronounced. The PACF plot, on the other hand, shows strong correlation at lag 1. Since we see autocorrelation in the plots, we can conclude this data is not stationary and will require differencing to make it stationary.\n\n\n\n\nView Code\nexports<-read.csv(\"datasets/eda/Exports.csv\")\n\n# Convert \"Month\" column to Date type\nexports$Date <- as.Date(exports$Month, format = \"%Y-%m-%d\")\n\nexports_ts = ts(exports$Exports, start = 1997,end = c(2022,12),frequency = 12)\n\nexportsacf <- ggAcf(exports_ts,100)+ggtitle(\"ACF Plot for Natural Gas Exports \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \nexportspacf <- ggPacf(exports_ts,100)+ggtitle(\"PACF Plot for Natural Gas Exports \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \ngrid.arrange(exportsacf, exportspacf, nrow=2)\n\n\n\n\n\nFrom the visualization above, it looks like there are significant lags at 1-60, with the first lag being the most pronounced. The PACF plot, on the other hand, shows strong correlation at lag 1. Since we see autocorrelation in the plots, we can conclude this data is not stationary and will require differencing to make it stationary.\n\n\n\n\nView Code\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\ntickers = c(\"CVX\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2000-01-02\",\n             to = \"2022-12-29\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\ncvx_stk <- data.frame(CVX$CVX.Adjusted)\n\ncvx_stk <- data.frame(cvx_stk,rownames(cvx_stk))\ncolnames(cvx_stk) <- append(tickers,'Dates')\n\ncvx_stk$Dates<-as.Date(cvx_stk$Dates,\"%Y-%m-%d\")\n\ncvx_stk_ts <- ts(cvx_stk$CVX, start = decimal_date(as.Date(\"2000-01-02\")), frequency = 365.25)\n\nstocksacf <- ggAcf(cvx_stk_ts,100)+ggtitle(\"ACF Plot for Chevron Corp Stock \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\nstockspacf <- ggPacf(cvx_stk_ts,100)+ggtitle(\"PACF Plot for Chevron Corp Stock \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\ngrid.arrange(stocksacf, stockspacf, nrow=2)\n\n\n\n\n\nFrom the visualization above, it looks like there are significant lags at 1-100. The PACF plot, on the other hand, shows strong correlation at lag 1. Since we see autocorrelation in the plots, we can conclude this data is not stationary and will require differencing to make it stationary.\n\n\n\n\nView Code\nco2_emissions<-read.csv(\"datasets/eda/co2_emissions.csv\")\n\n# Convert \"Month\" column to Date type\nco2_emissions$Date <- as.Date(co2_emissions$Date, format = \"%Y-%m-%d\")\n\nco2_ts = ts(co2_emissions$co2_value, start = 2000,end = c(2023,11),frequency = 12)\n\ncarbonacf <- ggAcf(co2_ts)+ggtitle(\"ACF Plot for Natural Gas CO2 emission\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \ncarbonpacf <- ggPacf(co2_ts)+ggtitle(\"PACF Plot for Natural Gas CO2 emission\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \ngrid.arrange(carbonacf, carbonpacf, nrow=2)\n\n\n\n\n\nFrom the visualization above, a little autocorrelation at lags 13 in the Autocorrelation Function (ACF) plot is seen. In the Partial Autocorrelation Function (PACF) plot, this close relationship shows up at lag 9, 10, 11, 12 and 13. Seeing this kind of pattern, points to the data being stationary and won’t require further differencing.\n\n\n\n\n\nDifferencing\n\nPricesConsumptionProductionImportsExportsCVX Stocks\n\n\n\n\nView Code\nplot1<-autoplot(diff(prices_ts), main=\"First Difference Plot for Natural Gas Residential Prices\", colour = \"#92c54b\") +theme_bw()\npricesacf <- ggAcf(diff(prices_ts))+ggtitle(\"First Difference ACF Plot \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\npricespacf <- ggPacf(diff(prices_ts))+ggtitle(\"First Difference PACF Plot \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\n# Combine the plots\ncombined_plot <- plot1 / (pricesacf | pricespacf)\n\n# Print the combined plot\nprint(combined_plot)\n\n\n\n\n\nAfter performing First Order Differencing, we can see no significant correlation in the ACF and PACF plots, which makes the data stationary. However, we can see over differencing here.\nHere the parameters are d = 0,1 p = 1-4 (PACF Plot) q = 1-4 (ACF Plot)\n\n\n\n\nView Code\nplot1<-autoplot(diff(consumption_ts), main=\"First Difference Plot for Natural Gas Consumption\", colour = \"#92c54b\") +theme_bw()\nconsumptionacf <- ggAcf(diff(consumption_ts))+ggtitle(\"First Difference ACF Plot \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\nconsumptionpacf <- ggPacf(diff(consumption_ts))+ggtitle(\"First Difference PACF Plot \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\n# Combine the plots\ncombined_plot <- plot1 / (consumptionacf | consumptionpacf)\n\n# Print the combined plot\nprint(combined_plot)\n\n\n\n\n\nAfter performing First Order Differencing, we can see no significant correlation in the ACF and PACF plots, which makes the data stationary.\nHere the parameters are d = 0,1 p = 1,2,3 (PACF Plot) q = 1 (ACF Plot)\n\n\n\n\nView Code\nplot1<-autoplot(diff(production_ts), main=\"First Difference Plot for Natural Gas Production\", colour = \"#92c54b\") +theme_bw()\nproductionacf <- ggAcf(diff(production_ts))+ggtitle(\"First Difference ACF Plot \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\nproductionpacf <- ggPacf(diff(production_ts))+ggtitle(\"First Difference PACF Plot \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\n# Combine the plots\ncombined_plot <- plot1 / (productionacf | productionpacf)\n\n# Print the combined plot\nprint(combined_plot)\n\n\n\n\n\nAfter performing First Order Differencing, we can see no significant correlation in the ACF and PACF plots, which makes the data stationary.\nHere the parameters are d = 0,1 p = 1,2,3,4 (PACF Plot) q = 1 (ACF Plot)\n\n\n\n\nView Code\nplot1<-autoplot(diff(imports_ts), main=\"First Difference Plot for Natural Gas Imports\", colour = \"#92c54b\") +theme_bw()\nimportsacf <- ggAcf(diff(imports_ts))+ggtitle(\"First Difference ACF Plot \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\nimportspacf <- ggPacf(diff(imports_ts))+ggtitle(\"First Difference PACF Plot \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\n# Combine the plots\ncombined_plot <- plot1 / (importsacf | importspacf)\n\n# Print the combined plot\nprint(combined_plot)\n\n\n\n\n\nAfter performing First Order Differencing, we can see no significant correlation in the ACF and PACF plots, which makes the data stationary.\nHere the parameters are d = 0,1 p = 1,2 (PACF Plot) q = 1 (ACF Plot)\n\n\n\n\nView Code\nplot1<-autoplot(diff(exports_ts), main=\"First Difference Plot for Natural Gas Exports\", colour = \"#92c54b\") +theme_bw()\nexportsacf <- ggAcf(diff(exports_ts))+ggtitle(\"First Difference ACF Plot \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\nexportspacf <- ggPacf(diff(exports_ts))+ggtitle(\"First Difference PACF Plot \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\n# Combine the plots\ncombined_plot <- plot1 / (exportsacf | exportspacf)\n\n# Print the combined plot\nprint(combined_plot)\n\n\n\n\n\nAfter performing First Order Differencing, we can see no significant correlation in the ACF and PACF plots, which makes the data stationary.\nHere the parameters are d = 0,1 p = 1 (PACF Plot) q = 1,2 (ACF Plot)\n\n\n\n\nView Code\nplot1<-autoplot(diff(cvx_stk_ts), main=\"First Difference Plot for Chevron Corp Stocks\", colour = \"#92c54b\") +theme_bw()\nstocksacf <- ggAcf(diff(cvx_stk_ts),24)+ggtitle(\"First Difference ACF Plot \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\nstockspacf <- ggPacf(diff(cvx_stk_ts),24)+ggtitle(\"First Difference PACF Plot \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\n# Combine the plots\ncombined_plot <- plot1 / (stocksacf | stockspacf)\n\n# Print the combined plot\nprint(combined_plot)\n\n\n\n\n\nAfter performing First Order Differencing, we can see no significant correlation in the ACF and PACF plots, which makes the data stationary.\nHere the parameters are d = 0,1 p = 1,2 (PACF Plot) q = 1,2 (ACF Plot)\n\n\n\n\n\nAdjusted Dickey-Fuller Test\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 emissions\n\n\n\n\nView Code\nadf.test(diff(prices_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(prices_ts)\nDickey-Fuller = -6.7747, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nView Code\nadf.test(diff(consumption_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(consumption_ts)\nDickey-Fuller = -7.984, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nView Code\nadf.test(diff(production_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(production_ts)\nDickey-Fuller = -7.9263, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nView Code\nadf.test(diff(imports_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(imports_ts)\nDickey-Fuller = -8.4953, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nView Code\nadf.test(diff(exports_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(exports_ts)\nDickey-Fuller = -10.846, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nView Code\nadf.test(diff(cvx_stk_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(cvx_stk_ts)\nDickey-Fuller = -18.023, Lag order = 17, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nView Code\nadf.test(co2_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  co2_ts\nDickey-Fuller = -10.018, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nThe ADF test shows that the p-value less than 0.05, therefore we can conclude that all the data used for this analysis is stationary. And it also validates all the observations above.\n\n\nModel Parameter Selection\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 emissions\n\n\nBest Parameters Selected from the ACF/PACF plot.\nq = 1, 2,3,4\nd = 0, 1\np = 1,2,3,4\n\n\nView Code\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*32),nrow=32) \n\n\nfor (p in 1:4)\n{\n  for(q in 1:4)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(prices_ts,order=c(p,d,q),include.drift=TRUE, method=\"ML\") \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n1\n0\n1\n534.4715\n551.3479\n534.7572\n\n\n1\n1\n1\n525.3043\n538.7868\n525.4947\n\n\n1\n0\n2\n525.6674\n545.9191\n526.0693\n\n\n1\n1\n2\n529.3414\n546.1946\n529.6285\n\n\n1\n0\n3\n525.1855\n548.8125\n525.7240\n\n\n1\n1\n3\n531.1267\n551.3506\n531.5306\n\n\n1\n0\n4\n526.3796\n553.3819\n527.0753\n\n\n1\n1\n4\n524.1385\n547.7329\n524.6795\n\n\n2\n0\n1\n524.5163\n544.7680\n524.9182\n\n\n2\n1\n1\n529.5400\n546.3932\n529.8270\n\n\n2\n0\n2\n526.2553\n549.8822\n526.7937\n\n\n2\n1\n2\n518.2534\n538.4773\n518.6573\n\n\n2\n0\n3\n526.9990\n554.0013\n527.6947\n\n\n2\n1\n3\n519.9304\n543.5249\n520.4715\n\n\n2\n0\n4\n523.9014\n554.2789\n524.7752\n\n\n2\n1\n4\n518.9747\n545.9398\n519.6738\n\n\n3\n0\n1\n526.2301\n549.8570\n526.7685\n\n\n3\n1\n1\n531.5072\n551.7310\n531.9110\n\n\n3\n0\n2\n517.5471\n544.5493\n518.2427\n\n\n3\n1\n2\n519.8723\n543.4667\n520.4133\n\n\n3\n0\n3\n518.3086\n548.6861\n519.1824\n\n\n3\n1\n3\n526.0474\n553.0125\n526.7464\n\n\n3\n0\n4\n522.0558\n555.8086\n523.1290\n\n\n3\n1\n4\n527.9813\n558.3170\n528.8593\n\n\n4\n0\n1\n527.8569\n554.8591\n528.5526\n\n\n4\n1\n1\n529.2224\n552.8169\n529.7635\n\n\n4\n0\n2\n528.5765\n558.9540\n529.4502\n\n\n4\n1\n2\n520.9628\n547.9279\n521.6618\n\n\n4\n0\n3\n519.3728\n553.1256\n520.4460\n\n\n4\n1\n3\n527.0536\n557.3894\n527.9317\n\n\n4\n0\n4\n516.2053\n553.3334\n517.4994\n\n\n4\n1\n4\n518.3734\n552.0798\n519.4518\n\n\n\n\n\nNow, let’s get the lowest AIC, BIC, AICc\n\n\nView Code\ntemp[which.min(temp$AIC),]\n\n\n   p d q      AIC      BIC     AICc\n31 4 0 4 516.2053 553.3334 517.4994\n\n\n\n\nView Code\ntemp[which.min(temp$BIC),]\n\n\n   p d q      AIC      BIC     AICc\n12 2 1 2 518.2534 538.4773 518.6573\n\n\n\n\nView Code\ntemp[which.min(temp$AICc),]\n\n\n   p d q      AIC      BIC     AICc\n31 4 0 4 516.2053 553.3334 517.4994\n\n\nAccording to the above results the Best Models are : ARIMA(4,0,4) and ARIMA(2,1,2) .\n\n\nBest Parameters Selected from the ACF/PACF plot.\nq = 1\nd = 0, 1\np = 1,2,3\n\n\nView Code\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*6),nrow=6) \n\n\nfor (p in 1:3)\n{\n  for(q in 1)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(consumption_ts,order=c(p,d,q),include.drift=TRUE, method=\"ML\") \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n1\n0\n1\n5997.884\n6014.760\n5998.169\n\n\n1\n1\n1\n6010.820\n6024.303\n6011.011\n\n\n2\n0\n1\n5936.317\n5956.569\n5936.719\n\n\n2\n1\n1\n5934.341\n5951.194\n5934.628\n\n\n3\n0\n1\n5951.472\n5975.099\n5952.010\n\n\n3\n1\n1\n5928.213\n5948.436\n5928.616\n\n\n\n\n\nNow, let’s get the lowest AIC, BIC, AICc\n\n\nView Code\ntemp[which.min(temp$AIC),]\n\n\n  p d q      AIC      BIC     AICc\n6 3 1 1 5928.213 5948.436 5928.616\n\n\n\n\nView Code\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n6 3 1 1 5928.213 5948.436 5928.616\n\n\n\n\nView Code\ntemp[which.min(temp$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n6 3 1 1 5928.213 5948.436 5928.616\n\n\nAccording to the above results the Best Model is : ARIMA(3,1,1).\n\n\nBest Parameters Selected from the ACF/PACF plot.\nq = 1\nd = 0, 1\np = 1,2,3,4\n\n\nView Code\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*8),nrow=8) \n\n\nfor (p in 1:4)\n{\n  for(q in 1)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(production_ts,order=c(p,d,q),include.drift=TRUE, method=\"ML\") \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n1\n0\n1\n5607.921\n5624.797\n5608.207\n\n\n1\n1\n1\n5570.284\n5583.766\n5570.474\n\n\n2\n0\n1\n5597.967\n5618.219\n5598.369\n\n\n2\n1\n1\n5571.833\n5588.686\n5572.120\n\n\n3\n0\n1\n5609.138\n5632.765\n5609.677\n\n\n3\n1\n1\n5573.352\n5593.575\n5573.755\n\n\n4\n0\n1\n5600.933\n5627.935\n5601.628\n\n\n4\n1\n1\n5555.680\n5579.275\n5556.221\n\n\n\n\n\nNow, let’s get the lowest AIC, BIC, AICc\n\n\nView Code\ntemp[which.min(temp$AIC),]\n\n\n  p d q     AIC      BIC     AICc\n8 4 1 1 5555.68 5579.275 5556.221\n\n\n\n\nView Code\ntemp[which.min(temp$BIC),]\n\n\n  p d q     AIC      BIC     AICc\n8 4 1 1 5555.68 5579.275 5556.221\n\n\n\n\nView Code\ntemp[which.min(temp$AICc),]\n\n\n  p d q     AIC      BIC     AICc\n8 4 1 1 5555.68 5579.275 5556.221\n\n\nAccording to the above results the Best Model is : ARIMA(4,1,1).\n\n\nBest Parameters Selected from the ACF/PACF plot.\nq = 1\nd = 0, 1\np = 1,2\n\n\nView Code\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*4),nrow=4) \n\n\nfor (p in 1:2)\n{\n  for(q in 1)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(imports_ts,order=c(p,d,q),include.drift=TRUE, method=\"ML\") \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n1\n0\n1\n7186.709\n7205.424\n7186.905\n\n\n1\n1\n1\n7137.596\n7152.555\n7137.727\n\n\n2\n0\n1\n7163.893\n7186.351\n7164.168\n\n\n2\n1\n1\n7137.858\n7156.557\n7138.055\n\n\n\n\n\nNow, let’s get the lowest AIC, BIC, AICc\n\n\nView Code\ntemp[which.min(temp$AIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 7137.596 7152.555 7137.727\n\n\n\n\nView Code\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 7137.596 7152.555 7137.727\n\n\n\n\nView Code\ntemp[which.min(temp$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 7137.596 7152.555 7137.727\n\n\nAccording to the above results the Best Model is : ARIMA(1,1,1).\n\n\nBest Parameters Selected from the ACF/PACF plot.\nq = 1,2\nd = 0, 1\np = 1\n\n\nView Code\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*4),nrow=4) \n\n\nfor (p in 1)\n{\n  for(q in 1:2)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(diff(exports_ts),order=c(p,d,q),include.drift=TRUE, method=\"ML\") \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n1\n0\n1\n7078.048\n7096.747\n7078.245\n\n\n1\n1\n1\n7062.332\n7077.278\n7062.463\n\n\n1\n0\n2\n7080.008\n7102.447\n7080.285\n\n\n1\n1\n2\n7062.534\n7081.217\n7062.731\n\n\n\n\n\nNow, let’s get the lowest AIC, BIC, AICc\n\n\nView Code\ntemp[which.min(temp$AIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 7062.332 7077.278 7062.463\n\n\n\n\nView Code\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 7062.332 7077.278 7062.463\n\n\n\n\nView Code\ntemp[which.min(temp$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 7062.332 7077.278 7062.463\n\n\nAccording to the above results the Best Model is : ARIMA(1,1,1).\n\n\nBest Parameters Selected from the ACF/PACF plot.\nq = 1,2\nd = 0, 1\np = 1,2\n\n\nView Code\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*8),nrow=8) \n\n\nfor (p in 1:2)\n{\n  for(q in 1:2)\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(cvx_stk_ts,order=c(p,d,q),include.drift=TRUE, method=\"ML\") \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n1\n0\n1\n18058.94\n18092.25\n18058.95\n\n\n1\n1\n1\n18047.16\n18073.81\n18047.17\n\n\n1\n0\n2\n18063.98\n18103.96\n18063.99\n\n\n1\n1\n2\n18048.21\n18081.53\n18048.22\n\n\n2\n0\n1\n18067.50\n18107.48\n18067.51\n\n\n2\n1\n1\n18048.25\n18081.57\n18048.26\n\n\n2\n0\n2\n18060.33\n18106.97\n18060.35\n\n\n2\n1\n2\n18050.38\n18090.35\n18050.39\n\n\n\n\n\nNow, let’s get the lowest AIC, BIC, AICc\n\n\nView Code\ntemp[which.min(temp$AIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 18047.16 18073.81 18047.17\n\n\n\n\nView Code\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 18047.16 18073.81 18047.17\n\n\n\n\nView Code\ntemp[which.min(temp$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 18047.16 18073.81 18047.17\n\n\nAccording to the above results the Best Model is : ARIMA(1,1,1).\n\n\nBest Parameters Selected from the ACF/PACF plot.\nq = 13\nd = 0\np = 9, 10, 11, 12, 13\n\n\nView Code\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*45),nrow=45) \n\n\nfor (p in 1:13)\n{\n  for(q in 1:13)\n  {\n    for(d in 0)\n    {\n      \n      if(p-1+d+q-1<=8) #usual threshold\n      {\n        \n        model<- Arima(co2_ts,order=c(p,d,q),include.drift=TRUE, method=\"ML\") \n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n1\n0\n1\n4123.294\n4141.592\n4123.508\n\n\n1\n0\n2\n4119.382\n4141.339\n4119.682\n\n\n1\n0\n3\n4118.255\n4143.871\n4118.657\n\n\n1\n0\n4\n4118.279\n4147.555\n4118.797\n\n\n1\n0\n5\n4119.820\n4152.755\n4120.470\n\n\n1\n0\n6\n4018.293\n4054.887\n4019.090\n\n\n1\n0\n7\n3988.080\n4028.334\n3989.040\n\n\n1\n0\n8\n4016.451\n4060.365\n4017.590\n\n\n1\n0\n9\n3963.506\n4011.079\n3964.839\n\n\n2\n0\n1\n4116.750\n4138.707\n4117.050\n\n\n2\n0\n2\n4033.124\n4058.740\n4033.525\n\n\n2\n0\n3\n3986.191\n4015.467\n3986.709\n\n\n2\n0\n4\n4078.157\n4111.093\n4078.807\n\n\n2\n0\n5\n3924.227\n3960.822\n3925.024\n\n\n2\n0\n6\n4008.551\n4048.806\n4009.511\n\n\n2\n0\n7\n4026.254\n4070.168\n4027.393\n\n\n2\n0\n8\n3973.050\n4020.623\n3974.383\n\n\n3\n0\n1\n4110.960\n4136.576\n4111.361\n\n\n3\n0\n2\n4025.402\n4054.678\n4025.920\n\n\n3\n0\n3\n3982.143\n4015.079\n3982.793\n\n\n3\n0\n4\n4068.003\n4104.598\n4068.800\n\n\n3\n0\n5\n4070.827\n4111.081\n4071.787\n\n\n3\n0\n6\n4000.836\n4044.749\n4001.974\n\n\n3\n0\n7\n3921.485\n3969.058\n3922.818\n\n\n4\n0\n1\n4104.329\n4133.605\n4104.847\n\n\n4\n0\n2\n4014.453\n4047.388\n4015.103\n\n\n4\n0\n3\n4058.114\n4094.709\n4058.911\n\n\n4\n0\n4\n3959.653\n3999.907\n3960.613\n\n\n4\n0\n5\n3943.570\n3987.484\n3944.708\n\n\n4\n0\n6\n3998.277\n4045.850\n3999.611\n\n\n5\n0\n1\n4097.628\n4130.563\n4098.278\n\n\n5\n0\n2\n3997.889\n4034.484\n3998.686\n\n\n5\n0\n3\n3999.315\n4039.570\n4000.275\n\n\n5\n0\n4\n3989.337\n4033.251\n3990.476\n\n\n5\n0\n5\n3975.231\n4022.805\n3976.565\n\n\n6\n0\n1\n4088.320\n4124.914\n4089.117\n\n\n6\n0\n2\n3974.011\n4014.266\n3974.971\n\n\n6\n0\n3\n4045.323\n4089.237\n4046.462\n\n\n6\n0\n4\n3970.586\n4018.159\n3971.919\n\n\n7\n0\n1\n4075.743\n4115.997\n4076.703\n\n\n7\n0\n2\n3947.457\n3991.370\n3948.595\n\n\n7\n0\n3\n3957.330\n4004.903\n3958.663\n\n\n8\n0\n1\n4058.311\n4102.225\n4059.450\n\n\n8\n0\n2\n4028.919\n4076.493\n4030.253\n\n\n9\n0\n1\n4033.149\n4080.722\n4034.483\n\n\n\n\n\nNow, let’s get the lowest AIC, BIC, AICc\n\n\nView Code\ntemp[which.min(temp$AIC),]\n\n\n   p d q      AIC      BIC     AICc\n24 3 0 7 3921.485 3969.058 3922.818\n\n\n\n\nView Code\ntemp[which.min(temp$BIC),]\n\n\n   p d q      AIC      BIC     AICc\n14 2 0 5 3924.227 3960.822 3925.024\n\n\n\n\nView Code\ntemp[which.min(temp$AICc),]\n\n\n   p d q      AIC      BIC     AICc\n24 3 0 7 3921.485 3969.058 3922.818\n\n\nAccording to the above results the First Best Model is : ARIMA(3,0,7) and Next Best Model is : ARIMA(2,0,5).\n\n\n\n\n\nAuto ARIMA\nNow, let’s find the best model using Auto ARIMA and compare the results to the models found above.\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 emissions\n\n\n\n\nView Code\nauto.arima(prices_ts)\n\n\nSeries: prices_ts \nARIMA(1,1,1)(2,0,0)[12] \n\nCoefficients:\n          ar1     ma1     sar1     sar2\n      -0.9618  0.9071  -0.1570  -0.1649\ns.e.   0.0728  0.1003   0.0865   0.0996\n\nsigma^2 = 0.6418:  log likelihood = -255.84\nAIC=521.68   AICc=521.96   BIC=538.53\n\n\nThe superior performance of the ARIMA(1,1,1)(2,0,0)[12] model with drift as compared to the ARIMA(4,0,4) and ARIMA(2,1,2) model could be due to the former’s ability to more effectively capture the seasonality within the dataset. While the ARIMA(4,0,4) and ARIMA(2,1,2) model were decent, it may have not fully addressed the non-stationarity. The dataset likely exhibits strong seasonal patterns that a simple ARIMA model cannot adequately model, hence the need for a SARIMA (Seasonal ARIMA) approach. The SARIMA model includes seasonal differencing, which helps in managing seasonal structures that ARIMA models do not account for.\n\n\n\n\nView Code\nauto.arima(consumption_ts)\n\n\nSeries: consumption_ts \nARIMA(2,0,3)(2,1,2)[12] with drift \n\nCoefficients:\n          ar1     ar2     ma1      ma2      ma3    sar1     sar2     sma1\n      -0.1423  0.8233  0.4830  -0.5788  -0.1284  0.6048  -0.3901  -1.3803\ns.e.   0.1536  0.1483  0.1688   0.2451   0.1228  0.1723   0.0885   0.1878\n        sma2      drift\n      0.5664  4319.2878\ns.e.  0.1680   366.7623\n\nsigma^2 = 1.06e+10:  log likelihood = -2648.06\nAIC=5318.11   AICc=5319.49   BIC=5354.61\n\n\nThe superior performance of the ARIMA(2,0,3)(2,1,2)[12] model with drift as compared to the ARIMA(3,1,1) model could be due to the former’s ability to more effectively capture the seasonality within the dataset. While the ARIMA(3,1,1) model was decent, it may have not fully addressed the non-stationarity. The dataset likely exhibits strong seasonal patterns that a simple ARIMA model cannot adequately model, hence the need for a SARIMA (Seasonal ARIMA) approach. The SARIMA model includes seasonal differencing, which helps in managing seasonal structures that ARIMA models do not account for.\n\n\n\n\nView Code\nauto.arima(production_ts)\n\n\nSeries: production_ts \nARIMA(0,1,1)(1,1,1)[12] \n\nCoefficients:\n          ma1     sar1     sma1\n      -0.2318  -0.1148  -0.6329\ns.e.   0.0806   0.0976   0.0785\n\nsigma^2 = 3.033e+09:  log likelihood = -2506.58\nAIC=5021.16   AICc=5021.36   BIC=5034.41\n\n\nThe superior performance of the ARIMA(0,1,1)(1,1,1)[12] model with drift as compared to the ARIMA(4,1,1) model could be due to the former’s ability to more effectively capture the seasonality within the dataset. While the ARIMA(4,1,1) model was decent, it may have not fully addressed the non-stationarity. The dataset likely exhibits strong seasonal patterns that a simple ARIMA model cannot adequately model, hence the need for a SARIMA (Seasonal ARIMA) approach. The SARIMA model includes seasonal differencing, which helps in managing seasonal structures that ARIMA models do not account for.\n\n\n\n\nView Code\nauto.arima(imports_ts)\n\n\nSeries: imports_ts \nARIMA(0,1,3)(1,1,1)[12] \n\nCoefficients:\n          ma1      ma2      ma3    sar1     sma1\n      -0.3251  -0.0821  -0.1358  0.1119  -0.8805\ns.e.   0.0578   0.0629   0.0633  0.0780   0.0621\n\nsigma^2 = 232480233:  log likelihood = -3309.68\nAIC=6631.35   AICc=6631.64   BIC=6653.55\n\n\nThe superior performance of the ARIMA(0,1,3)(1,1,1)[12] model with drift as compared to the ARIMA(1,1,1) model could be due to the former’s ability to more effectively capture the seasonality within the dataset. While the ARIMA(1,1,1) model was decent, it may have not fully addressed the non-stationarity. The dataset likely exhibits strong seasonal patterns that a simple ARIMA model cannot adequately model, hence the need for a SARIMA (Seasonal ARIMA) approach. The SARIMA model includes seasonal differencing, which helps in managing seasonal structures that ARIMA models do not account for.\n\n\n\n\nView Code\nauto.arima(exports_ts)\n\n\nSeries: exports_ts \nARIMA(0,1,1)(1,0,0)[12] \n\nCoefficients:\n          ma1    sar1\n      -0.1145  0.5996\ns.e.   0.0595  0.0505\n\nsigma^2 = 316782460:  log likelihood = -3486.68\nAIC=6979.36   AICc=6979.44   BIC=6990.58\n\n\nThe superior performance of the ARIMA(0,1,1)(1,0,0)[12] model with drift as compared to the ARIMA(1,1,1) model could be due to the former’s ability to more effectively capture the seasonality within the dataset. While the ARIMA(1,1,1) model was decent, it may have not fully addressed the non-stationarity. The dataset likely exhibits strong seasonal patterns that a simple ARIMA model cannot adequately model, hence the need for a SARIMA (Seasonal ARIMA) approach. The SARIMA model includes seasonal differencing, which helps in managing seasonal structures that ARIMA models do not account for.\n\n\n\n\nView Code\nauto.arima(cvx_stk_ts)\n\n\nSeries: cvx_stk_ts \nARIMA(0,1,2) with drift \n\nCoefficients:\n          ma1     ma2   drift\n      -0.0394  0.0292  0.0262\ns.e.   0.0132  0.0129  0.0150\n\nsigma^2 = 1.325:  log likelihood = -9019.92\nAIC=18047.84   AICc=18047.84   BIC=18074.49\n\n\nThe best model from the step above was ARIMA(1,1,1), while the best model Auto ARIMA gave me is ARIMA(0,1,2). We see that model from the step above has lower AIC, BIC and AICc than auto ARIMA model. Therefore, we’ll do further analysis to choose the best model.\n\n\n\n\nView Code\nauto.arima(co2_ts)\n\n\nSeries: co2_ts \nARIMA(5,0,1) with non-zero mean \n\nCoefficients:\n         ar1      ar2      ar3      ar4      ar5      ma1      mean\n      0.6614  -0.0637  -0.0522  -0.0591  -0.1393  -0.7994  208.6951\ns.e.  0.0680   0.0716   0.0724   0.0723   0.0625   0.0377    5.8139\n\nsigma^2 = 101719:  log likelihood = -2058.67\nAIC=4133.33   AICc=4133.85   BIC=4162.61\n\n\nThe best model from the step above was ARIMA(3,0,7), while the best model Auto ARIMA gave me is ARIMA(5,0,1). We see that model from the step above has lower AIC, BIC and AICc than auto ARIMA model. Therefore, we’ll do further analysis to choose the best model.\n\n\n\n\n\nModel Selection & Fitting\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 emissions\n\n\nThe best models selected from the steps above was ARIMA(4,0,4) and ARIMA(2,1,2)\n\nModel1: ARIMA(4,0,4)Model2: ARIMA(2,1,2)\n\n\n\n\nView Code\nfit1_prices_ts <- Arima(prices_ts, order=c(4,0,4),include.drift = TRUE, method=\"ML\") \nsummary(fit1_prices_ts)\n\n\nSeries: prices_ts \nARIMA(4,0,4) with drift \n\nCoefficients:\n          ar1      ar2     ar3     ar4     ma1     ma2     ma3     ma4\n      -0.0912  -0.0787  0.1295  0.6641  0.9785  1.1009  0.9370  0.1999\ns.e.   0.1294   0.1153  0.1130  0.1103  0.1405  0.0901  0.1283  0.0818\n      intercept    drift\n         6.4281  -0.0171\ns.e.     1.0489   0.0082\n\nsigma^2 = 0.5963:  log likelihood = -247.1\nAIC=516.21   AICc=517.5   BIC=553.33\n\nTraining set error measures:\n                      ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.002023897 0.7541294 0.5065611 -2.23019 11.14678 0.3582668\n                    ACF1\nTraining set 0.008495682\n\n\n\n\n\n\nView Code\nfit2_prices_ts <- Arima(prices_ts, order=c(2,1,2),include.drift = TRUE, method=\"ML\") \nsummary(fit2_prices_ts)\n\n\nSeries: prices_ts \nARIMA(2,1,2) with drift \n\nCoefficients:\n          ar1      ar2      ma1     ma2    drift\n      -0.0683  -0.8735  -0.0422  0.9555  -0.0038\ns.e.   0.0496   0.0630   0.0353  0.0456   0.0526\n\nsigma^2 = 0.6278:  log likelihood = -253.13\nAIC=518.25   AICc=518.66   BIC=538.48\n\nTraining set error measures:\n                        ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.0005168475 0.7812559 0.5140828 -1.026128 10.96199 0.3635865\n                   ACF1\nTraining set 0.03286363\n\n\n\n\n\n\n\nThe best model from the steps above was ARIMA(3,1,1)\n\n\nView Code\nfit_consumption_ts <- Arima(consumption_ts, order=c(3,1,1),include.drift = TRUE) \nsummary(fit_consumption_ts)\n\n\nSeries: consumption_ts \nARIMA(3,1,1) with drift \n\nCoefficients:\n         ar1      ar2      ar3     ma1      drift\n      1.0200  -0.3582  -0.1971  -1.000  4252.9427\ns.e.  0.0681   0.0946   0.0684   0.013   463.5446\n\nsigma^2 = 5.208e+10:  log likelihood = -2958.11\nAIC=5928.21   AICc=5928.62   BIC=5948.44\n\nTraining set error measures:\n                    ME     RMSE      MAE      MPE     MAPE     MASE      ACF1\nTraining set -13524.57 225013.1 172525.9 -1.68721 7.774267 1.475835 0.0159957\n\n\n\n\nThe best model from the steps above was ARIMA(4,1,1)\n\n\nView Code\nfit_production_ts <- Arima(production_ts, order=c(4,1,1),include.drift = TRUE) \nsummary(fit_production_ts)\n\n\nSeries: production_ts \nARIMA(4,1,1) with drift \n\nCoefficients:\n          ar1      ar2      ar3      ar4     ma1     drift\n      -1.3417  -0.9779  -0.6296  -0.3705  0.4865  8237.759\ns.e.   0.1044   0.1221   0.1058   0.0631  0.0969  2251.572\n\nsigma^2 = 9.36e+09:  log likelihood = -2770.84\nAIC=5555.68   AICc=5556.22   BIC=5579.27\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -200.3845 95168.88 66989.42 -0.1899445 2.545294 0.5141142\n                   ACF1\nTraining set 0.02416993\n\n\n\n\nThe best model from the steps above was ARIMA(1,1,1)\n\n\nView Code\nfit_imports_ts <- Arima(imports_ts, order=c(1,1,1),include.drift = TRUE) \nsummary(fit_imports_ts)\n\n\nSeries: imports_ts \nARIMA(1,1,1) with drift \n\nCoefficients:\n         ar1      ma1     drift\n      0.4842  -0.8693   -9.8334\ns.e.  0.0648   0.0312  336.6107\n\nsigma^2 = 533347952:  log likelihood = -3564.8\nAIC=7137.6   AICc=7137.73   BIC=7152.56\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -91.77133 22945.81 18085.21 -0.6031078 6.369912 0.8085767\n                   ACF1\nTraining set 0.03378889\n\n\n\n\nThe best model from the steps above was ARIMA(1,1,1)\n\n\nView Code\nfit_exports_ts <- Arima(exports_ts, order=c(1,1,1),include.drift = TRUE) \nsummary(fit_exports_ts)\n\n\nSeries: exports_ts \nARIMA(1,1,1) with drift \n\nCoefficients:\n          ar1     ma1      drift\n      -0.5499  0.2305  1853.9829\ns.e.   0.1236  0.1427   942.7753\n\nsigma^2 = 442299121:  log likelihood = -3535.47\nAIC=7078.94   AICc=7079.07   BIC=7093.9\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -1.271411 20895.66 12323.27 -4.178759 11.35464 0.4469031\n                     ACF1\nTraining set -0.001459686\n\n\n\n\nThe best model from the steps above were ARIMA(1,1,1) and ARIMA(0,1,2)\n\nModel1: ARIMA(1,1,1)Model2: ARIMA(0,1,2)\n\n\n\n\nView Code\nfit1_cvx_stk_ts <- Arima(cvx_stk_ts, order=c(1,1,1),include.drift = TRUE) \nsummary(fit1_cvx_stk_ts)\n\n\nSeries: cvx_stk_ts \nARIMA(1,1,1) with drift \n\nCoefficients:\n          ar1     ma1   drift\n      -0.5021  0.4579  0.0264\ns.e.   0.1233  0.1264  0.0147\n\nsigma^2 = 1.325:  log likelihood = -9019.58\nAIC=18047.16   AICc=18047.17   BIC=18073.81\n\nTraining set error measures:\n                        ME     RMSE       MAE         MPE     MAPE       MASE\nTraining set -0.0002444368 1.150707 0.6858501 -0.04429528 1.190633 0.05192604\n                    ACF1\nTraining set 0.004498203\n\n\n\n\n\n\nView Code\nfit2_cvx_stk_ts <- Arima(cvx_stk_ts, order=c(0,1,2),include.drift = TRUE) \nsummary(fit2_cvx_stk_ts)\n\n\nSeries: cvx_stk_ts \nARIMA(0,1,2) with drift \n\nCoefficients:\n          ma1     ma2   drift\n      -0.0394  0.0292  0.0262\ns.e.   0.0132  0.0129  0.0150\n\nsigma^2 = 1.325:  log likelihood = -9019.92\nAIC=18047.84   AICc=18047.84   BIC=18074.49\n\nTraining set error measures:\n                       ME     RMSE       MAE         MPE    MAPE       MASE\nTraining set 1.805754e-05 1.150774 0.6860782 -0.04283954 1.19083 0.05194331\n                      ACF1\nTraining set -0.0003619028\n\n\n\n\n\n\n\nThe best model from the steps above were ARIMA(3,0,7) and ARIMA(5,0,1)\n\nModel1: ARIMA(3,0,7)Model2: ARIMA(5,0,1)\n\n\n\n\nView Code\nfit1_co2_ts <- Arima(co2_ts, order=c(3,0,7),include.drift = TRUE, method=\"ML\") \nsummary(fit1_co2_ts)\n\n\nSeries: co2_ts \nARIMA(3,0,7) with drift \n\nCoefficients:\n          ar1      ar2      ar3     ma1     ma2     ma3      ma4      ma5\n      -0.4724  -0.8273  -0.7126  0.3851  0.9296  0.6374  -0.4171  -0.7718\ns.e.   0.0632   0.0149   0.0632  0.0683  0.0472  0.0628   0.0616   0.0553\n          ma6      ma7  intercept   drift\n      -0.2811  -0.9555   163.1111  0.3103\ns.e.   0.0692   0.0270     5.1354  0.0317\n\nsigma^2 = 43376:  log likelihood = -1947.74\nAIC=3921.48   AICc=3922.82   BIC=3969.06\n\nTraining set error measures:\n                   ME    RMSE      MAE      MPE     MAPE      MASE       ACF1\nTraining set 1.142359 203.868 141.2986 -38.8954 101.0743 0.7055152 -0.0175251\n\n\n\n\n\n\nView Code\nfit2_co2_ts <- Arima(co2_ts, order=c(5,0,1),include.drift = TRUE, method=\"ML\") \nsummary(fit2_co2_ts)\n\n\nSeries: co2_ts \nARIMA(5,0,1) with drift \n\nCoefficients:\n         ar1      ar2      ar3      ar4      ar5      ma1  intercept   drift\n      0.6507  -0.0685  -0.0586  -0.0642  -0.1826  -0.9140   163.9672  0.3071\ns.e.  0.0605   0.0706   0.0715   0.0714   0.0609   0.0205     4.6257  0.0284\n\nsigma^2 = 89155:  log likelihood = -2039.81\nAIC=4097.63   AICc=4098.28   BIC=4130.56\n\nTraining set error measures:\n                     ME     RMSE      MAE       MPE    MAPE      MASE\nTraining set 0.01146658 294.3975 186.3157 -54.51261 113.047 0.9302892\n                    ACF1\nTraining set -0.03406336\n\n\n\n\n\n\n\n\n\n\nModel Diagnostics\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 emissions\n\n\n\nModel1: ARIMA(4,0,4)Model2: ARIMA(2,1,2)\n\n\n\n\nView Code\nmodel_output_p1 <- capture.output(sarima(diff(prices_ts),4,0,4))\n\n\n\n\n\n\n\nView Code\ncat(model_output_p1[213:228], model_output_p1[length(model_output_p1)], sep = \"\\n\") \n\n\nCoefficients: \n      Estimate     SE  t.value p.value\nar1    -0.8740 0.2420  -3.6107  0.0004\nar2    -0.3581 0.1188  -3.0141  0.0029\nar3    -0.9334 0.0842 -11.0797  0.0000\nar4    -0.6375 0.2451  -2.6014  0.0100\nma1     0.8473 0.2466   3.4368  0.0007\nma2     0.4485 0.1122   3.9964  0.0001\nma3     1.0546 0.0862  12.2306  0.0000\nma4     0.7297 0.2775   2.6301  0.0092\nxmean  -0.0019 0.0574  -0.0329  0.9738\n\nsigma^2 estimated as 0.6155658 on 206 degrees of freedom \n \nAIC = 2.461982  AICc = 2.466066  BIC = 2.618756 \n \n \n\n\n\n\n\n\nView Code\nmodel_output_p2 <- capture.output(sarima(diff(prices_ts),2,1,2))\n\n\n\n\n\n\n\nView Code\ncat(model_output_p2[79:90], model_output_p2[length(model_output_p2)], sep = \"\\n\") \n\n\nCoefficients: \n         Estimate     SE t.value p.value\nar1        0.0886 0.5553  0.1595  0.8734\nar2        0.0748 0.0768  0.9734  0.3315\nma1       -1.1562 0.5536 -2.0886  0.0380\nma2        0.1562 0.5534  0.2823  0.7780\nconstant   0.0003 0.0009  0.3732  0.7094\n\nsigma^2 estimated as 0.6587358 on 209 degrees of freedom \n \nAIC = 2.501603  AICc = 2.502951  BIC = 2.595976 \n \n \n\n\n\n\n\nThe standard residual plot for both the models looks promising, indicating a relatively stable mean and variance, although there are noticeable spikes. The ACF plot for model 1 reveals no correlation, suggesting that the model 1 has captured all the dependencies in the data, leaving no degree of correlation unexplained. The Q-Q plot for both the models suggests that the residuals are approximately normally distributed. However, the Ljung-Box test produces most of the values under the 0.05 significance level for both the models, implying the presence of significant correlations within the residuals. This indicates that a SARIMA model, which can better account for seasonality, may provide a more accurate fit for the data. However, the AIC and AICc values for model 1 i.e. ARIMA(4,0,4) is better than model 2 i.e. ARIMA(2,1,2). Hence, ARIMA(4,0,4) is the best model.\n\n\n\n\nView Code\nmodel_output_c1 <- capture.output(sarima(consumption_ts,3,1,1))\n\n\n\n\n\n\n\nView Code\ncat(model_output_c1[50:64], model_output_c1[length(model_output_c1)], sep = \"\\n\") \n\n\nconverged\n<><><><><><><><><><><><><><>\n \nCoefficients: \n          Estimate       SE  t.value p.value\nar1         1.0200   0.0681  14.9801  0.0000\nar2        -0.3582   0.0946  -3.7886  0.0002\nar3        -0.1971   0.0684  -2.8828  0.0044\nma1        -1.0000   0.0130 -77.0750  0.0000\nconstant 4252.9427 463.5446   9.1748  0.0000\n\nsigma^2 estimated as 50866377554 on 210 degrees of freedom \n \nAIC = 27.57308  AICc = 27.57442  BIC = 27.66715 \n \n \n\n\nThe standard residual plot looks promising, indicating a relatively stable mean and variance, although there are noticeable spikes. The ACF plot reveals minimal correlation, suggesting that the current model has not captured all the dependencies in the data, leaving some degree of correlation unexplained. The Q-Q plot suggests that the residuals are approximately normally distributed. However, the Ljung-Box test produces values under the 0.05 significance level, implying the presence of significant correlations within the residuals. This indicates that a SARIMA model, which can better account for seasonality, may provide a more accurate fit for the data.\n\n\n\n\nView Code\nmodel_output_prod1 <- capture.output(sarima(production_ts,4,1,1))\n\n\n\n\n\n\n\nView Code\ncat(model_output_prod1[39:54], model_output_prod1[length(model_output_prod1)], sep = \"\\n\") \n\n\nconverged\n<><><><><><><><><><><><><><>\n \nCoefficients: \n          Estimate        SE  t.value p.value\nar1        -1.3417    0.1044 -12.8463   0e+00\nar2        -0.9779    0.1221  -8.0099   0e+00\nar3        -0.6296    0.1058  -5.9510   0e+00\nar4        -0.3705    0.0631  -5.8766   0e+00\nma1         0.4865    0.0969   5.0230   0e+00\nconstant 8237.7592 2251.5716   3.6587   3e-04\n\nsigma^2 estimated as 9099221908 on 209 degrees of freedom \n \nAIC = 25.84037  AICc = 25.84225  BIC = 25.95012 \n \n \n\n\nThe standard residual plot looks promising, indicating a relatively stable mean and variance, although there are noticeable spikes. The ACF plot reveals minimal correlation, suggesting that the current model has not captured all the dependencies in the data, leaving some degree of correlation unexplained. The Q-Q plot suggests that the residuals are approximately normally distributed. However, the Ljung-Box test produces most of the values under the 0.05 significance level, implying the presence of significant correlations within the residuals. This indicates that a SARIMA model, which can better account for seasonality, may provide a more accurate fit for the data.\n\n\n\n\nView Code\nmodel_output_i1 <- capture.output(sarima(imports_ts,1,1,1))\n\n\n\n\n\n\n\nView Code\ncat(model_output_i1[28:39], model_output_i1[length(model_output_i1)], sep = \"\\n\") \n\n\nconverged\n<><><><><><><><><><><><><><>\n \nCoefficients: \n         Estimate       SE  t.value p.value\nar1        0.4842   0.0648   7.4731  0.0000\nma1       -0.8693   0.0312 -27.8550  0.0000\nconstant  -9.8334 336.6107  -0.0292  0.9767\n\nsigma^2 estimated as 528202867 on 308 degrees of freedom \n \nAIC = 22.95047  AICc = 22.95072  BIC = 22.99857 \n \n\n\nThe standard residual plot looks promising, indicating a relatively stable mean and variance, although there are noticeable spikes. The ACF plot reveals minimal correlation, suggesting that the current model has not captured all the dependencies in the data, leaving some degree of correlation unexplained. The Q-Q plot suggests that the residuals are approximately normally distributed. However, the Ljung-Box test produces values under the 0.05 significance level, implying the presence of significant correlations within the residuals. This indicates that a SARIMA model, which can better account for seasonality, may provide a more accurate fit for the data.\n\n\n\n\nView Code\nmodel_output_e1 <- capture.output(sarima(exports_ts,1,1,1))\n\n\n\n\n\n\n\nView Code\ncat(model_output_e1[22:33], model_output_e1[length(model_output_e1)], sep = \"\\n\") \n\n\nconverged\n<><><><><><><><><><><><><><>\n \nCoefficients: \n          Estimate       SE t.value p.value\nar1        -0.5499   0.1236 -4.4483  0.0000\nma1         0.2305   0.1427  1.6150  0.1073\nconstant 1853.9829 942.7753  1.9665  0.0501\n\nsigma^2 estimated as 438032570 on 308 degrees of freedom \n \nAIC = 22.76187  AICc = 22.76212  BIC = 22.80997 \n \n\n\nThe standard residual plot looks promising, indicating a relatively stable mean and variance, although there are noticeable spikes. The ACF plot reveals minimal correlation, suggesting that the current model has not captured all the dependencies in the data, leaving some degree of correlation unexplained. The Q-Q plot suggests that the residuals are approximately normally distributed. However, the Ljung-Box test produces most of the values under the 0.05 significance level, implying the presence of significant correlations within the residuals. This indicates that a SARIMA model, which can better account for seasonality, may provide a more accurate fit for the data.\n\n\n\nModel1: ARIMA(1,1,1)Model2: ARIMA(0,1,2)\n\n\n\n\nView Code\nmodel_output_s1 <- capture.output(sarima(cvx_stk_ts,1,1,1))\n\n\n\n\n\n\n\nView Code\ncat(model_output_s1[21:33], model_output_s1[length(model_output_s1)], sep = \"\\n\") \n\n\nconverged\n<><><><><><><><><><><><><><>\n \nCoefficients: \n         Estimate     SE t.value p.value\nar1       -0.5021 0.1233 -4.0710  0.0000\nma1        0.4579 0.1264  3.6235  0.0003\nconstant   0.0264 0.0147  1.7983  0.0722\n\nsigma^2 estimated as 1.324355 on 5781 degrees of freedom \n \nAIC = 3.120187  AICc = 3.120187  BIC = 3.124794 \n \n \n\n\n\n\n\n\nView Code\nmodel_output_s2 <- capture.output(sarima(cvx_stk_ts,0,1,2))\n\n\n\n\n\n\n\nView Code\ncat(model_output_s2[15:24], model_output_s2[length(model_output_s2)], sep = \"\\n\") \n\n\nCoefficients: \n         Estimate     SE t.value p.value\nma1       -0.0394 0.0132 -2.9991  0.0027\nma2        0.0292 0.0129  2.2615  0.0238\nconstant   0.0262 0.0150  1.7468  0.0807\n\nsigma^2 estimated as 1.32451 on 5781 degrees of freedom \n \nAIC = 3.120303  AICc = 3.120304  BIC = 3.124911 \n \n \n\n\n\n\n\nFor both the models, the Standard Residual Plot looks promising, indicating stationarity with consistent mean and variance across the series. The Autocorrelation Function (ACF) plot presents minimal correlation, suggesting the model has captured the majority of the information in the data, leaving behind what appears to be merely white noise. This is a sign of an appropriate model fit. The Quantile-Quantile (Q-Q) Plot approximates normal distribution, reinforcing the model’s adequacy. Lastly, the Ljung-Box test yields some values exceeding the 0.05 significance level, implying that any remaining correlations in the residuals are not statistically significant, which further supports the model’s suitability. However, the AIC, BIC and AICc values for model 1 i.e. ARIMA(1,1,1) is better than model 2 i.e. ARIMA(0,1,2). Hence, ARIMA(1,1,1) is the best model.\n\n\n\nModel1: ARIMA(3,0,7)Model2: ARIMA(5,0,1)\n\n\n\n\nView Code\nmodel_output_ce1 <- capture.output(sarima(co2_ts,3,0,7))\n\n\n\n\n\n\n\nView Code\ncat(model_output_ce1[57:77], model_output_ce1[length(model_output_ce1)], sep = \"\\n\") \n\n\nconverged\n<><><><><><><><><><><><><><>\n \nCoefficients: \n      Estimate     SE  t.value p.value\nar1     0.3492 0.0733   4.7602  0.0000\nar2    -0.0203 0.0802  -0.2535  0.8001\nar3    -0.1420 0.0708  -2.0042  0.0460\nma1    -0.3656 0.0473  -7.7354  0.0000\nma2    -0.0197 0.0475  -0.4137  0.6794\nma3     0.0865 0.0456   1.8972  0.0588\nma4     0.0054 0.0396   0.1365  0.8915\nma5     0.1142 0.0410   2.7832  0.0058\nma6     0.5095 0.0382  13.3256  0.0000\nma7    -0.9066 0.0291 -31.1202  0.0000\nxmean 209.2478 7.5749  27.6240  0.0000\n\nsigma^2 estimated as 57661.57 on 276 degrees of freedom \n \nAIC = 13.96322  AICc = 13.96657  BIC = 14.11623 \n \n \n\n\n\n\n\n\nView Code\nmodel_output_ce2 <- capture.output(sarima(co2_ts,5,0,1))\n\n\n\n\n\n\n\nView Code\ncat(model_output_ce2[36:49], model_output_ce2[length(model_output_ce2)], sep = \"\\n\") \n\n\nCoefficients: \n      Estimate     SE  t.value p.value\nar1     0.6614 0.0680   9.7219  0.0000\nar2    -0.0637 0.0716  -0.8907  0.3739\nar3    -0.0522 0.0724  -0.7208  0.4717\nar4    -0.0591 0.0723  -0.8172  0.4145\nar5    -0.1393 0.0625  -2.2292  0.0266\nma1    -0.7994 0.0377 -21.2288  0.0000\nxmean 208.6951 5.8139  35.8962  0.0000\n\nsigma^2 estimated as 99237.66 on 280 degrees of freedom \n \nAIC = 14.40185  AICc = 14.40325  BIC = 14.50386 \n \n \n\n\n\n\n\nFor both the models, the Standard Residual Plot looks promising, indicating stationarity with consistent mean and variance across the series. The Autocorrelation Function (ACF) plot presents minimal correlation, suggesting the model has captured the majority of the information in the data, leaving behind what appears to be merely white noise. This is a sign of an appropriate model fit. The Quantile-Quantile (Q-Q) Plot approximates near-normal distribution, reinforcing the model’s adequacy. Lastly, the Ljung-Box test yields some values exceeding the 0.05 significance level for model2, implying that any remaining correlations in the residuals are not statistically significant, which further supports the model’s suitability. However, the AIC, BIC and AICc values for model 1 i.e. ARIMA(3,0,7) is better than model 2 i.e. ARIMA(5,0,1). Hence, ARIMA(3,0,7) is the best model.\n\n\n\n\n\nFinal Model Fitting\nHere I’ll fit the best model obtained from the above analysis\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 emissions\n\n\n\n\nView Code\nfit_prices <- Arima(prices_ts, order=c(4, 0, 4))\nsummary(fit_prices)\n\n\nSeries: prices_ts \nARIMA(4,0,4) with non-zero mean \n\nCoefficients:\n          ar1      ar2     ar3     ar4     ma1     ma2     ma3     ma4    mean\n      -0.0636  -0.0499  0.1538  0.6928  0.9653  1.0853  0.9244  0.1833  4.6011\ns.e.   0.1285   0.1163  0.1135  0.1107  0.1406  0.0898  0.1279  0.0811  0.7610\n\nsigma^2 = 0.601:  log likelihood = -248.62\nAIC=517.24   AICc=518.31   BIC=550.99\n\nTraining set error measures:\n                       ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.009279726 0.7589394 0.5121613 -2.632714 11.30023 0.3622275\n                    ACF1\nTraining set 0.005008433\n\n\nEquation \\(x_t= -0.0636x_{t-1}-0.0499x_{t-2}+0.1538x_{t-3}+0.6928x_{t-4}+ w_t+ 0.9653w_{t-1}+1.0853w_{t-2} +0.9244w_{t-3} +0.1833w_{t-4}\\)\n\n\n\n\nView Code\nfit_consumption <- Arima(consumption_ts, order=c(3, 1, 1))\nsummary(fit_consumption)\n\n\nSeries: consumption_ts \nARIMA(3,1,1) \n\nCoefficients:\n         ar1      ar2      ar3      ma1\n      1.0319  -0.3683  -0.1805  -0.9426\ns.e.  0.0695   0.0954   0.0694   0.0172\n\nsigma^2 = 5.56e+10:  log likelihood = -2963.99\nAIC=5937.97   AICc=5938.26   BIC=5954.83\n\nTraining set error measures:\n                   ME     RMSE    MAE       MPE    MAPE     MASE        ACF1\nTraining set 26124.46 233047.6 179130 0.1997962 7.93188 1.532328 0.007788663\n\n\nEquation \\(x_t= 1.0319x_{t-1}-0.3683x_{t-2}-0.1805x_{t-3}+ w_t -0.9426 w_{t-1}\\)\n\n\n\n\nView Code\nfit_production <- Arima(production_ts, order=c(4, 1, 1))\nsummary(fit_production)\n\n\nSeries: production_ts \nARIMA(4,1,1) \n\nCoefficients:\n          ar1      ar2      ar3      ar4     ma1\n      -1.3277  -0.9170  -0.5585  -0.3339  0.5135\ns.e.   0.1039   0.1217   0.1066   0.0642  0.0928\n\nsigma^2 = 9.867e+09:  log likelihood = -2776.95\nAIC=5565.9   AICc=5566.31   BIC=5586.13\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE     MASE        ACF1\nTraining set 22028.81 97945.77 73544.56 0.6846512 2.767895 0.564422 -0.01775295\n\n\nEquation \\(x_t= -1.3277x_{t-1}-0.9170x_{t-2}-0.5585x_{t-3}-0.3339x_{t-4}+ w_t + 0.5135 w_{t-1}\\)\n\n\n\n\nView Code\nfit_imports <- Arima(imports_ts, order=c(1, 1, 1))\nsummary(fit_imports)\n\n\nSeries: imports_ts \nARIMA(1,1,1) \n\nCoefficients:\n         ar1      ma1\n      0.4841  -0.8693\ns.e.  0.0648   0.0312\n\nsigma^2 = 531623633:  log likelihood = -3564.8\nAIC=7135.6   AICc=7135.68   BIC=7146.82\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -129.5084 22945.85 18087.87 -0.6167255 6.371787 0.8086957\n                   ACF1\nTraining set 0.03376946\n\n\nEquation \\(x_t= 0.4841x_{t-1} + w_t -0.8693 w_{t-1}\\)\n\n\n\n\nView Code\nfit_exports <- Arima(exports_ts, order=c(1, 1, 1))\nsummary(fit_exports)\n\n\nSeries: exports_ts \nARIMA(1,1,1) \n\nCoefficients:\n          ar1     ma1\n      -0.5604  0.2505\ns.e.   0.1204  0.1387\n\nsigma^2 = 446310196:  log likelihood = -3537.38\nAIC=7080.75   AICc=7080.83   BIC=7091.97\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 2304.257 21024.24 12413.88 0.3732122 10.45859 0.4501893\n                    ACF1\nTraining set -0.01084399\n\n\nEquation \\(x_t= -0.5604x_{t-1} + w_t +0.2505w_{t-1}\\)\n\n\n\n\nView Code\nfit_stocks <- Arima(cvx_stk_ts, order=c(1, 1, 1))\nsummary(fit_stocks)\n\n\nSeries: cvx_stk_ts \nARIMA(1,1,1) \n\nCoefficients:\n          ar1     ma1\n      -0.5014  0.4575\ns.e.   0.1239  0.1269\n\nsigma^2 = 1.326:  log likelihood = -9021.17\nAIC=18048.33   AICc=18048.34   BIC=18068.32\n\nTraining set error measures:\n                    ME     RMSE       MAE        MPE     MAPE       MASE\nTraining set 0.0269576 1.151023 0.6867105 0.02465471 1.191267 0.05199118\n                    ACF1\nTraining set 0.004223936\n\n\nEquation \\(x_t= -0.5014x_{t-1} + w_t +0.4575w_{t-1}\\)\n\n\n\n\nView Code\nfit_co2emissions <- Arima(co2_ts, order=c(3, 0, 7))\nsummary(fit_co2emissions)\n\n\nSeries: co2_ts \nARIMA(3,0,7) with non-zero mean \n\nCoefficients:\n         ar1      ar2      ar3      ma1      ma2     ma3     ma4     ma5\n      0.3492  -0.0203  -0.1420  -0.3656  -0.0197  0.0865  0.0054  0.1142\ns.e.  0.0733   0.0802   0.0708   0.0473   0.0475  0.0456  0.0396  0.0410\n         ma6      ma7      mean\n      0.5095  -0.9066  209.2478\ns.e.  0.0382   0.0291    7.5749\n\nsigma^2 = 59960:  log likelihood = -1991.72\nAIC=4007.44   AICc=4008.58   BIC=4051.36\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -2.504469 240.1282 153.9421 -54.13229 110.8541 0.7686453\n                     ACF1\nTraining set -0.006218897\n\n\nEquation \\(x_t= 0.3492x_{t-1}-0.0203x_{t-2}-0.1420x_{t-3} + w_t -0.3656w_{t-1} -0.0197w_{t-2} +0.0865w_{t-3}+ 0.0054w_{t-4}+ 0.1142w_{t-5}+ 0.5095w_{t-6}-0.9066w_{t-7}\\)\n\n\n\n\n\nForecasting\nNext, we forecast the values and plot the forecasts using the best model selected above.\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 emissions\n\n\n\n\nView Code\npred_prices_ts=forecast(fit_prices,15)\nautoplot(pred_prices_ts,fcol = '#92c54b') + theme_bw() \n\n\n\n\n\nThe forecasted trend decline downwards. The prediction interval widens as time progresses, which is typical in time series forecasts as uncertainty increases with longer forecast horizons. The model seems to expect the continuation of the cyclical behavior, but with an downward trend in the mean of the series. However, the forecast does not show any sharp increases or decreases, indicating that the model does not anticipate any abrupt changes in the pattern of prices.\n\n\n\n\nView Code\npred_consumption_ts=forecast(fit_consumption,15)\nautoplot(pred_consumption_ts,fcol = '#92c54b') + theme_bw() \n\n\n\n\n\nThe forecast plot illustrates an initial rise in natural gas consumption, which swiftly dips and then climbs once more. While the model captures the cyclical nature of the demand, it seems to underestimate the magnitude of these seasonal swings. The anticipated peaks in consumption are comparable to or exceed previous levels, indicating the model’s limited ability to fully account for the strength of these seasonal variations.\n\n\n\n\nView Code\npred_production_ts=forecast(fit_production,15)\nautoplot(pred_production_ts,fcol = '#92c54b') + theme_bw() \n\n\n\n\n\nThe forecast suggests a continued upward trend in production, consistent with the observed increase over the historical period. This aligns with the “with drift” aspect of the model, which typically represents a steady average change over time, rather than a static mean level. The anticipated peaks in production are comparable to or exceed previous levels, indicating the model’s limited ability to fully account for the strength of these seasonal variations.\n\n\n\n\nView Code\npred_imports_ts=forecast(fit_imports,15)\nautoplot(pred_imports_ts,fcol = '#92c54b') + theme_bw() \n\n\n\n\n\nThe forecast plot predicts a falling trajectory in import data over the forthcoming fifteen-year span. The shaded bands delineate the 95% confidence interval, encapsulating the range within which future data points are expected to fall with a high degree of certainty. However, the forecasted line seems overly smooth, lacking any evident fluctuations. This suggests that the model may not adequately account for potential variability and could be oversimplifying the complex nature of the underlying processes influencing imports.\n\n\n\n\nView Code\npred_exports_ts=forecast(fit_exports,15)\nautoplot(pred_exports_ts,fcol = '#92c54b') + theme_bw() \n\n\n\n\n\nThe forecast plot predicts a rising trajectory in export data over the forthcoming fifteen-year span. The shaded bands delineate the 95% confidence interval, encapsulating the range within which future data points are expected to fall with a high degree of certainty. However, the forecasted line seems overly smooth, lacking any evident fluctuations. This suggests that the model may not adequately account for potential variability and could be oversimplifying the complex nature of the underlying processes influencing imports.\n\n\n\n\nView Code\npred_cvx_stk_ts=forecast(fit_stocks,365)\nautoplot(pred_cvx_stk_ts,fcol = '#92c54b') + theme_bw() \n\n\n\n\n\nThe forecast plot predicts a rising trajectory in CVX Stocks data over the forthcoming fifteen-year span. The shaded bands delineate the 95% confidence interval, encapsulating the range within which future data points are expected to fall with a high degree of certainty. However, the forecasted line seems overly smooth, lacking any evident fluctuations. This suggests that the model may not adequately account for potential variability and could be oversimplifying the complex nature of the underlying processes influencing imports.\n\n\n\n\nView Code\npred_co2_ts=forecast(fit_co2emissions,15)\nautoplot(pred_co2_ts,fcol = '#92c54b') + theme_bw() \n\n\n\n\n\nThe forecast plot projects an ongoing pattern consistent with the historical seasonal trend in CO2 emissions data. While the model replicates the seasonal nature of the post-2020 data, the predicted variations seem to be of a lesser magnitude. Hence, the model appears to underrepresent the full intensity of the seasonal fluctuations observed in the emissions data.\n\n\n\n\n\nARIMA vs. benchmark methods.\nThis section plots and compares the performance of ARIMA model with the benchmark methods\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 emissions\n\n\n\n\nView Code\nautoplot(prices_ts) +\n  autolayer(meanf(prices_ts, h=15),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(prices_ts, h=15),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(prices_ts, drift=TRUE, h=15),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(pred_prices_ts, \n            series=\"ARIMA\",PI=FALSE) + ggtitle(\"Natural Gas Henry Hub Prices ARIMA vs. Benchmarks\")+\n  guides(colour=guide_legend(title=\"Forecast\")) +theme_bw()\n\n\n\n\n\nComparing Accuracy Metrics\n\n\nView Code\noutput1 <- accuracy(pred_prices_ts)\noutput2 <- accuracy(meanf(prices_ts, h=15))\noutput3 <- accuracy(naive(prices_ts, h=15))\noutput4 <- accuracy(rwf(prices_ts, drift=TRUE, h=15))\n\noutput_list <- list(output1, output2, output3, output4)\n\n# Put into df\noutput_df <- do.call(rbind, output_list)\n\n# Set row names for the dataframe\nrow.names(output_df) <- c(\"ARIMA\", \"Meanf\", \"Naive\", \"Drift\")\n\noutput_df\n\n\n                 ME      RMSE       MAE        MPE     MAPE      MASE\nARIMA -9.279726e-03 0.7589394 0.5121613  -2.632714 11.30023 0.3622275\nMeanf  1.517356e-16 2.3123213 1.7996103 -23.725893 46.03433 1.2727795\nNaive -2.883721e-03 0.8138242 0.5097674  -1.144999 10.63560 0.3605345\nDrift -2.895201e-17 0.8138191 0.5096467  -1.065391 10.62714 0.3604491\n              ACF1\nARIMA  0.005008433\nMeanf  0.936691654\nNaive -0.076051915\nDrift -0.076051915\n\n\nFrom the plot it is seen that the ARIMA model forecast outperforms other benchmark methods.\nFrom the accuracy metrics, it is seen that the ARIMA model has the least RMSE and MAE values. This indicates that the ARIMA model outperforms the benchmark models and validates the above findings from the plot.\n\n\n\n\nView Code\nautoplot(consumption_ts) +\n  autolayer(meanf(consumption_ts, h=15),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(consumption_ts, h=15),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(consumption_ts, drift=TRUE, h=15),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(pred_consumption_ts, \n            series=\"ARIMA\",PI=FALSE) + ggtitle(\"Natural Gas Consumption ARIMA vs. Benchmarks\")+\n  guides(colour=guide_legend(title=\"Forecast\")) +theme_bw() + theme_bw()\n\n\n\n\n\nComparing Accuracy Metrics\n\n\nView Code\noutput1 <- accuracy(pred_consumption_ts)\noutput2 <- accuracy(meanf(consumption_ts, h=15))\noutput3 <- accuracy(naive(consumption_ts, h=15))\noutput4 <- accuracy(rwf(consumption_ts, drift=TRUE, h=15))\n\noutput_list <- list(output1, output2, output3, output4)\n\n# Put into df\noutput_df <- do.call(rbind, output_list)\n\n# Set row names for the dataframe\nrow.names(output_df) <- c(\"ARIMA\", \"Meanf\", \"Naive\", \"Drift\")\n\noutput_df\n\n\n                 ME     RMSE      MAE        MPE     MAPE     MASE        ACF1\nARIMA  2.612446e+04 233047.6 179130.0  0.1997962  7.93188 1.532328 0.007788663\nMeanf  2.244115e-10 475539.9 388753.7 -4.5394828 18.15836 3.325507 0.784939243\nNaive  3.786088e+03 301268.6 241294.6 -0.7161268 10.68622 2.064101 0.350347620\nDrift -3.142357e-11 301244.8 241185.0 -0.8957034 10.68819 2.063164 0.350347620\n\n\nFrom the plot it is seen that the ARIMA model forecast outperforms other benchmark methods.\nFrom the accuracy metrics, it is seen that the ARIMA model has the least RMSE and MAE values. This indicates that the ARIMA model outperforms the benchmark models and validates the above findings from the plot.\n\n\n\n\nView Code\nautoplot(production_ts) +\n  autolayer(meanf(production_ts, h=15),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(production_ts, h=15),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(production_ts, drift=TRUE, h=15),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(pred_production_ts, \n            series=\"ARIMA\",PI=FALSE) + ggtitle(\"Natural Gas Production ARIMA vs. Benchmarks\")+\n  guides(colour=guide_legend(title=\"Forecast\")) +theme_bw() + theme_bw()\n\n\n\n\n\nComparing Accuracy Metrics\n\n\nView Code\noutput1 <- accuracy(pred_production_ts)\noutput2 <- accuracy(meanf(production_ts, h=15))\noutput3 <- accuracy(naive(production_ts, h=15))\noutput4 <- accuracy(rwf(production_ts, drift=TRUE, h=15))\n\noutput_list <- list(output1, output2, output3, output4)\n\n# Put into df\noutput_df <- do.call(rbind, output_list)\n\n# Set row names for the dataframe\nrow.names(output_df) <- c(\"ARIMA\", \"Meanf\", \"Naive\", \"Drift\")\n\noutput_df\n\n\n                 ME      RMSE       MAE        MPE      MAPE      MASE\nARIMA  2.202881e+04  97945.77  73544.56  0.6846512  2.767895 0.5644220\nMeanf -1.546615e-10 545658.73 456839.53 -4.1435745 17.582835 3.5060416\nNaive  7.998926e+03 136882.41 103582.25  0.1519339  3.955783 0.7949480\nDrift  1.494482e-10 136648.50 103540.46 -0.1621490  3.957111 0.7946273\n             ACF1\nARIMA -0.01775295\nMeanf  0.95624518\nNaive -0.60115437\nDrift -0.60115437\n\n\nFrom the plot it is seen that the ARIMA model forecast outperforms other benchmark methods.\nFrom the accuracy metrics, it is seen that the ARIMA model has the least RMSE, ME and MAE values. This indicates that the ARIMA model outperforms the benchmark models and validates the above findings from the plot.\n\n\n\n\nView Code\nautoplot(imports_ts) +\n  autolayer(meanf(imports_ts, h=15),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(imports_ts, h=15),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(imports_ts, drift=TRUE, h=15),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(pred_imports_ts, \n            series=\"ARIMA\",PI=FALSE) + ggtitle(\"Natural Gas Imports ARIMA vs. Benchmarks\")+\n  guides(colour=guide_legend(title=\"Forecast\")) +theme_bw() + theme_bw()\n\n\n\n\n\nComparing Accuracy Metrics\n\n\nView Code\noutput1 <- accuracy(pred_imports_ts)\noutput2 <- accuracy(meanf(imports_ts, h=15))\noutput3 <- accuracy(naive(imports_ts, h=15))\noutput4 <- accuracy(rwf(imports_ts, drift=TRUE, h=15))\n\noutput_list <- list(output1, output2, output3, output4)\n\n# Put into df\noutput_df <- do.call(rbind, output_list)\n\n# Set row names for the dataframe\nrow.names(output_df) <- c(\"ARIMA\", \"Meanf\", \"Naive\", \"Drift\")\n\noutput_df\n\n\n                 ME     RMSE      MAE        MPE      MAPE      MASE\nARIMA -1.295084e+02 22945.85 18087.87 -0.6167255  6.371787 0.8086957\nMeanf -1.794955e-11 54476.42 46530.12 -3.7116764 16.899929 2.0803278\nNaive  4.711254e+01 25084.96 19464.36 -0.3595737  6.791850 0.8702374\nDrift  1.225908e-11 25084.91 19464.21 -0.3766501  6.792339 0.8702307\n             ACF1\nARIMA  0.03376946\nMeanf  0.89426354\nNaive -0.18972707\nDrift -0.18972707\n\n\nFrom the plot it is seen that the ARIMA model forecast outperforms other benchmark methods.\nFrom the accuracy metrics, it is seen that the ARIMA model has the least RMSE, ME and MAE values. This indicates that the ARIMA model outperforms the benchmark models and validates the above findings from the plot.\n\n\n\n\nView Code\nautoplot(exports_ts) +\n  autolayer(meanf(exports_ts, h=15),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(exports_ts, h=15),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(exports_ts, drift=TRUE, h=15),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(pred_exports_ts, \n            series=\"ARIMA\",PI=FALSE) + ggtitle(\"Natural Gas Exports ARIMA vs. Benchmarks\")+\n  guides(colour=guide_legend(title=\"Forecast\")) +theme_bw() + theme_bw()\n\n\n\n\n\nComparing Accuracy Metrics\n\n\nView Code\noutput1 <- accuracy(pred_exports_ts)\noutput2 <- accuracy(meanf(exports_ts, h=15))\noutput3 <- accuracy(naive(exports_ts, h=15))\noutput4 <- accuracy(rwf(exports_ts, drift=TRUE, h=15))\n\noutput_list <- list(output1, output2, output3, output4)\n\n# Put into df\noutput_df <- do.call(rbind, output_list)\n\n# Set row names for the dataframe\nrow.names(output_df) <- c(\"ARIMA\", \"Meanf\", \"Naive\", \"Drift\")\n\noutput_df\n\n\n                 ME      RMSE       MAE          MPE      MAPE      MASE\nARIMA  2.304257e+03  21024.24  12413.88    0.3732122  10.45859 0.4501893\nMeanf  1.042567e-11 162271.31 124173.29 -213.0207422 241.60702 4.5031422\nNaive  1.881955e+03  22476.64  12884.71    0.1542360  10.80099 0.4672639\nDrift -2.593253e-12  22397.71  12829.33   -3.5443285  11.47689 0.4652556\n             ACF1\nARIMA -0.01084399\nMeanf  0.97738243\nNaive -0.34599428\nDrift -0.34599428\n\n\nFrom the plot it is seen that the ARIMA model forecast and Drift benchmark model have similar trajectories for the forecast and follow the trend the best outperforming other benchmark methods.\nFrom the accuracy metrics, it is seen that the ARIMA model has the least RMSE, ME and MAE values followed by Drift method. This indicates that the ARIMA model outperforms the benchmark models and validates the above findings from the plot.\n\n\n\n\nView Code\nautoplot(cvx_stk_ts) +\n  autolayer(meanf(cvx_stk_ts, h=365),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(cvx_stk_ts, h=365),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(cvx_stk_ts, drift=TRUE, h=365),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(pred_cvx_stk_ts, \n            series=\"ARIMA\",PI=FALSE) + ggtitle(\"Chevron Corp Stocks ARIMA vs. Benchmarks\")+\n  guides(colour=guide_legend(title=\"Forecast\")) +theme_bw() + theme_bw()\n\n\n\n\n\nComparing Accuracy Metrics\n\n\nView Code\noutput1 <- accuracy(pred_cvx_stk_ts)\noutput2 <- accuracy(meanf(cvx_stk_ts, h=365))\noutput3 <- accuracy(naive(cvx_stk_ts, h=365))\noutput4 <- accuracy(rwf(cvx_stk_ts, drift=TRUE, h=365))\n\noutput_list <- list(output1, output2, output3, output4)\n\n# Put into df\noutput_df <- do.call(rbind, output_list)\n\n# Set row names for the dataframe\nrow.names(output_df) <- c(\"ARIMA\", \"Meanf\", \"Naive\", \"Drift\")\n\noutput_df\n\n\n                 ME      RMSE        MAE          MPE      MAPE       MASE\nARIMA  2.695760e-02  1.151023  0.6867105   0.02465471  1.191267 0.05199118\nMeanf  2.951653e-15 33.087596 27.3442876 -47.72471755 75.850088 2.07024927\nNaive  2.616031e-02  1.152604  0.6876453   0.02383861  1.193310 0.05206196\nDrift -1.656820e-15  1.152307  0.6868749  -0.04246424  1.192769 0.05200363\n              ACF1\nARIMA  0.004223936\nMeanf  0.998301438\nNaive -0.041236280\nDrift -0.041236280\n\n\nFrom the plot it is seen that the ARIMA model forecast outperforms other benchmark methods.\nFrom the accuracy metrics, it is seen that the ARIMA model has the least RMSE and MAE values. This indicates that the ARIMA model outperforms the benchmark models and validates the above findings from the plot.\n\n\n\n\nView Code\nautoplot(co2_ts) +\n  autolayer(meanf(co2_ts, h=15),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(co2_ts, h=15),\n            series=\"Naïve\", PI=FALSE)+\n  autolayer(rwf(co2_ts, drift=TRUE, h=15),\n            series=\"Drift\", PI=FALSE)+\n   autolayer(pred_co2_ts, \n            series=\"ARIMA\",PI=FALSE) + ggtitle(\"CO2 Emissions from Natural Gas ARIMA vs. Benchmarks\")+\n  guides(colour=guide_legend(title=\"Forecast\")) +theme_bw() + theme_bw()\n\n\n\n\n\nComparing Accuracy Metrics\n\n\nView Code\noutput1 <- accuracy(pred_co2_ts)\noutput2 <- accuracy(meanf(co2_ts, h=15))\noutput3 <- accuracy(naive(co2_ts, h=15))\noutput4 <- accuracy(rwf(co2_ts, drift=TRUE, h=15))\n\noutput_list <- list(output1, output2, output3, output4)\n\n# Put into df\noutput_df <- do.call(rbind, output_list)\n\n# Set row names for the dataframe\nrow.names(output_df) <- c(\"ARIMA\", \"Meanf\", \"Naive\", \"Drift\")\n\noutput_df\n\n\n                 ME     RMSE      MAE       MPE      MAPE      MASE\nARIMA -2.504469e+00 240.1282 153.9421 -54.13229 110.85413 0.7686453\nMeanf  4.471779e-16 337.0111 177.1023 -79.39437  92.33317 0.8842859\nNaive  2.035664e-01 480.8640 197.5995 -53.88390  75.55799 0.9866300\nDrift -6.405909e-15 480.8639 197.6104 -54.05791  75.58064 0.9866845\n              ACF1\nARIMA -0.006218897\nMeanf -0.014490610\nNaive -0.481297385\nDrift -0.481297385\n\n\nFrom the plot it is seen that the ARIMA model forecast outperforms other benchmark methods.\nFrom the accuracy metrics, it is seen that the ARIMA model has the least RMSE and MAE values. This indicates that the ARIMA model outperforms the benchmark models and validates the above findings from the plot."
  },
  {
    "objectID": "univariate.html#sarima-modeling",
    "href": "univariate.html#sarima-modeling",
    "title": "Univariate TS Models (ARIMA/SARIMA)",
    "section": "SARIMA Modeling",
    "text": "SARIMA Modeling\nThe analysis reveals clear seasonality in some of my datasets- Prices, Consumption, Production, Imports, Exports,and Industry Employment. Simple first, second, or third order differencing hasn’t adequately addressed the seasonal patterns observed in these datasets. Therefore, seasonal differencing is necessary, indicating that a SARIMA model is better suited for these types of data.\n\nSeasonal Time Series\n\nPricesConsumptionProductionImportsExports\n\n\n\n\nView Code\nprices_diff<-prices_ts %>% diff(lag = 12)\nplot1<-autoplot(prices_diff, main=\"First Difference Seasonal Plot for Natural Gas Residential Prices\", colour = \"#92c54b\") +theme_bw()\npricesacf <- ggAcf(prices_diff)+ggtitle(\"First Difference Seasonal ACF Plot \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\npricespacf <- ggPacf(prices_diff)+ggtitle(\"First Difference Seasonal PACF Plot \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\n# Combine the plots\ncombined_plot <- plot1 / (pricesacf | pricespacf)\n\n# Print the combined plot\nprint(combined_plot)\n\n\n\n\n\nWhen we apply seasonal differencing to the original dataset, a correlation still remains, yet it’s less pronounced than what was seen with first-order differencing. This observation leads to the possibility of setting the seasonal differencing parameter (D) to 1 and keeping the non-seasonal differencing parameter (d) at 0. Moving forward with performing both seasonal differencing and first-order differencing on the raw monthly data might result in over-differencing, which we want to avoid. Therefore, the appropriate P, D, Q, and d values for our SARIMA model would be determined as follows.\nP: 1 Q:1,2,3,4 D: 1 d: 0\n\n\n\n\nView Code\nconsumption_diff<-consumption_ts %>% diff(lag = 12)\nplot1<-autoplot(consumption_diff, main=\"First Difference Seasonal Plot for Natural Gas Consumption\", colour = \"#92c54b\") +theme_bw()\nconsumptionacf <- ggAcf(consumption_diff)+ggtitle(\"First Difference Seasonal ACF Plot \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\nconsumptionpacf <- ggPacf(consumption_diff)+ggtitle(\"First Difference Seasonal PACF Plot \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\n# Combine the plots\ncombined_plot <- plot1 / (consumptionacf | consumptionpacf)\n\n# Print the combined plot\nprint(combined_plot)\n\n\n\n\n\nWhen we apply seasonal differencing to the original dataset, correlation is less pronounced than what was seen with first-order differencing. This observation leads to the possibility of setting the seasonal differencing parameter (D) to 1 and keeping the non-seasonal differencing parameter (d) at 0. Moving forward with performing both seasonal differencing and first-order differencing on the raw monthly data might result in over-differencing, which we want to avoid. Therefore, the appropriate P, D, Q, and d values for our SARIMA model would be determined as follows.\nP: 1 Q: 1,2,3,4 D:1 d:0\n\n\n\n\nView Code\nproduction_diff<-production_ts %>% diff(lag = 12)%>% diff\nplot1<-autoplot(production_diff, main=\"Seasonal Plot for Natural Gas Production\", colour = \"#92c54b\") +theme_bw()\nproductionacf <- ggAcf(production_diff)+ggtitle(\"Seasonal ACF Plot \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\nproductionpacf <- ggPacf(production_diff)+ggtitle(\"Seasonal PACF Plot \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\n# Combine the plots\ncombined_plot <- plot1 / (productionacf | productionpacf)\n\n# Print the combined plot\nprint(combined_plot)\n\n\n\n\n\nWhen we apply seasonal differencing to the original dataset, a correlation still remains, yet it’s less pronounced than what was seen with first-order differencing. This observation leads to the possibility of setting the seasonal differencing parameter (D) to 1 and keeping the non-seasonal differencing parameter (d) at 0. Moving forward with performing both seasonal differencing and first-order differencing on the raw monthly data results in least correlation. Therefore, the appropriate P, D, Q, and d values for our SARIMA model would be determined as follows.\nP: 1 Q: 1 D:1 d:1\n\n\n\n\nView Code\nimports_diff<-imports_ts %>% diff(lag = 12)%>% diff\nplot1<-autoplot(imports_diff, main=\"Seasonal Plot for Natural Gas Imports\", colour = \"#92c54b\") +theme_bw()\nimportsacf <- ggAcf(imports_diff)+ggtitle(\"Seasonal ACF Plot \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\nimportspacf <- ggPacf(imports_diff)+ggtitle(\"Seasonal PACF Plot \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\n# Combine the plots\ncombined_plot <- plot1 / (importsacf | importspacf)\n\n# Print the combined plot\nprint(combined_plot)\n\n\n\n\n\nWhen we apply seasonal differencing to the original dataset, a correlation still remains, yet it’s less pronounced than what was seen with first-order differencing. This observation leads to the possibility of setting the seasonal differencing parameter (D) to 1 and keeping the non-seasonal differencing parameter (d) at 0. Moving forward with performing both seasonal differencing and first-order differencing on the raw monthly data results in least correlation. Therefore, the appropriate P, D, Q, and d values for our SARIMA model would be determined as follows.\nP: 1 Q: 1 D:1 d:1\n\n\n\n\nView Code\n# exports_diff<-exports_ts %>% diff(lag = 12)%>% diff\nexports_diff <- exports_ts %>%\n  diff(lag = 1) %>%  # First difference\n  na.omit()  # Remove NAs that result from differencing\n\nplot1<-autoplot(exports_diff, main=\"Seasonal Plot for Natural Gas Exports\", colour = \"#92c54b\") +theme_bw()\nexportsacf <- ggAcf(exports_diff, lag.max = 12)+ggtitle(\"Seasonal ACF Plot \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\nexportspacf <- ggPacf(exports_diff, lag.max = 12)+ggtitle(\"Seasonal PACF Plot \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\n# Combine the plots\ncombined_plot <- plot1 / (exportsacf | exportspacf)\n\n# Print the combined plot\nprint(combined_plot)\n\n\n\n\n\nWhen we apply non-seasonal first differencing to the original dataset, the correlation is less pronounced. This observation leads to the possibility of setting the seasonal differencing parameter (D) to 0 and keeping the non-seasonal differencing parameter (d) at 1. Moving forward with performing both seasonal differencing and first-order differencing on the raw monthly data might result in over-differencing, which we want to avoid. Therefore, the appropriate P, D, Q, and d values for our SARIMA model would be determined as follows.\nP: 1 Q: 1,2 D: 0 d:1\n\n\n\n\n\nModel Selection & Fitting\nIn this section, we’ll use insights from the ACF and PACF plots, alongside the differencing steps we’ve executed, to fit SARIMA(p,d,q)(P,D,Q) models manually by exploring various combinations of the parameters p, d, q and P, D, Q. And get the best models resulted from manual fitting and from Auto.ARIMA().\n\nPricesConsumptionProductionImportsExports\n\n\n\nManual Prameter SelectionAuto ARIMA\n\n\np :1,2,3,4 d: 0 q : 1, 2 P: 1 D: 1 Q:1,2,3,4\n\n\nView Code\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=0\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*150),nrow=150)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1),method=\"ML\")\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n# q=0,1,; Q=0,1 and PACF plot: p=0,1; P=0,1, D=0 and d=O\noutput=SARIMA.c(p1=1,p2=4,q1=1,q2=2,P1=1,P2=1,Q1=1,Q2=4,data=prices_ts)\n#output\n\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q      AIC      BIC     AICc\n10 1 0 0 0 1 1 534.6674 544.6218 534.7874\n\n\n\n\nView Code\noutput[which.min(output$BIC),] \n\n\n   p d q P D Q      AIC      BIC     AICc\n10 1 0 0 0 1 1 534.6674 544.6218 534.7874\n\n\n\n\nView Code\noutput[which.min(output$AICc),] \n\n\n   p d q P D Q      AIC      BIC     AICc\n10 1 0 0 0 1 1 534.6674 544.6218 534.7874\n\n\nThe model with the lowest AIC and BIC is ARIMA(1,0,0)x(0,1,1)[12].\n\n\n\n\nView Code\nauto.arima(prices_ts)\n\n\nSeries: prices_ts \nARIMA(1,1,1)(2,0,0)[12] \n\nCoefficients:\n          ar1     ma1     sar1     sar2\n      -0.9618  0.9071  -0.1570  -0.1649\ns.e.   0.0728  0.1003   0.0865   0.0996\n\nsigma^2 = 0.6418:  log likelihood = -255.84\nAIC=521.68   AICc=521.96   BIC=538.53\n\n\n\n\n\nBest Model 1: ARIMA(1,0,0)x(0,1,1)[12] Best Model 2 (from Auto ARIMA): ARIMA(1,1,1)(2,0,0)[12]\n\n\n\nManual Prameter SelectionAuto ARIMA\n\n\np: 1,2,3 d:0 q: 1 P: 1 D:1 Q: 1,2,3,4\n\n\nView Code\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=0\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*150),nrow=150)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1),method=\"ML\")\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n# q=0,1,; Q=0,1 and PACF plot: p=0,1; P=0,1, D=0 and d=O\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=1,P1=1,P2=1,Q1=1,Q2=4,data=consumption_ts)\n#output\n\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q      AIC      BIC     AICc\n11 2 0 0 0 1 2 5357.089 5373.679 5357.392\n\n\n\n\nView Code\noutput[which.min(output$BIC),] \n\n\n   p d q P D Q      AIC      BIC     AICc\n10 2 0 0 0 1 1 5357.631 5370.904 5357.832\n\n\n\n\nView Code\noutput[which.min(output$AICc),] \n\n\n   p d q P D Q      AIC      BIC     AICc\n11 2 0 0 0 1 2 5357.089 5373.679 5357.392\n\n\nThe model with the lowest AIC is ARIMA(2,0,0)x(0,1,2)[12] and the model with the lowest BIC is ARIMA(2,0,0)x(0,1,1)[12]\n\n\n\n\nView Code\nauto.arima(consumption_ts)\n\n\nSeries: consumption_ts \nARIMA(2,0,3)(2,1,2)[12] with drift \n\nCoefficients:\n          ar1     ar2     ma1      ma2      ma3    sar1     sar2     sma1\n      -0.1423  0.8233  0.4830  -0.5788  -0.1284  0.6048  -0.3901  -1.3803\ns.e.   0.1536  0.1483  0.1688   0.2451   0.1228  0.1723   0.0885   0.1878\n        sma2      drift\n      0.5664  4319.2878\ns.e.  0.1680   366.7623\n\nsigma^2 = 1.06e+10:  log likelihood = -2648.06\nAIC=5318.11   AICc=5319.49   BIC=5354.61\n\n\n\n\n\nBest Model 1: ARIMA(2,0,0)x(0,1,2)[12] Best Model 2: ARIMA(2,0,0)x(0,1,1)[12] Best Model 3 (from Auto ARIMA): ARIMA(2,0,3)(2,1,2)[12]\n\n\n\nManual Prameter SelectionAuto ARIMA\n\n\np: 1,2,3,4 d:1 q: 1 P: 1 D:1 Q: 1\n\n\nView Code\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*150),nrow=150)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1),method=\"ML\")\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n# q=0,1,; Q=0,1 and PACF plot: p=0,1; P=0,1, D=0 and d=O\noutput=SARIMA.c(p1=1,p2=4,q1=1,q2=1,P1=1,P2=1,Q1=1,Q2=1,data=production_ts)\n#output\n\noutput[which.min(output$AIC),] \n\n\n  p d q P D Q      AIC     BIC     AICc\n4 3 1 0 0 1 0 5101.137 5114.39 5101.339\n\n\n\n\nView Code\noutput[which.min(output$BIC),] \n\n\n  p d q P D Q      AIC      BIC     AICc\n2 1 1 0 0 1 0 5103.996 5110.623 5104.056\n\n\n\n\nView Code\noutput[which.min(output$AICc),] \n\n\n  p d q P D Q      AIC     BIC     AICc\n4 3 1 0 0 1 0 5101.137 5114.39 5101.339\n\n\nThe model with the lowest AIC is ARIMA(3,1,0)x(0,1,0)[12] and the model with the lowest BIC is ARIMA(1,1,0)x(0,1,0)[12]\n\n\n\n\nView Code\nauto.arima(production_ts)\n\n\nSeries: production_ts \nARIMA(0,1,1)(1,1,1)[12] \n\nCoefficients:\n          ma1     sar1     sma1\n      -0.2318  -0.1148  -0.6329\ns.e.   0.0806   0.0976   0.0785\n\nsigma^2 = 3.033e+09:  log likelihood = -2506.58\nAIC=5021.16   AICc=5021.36   BIC=5034.41\n\n\n\n\n\nBest Model 1: ARIMA(3,1,0)x(0,1,0)[12] Best Model 2: ARIMA(1,1,0)x(0,1,0)[12]\nBest Model 3 (from Auto ARIMA): ARIMA(0,1,1)(1,1,1)[12]\n\n\n\nManual Prameter SelectionAuto ARIMA\n\n\np: 1,2 d:1 q: 1 P: 1 D:1 Q: 1\n\n\nView Code\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*150),nrow=150)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1),method=\"ML\")\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n# q=0,1,; Q=0,1 and PACF plot: p=0,1; P=0,1, D=0 and d=O\n  output=SARIMA.c(p1=1,p2=2,q1=1,q2=1,P1=1,P2=1,Q1=1,Q2=1,data=imports_ts)\n#output\n\noutput[which.min(output$AIC),] \n\n\n  p d q P D Q      AIC      BIC     AICc\n2 1 1 0 0 1 0 6759.272 6766.673 6759.312\n\n\n\n\nView Code\noutput[which.min(output$BIC),] \n\n\n  p d q P D Q      AIC      BIC     AICc\n2 1 1 0 0 1 0 6759.272 6766.673 6759.312\n\n\n\n\nView Code\noutput[which.min(output$AICc),] \n\n\n  p d q P D Q      AIC      BIC     AICc\n2 1 1 0 0 1 0 6759.272 6766.673 6759.312\n\n\nThe model with the lowest AIC and BIC is ARIMA(1,1,0)x(0,1,0)[12]\n\n\n\n\nView Code\nauto.arima(imports_ts)\n\n\nSeries: imports_ts \nARIMA(0,1,3)(1,1,1)[12] \n\nCoefficients:\n          ma1      ma2      ma3    sar1     sma1\n      -0.3251  -0.0821  -0.1358  0.1119  -0.8805\ns.e.   0.0578   0.0629   0.0633  0.0780   0.0621\n\nsigma^2 = 232480233:  log likelihood = -3309.68\nAIC=6631.35   AICc=6631.64   BIC=6653.55\n\n\n\n\n\nBest Model 1: ARIMA(1,1,0)x(0,1,0)[12] Best Model 2 (from Auto ARIMA): ARIMA(0,1,3)(1,1,1)[12]\n\n\n\nManual Prameter SelectionAuto ARIMA\n\n\np: 1 d: 1 q: 1,2 P: 1 D: 0 Q: 1,2\n\n\nView Code\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=1\n  D=0\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*150),nrow=150)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1),method=\"ML\")\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n# q=0,1,; Q=0,1 and PACF plot: p=0,1; P=0,1, D=0 and d=O\noutput=SARIMA.c(p1=1,p2=1,q1=1,q2=2,P1=1,P2=1,Q1=1,Q2=2,data=exports_ts)\n#output\n\noutput[which.min(output$AIC),] \n\n\n  p d q P D Q      AIC      BIC     AICc\n4 0 1 1 0 0 1 7024.165 7035.384 7024.243\n\n\n\n\nView Code\noutput[which.min(output$BIC),] \n\n\n  p d q P D Q      AIC      BIC     AICc\n4 0 1 1 0 0 1 7024.165 7035.384 7024.243\n\n\n\n\nView Code\noutput[which.min(output$AICc),] \n\n\n  p d q P D Q      AIC      BIC     AICc\n4 0 1 1 0 0 1 7024.165 7035.384 7024.243\n\n\nThe model with the lowest AIC and BIC is ARIMA(0,1,1)x(0,0,1)[12].\n\n\n\n\nView Code\nauto.arima(exports_ts)\n\n\nSeries: exports_ts \nARIMA(0,1,1)(1,0,0)[12] \n\nCoefficients:\n          ma1    sar1\n      -0.1145  0.5996\ns.e.   0.0595  0.0505\n\nsigma^2 = 316782460:  log likelihood = -3486.68\nAIC=6979.36   AICc=6979.44   BIC=6990.58\n\n\n\n\n\nBest Model 1: ARIMA(0,1,1)x(0,0,1)[12] Best Model 2 (from Auto ARIMA): ARIMA(0,1,1)(1,0,0)[12]\n\n\n\n\n\nModel Diagnostics\nIn this part, we’re going to fit all the best model obtained from our earlier analysis, and then dive into some model diagnostics to see how each one of them is performing.\n\nPricesConsumptionProductionImportsExports\n\n\n\nModel 1Model 2\n\n\n\n\nView Code\nmodel_output <- capture.output(sarima(prices_ts, 1,0,0,0,1,1,12))\n\n\n\n\n\n\n\nView Code\ncat(model_output[39:49], model_output[length(model_output)], sep = \"\\n\") \n\n\nCoefficients: \n         Estimate     SE t.value p.value\nar1        0.9188 0.0294 31.2519   0.000\nsma1      -0.9387 0.1169 -8.0327   0.000\nconstant  -0.0161 0.0101 -1.5873   0.114\n\nsigma^2 estimated as 0.6847897 on 201 degrees of freedom \n \nAIC = 2.622464  AICc = 2.623052  BIC = 2.687525 \n \nNA\n \n\n\n\n\n\n\nView Code\nmodel_output1 <- capture.output(sarima(prices_ts, 1,1,1,2,0,0,12))\n\n\n\n\n\n\n\nView Code\ncat(model_output1[63:73], model_output1[length(model_output1)], sep = \"\\n\") \n\n\nCoefficients: \n         Estimate     SE  t.value p.value\nar1       -0.9620 0.0725 -13.2641  0.0000\nma1        0.9073 0.1000   9.0735  0.0000\nsar1      -0.1577 0.0865  -1.8223  0.0698\nsar2      -0.1657 0.0997  -1.6625  0.0979\nconstant  -0.0082 0.0406  -0.2021  0.8400\n\nsigma^2 estimated as 0.629751 on 210 degrees of freedom \n \nAIC = 2.43552  AICc = 2.436855  BIC = 2.529584 \n \n\n\n\n\n\nThe ACF plot reveals that most autocorrelation values are indeed within the confidence interval, indicating a good fit for both the models. The Q-Q plot suggest that the residuals are normally distributed for both the models.From the Ljung-Box Test, the p-values for model1 are typically below 0.05 suggesting that the residuals are not random, which is not desirable in a good model fit while for model2 are typically above 0.05 suggesting that the residuals are random, which is desirable in a good model fit. On comparing AIC, AICc, and BIC, model 2 has lower values indicating a better fit.\n\n\n\nModel 1Model 2Model 3\n\n\n\n\nView Code\nmodel_output <- capture.output(sarima(consumption_ts, 2,0,0,0,1,2,12))\n\n\n\n\n\n\n\nView Code\ncat(model_output[30:44], model_output[length(model_output)], sep = \"\\n\") \n\n\nconverged\n<><><><><><><><><><><><><><>\n \nCoefficients: \n          Estimate       SE t.value p.value\nar1         0.3520   0.0705  4.9902  0.0000\nar2         0.1232   0.0703  1.7539  0.0810\nsma1       -0.6989   0.0920 -7.5939  0.0000\nsma2       -0.1987   0.0944 -2.1045  0.0366\nconstant 4297.3233 272.1778 15.7887  0.0000\n\nsigma^2 estimated as 10865088632 on 199 degrees of freedom \n \nAIC = 26.09303  AICc = 26.09452  BIC = 26.19062 \n \n \n\n\n\n\n\n\nView Code\nmodel_output1 <- capture.output(sarima(consumption_ts, 2,0,0,0,1,1,12))\n\n\n\n\n\n\n\nView Code\ncat(model_output1[23:36], model_output1[length(model_output1)], sep = \"\\n\") \n\n\nconverged\n<><><><><><><><><><><><><><>\n \nCoefficients: \n          Estimate       SE  t.value p.value\nar1         0.3686   0.0697   5.2872  0.0000\nar2         0.1287   0.0697   1.8469  0.0662\nsma1       -0.8392   0.0570 -14.7302  0.0000\nconstant 4295.5678 295.3862  14.5422  0.0000\n\nsigma^2 estimated as 11279302738 on 200 degrees of freedom \n \nAIC = 26.1058  AICc = 26.10679  BIC = 26.18713 \n \n \n\n\n\n\n\n\nView Code\nmodel_output2 <- capture.output(sarima(consumption_ts, 2,0,3,2,1,2,12))\n\n\n\n\n\n\n\nView Code\ncat(model_output2[79:98], model_output1[length(model_output1)], sep = \"\\n\") \n\n\nconverged\n<><><><><><><><><><><><><><>\n \nCoefficients: \n          Estimate       SE t.value p.value\nar1        -0.1423   0.1536 -0.9264  0.3554\nar2         0.8233   0.1483  5.5496  0.0000\nma1         0.4830   0.1688  2.8619  0.0047\nma2        -0.5788   0.2451 -2.3615  0.0192\nma3        -0.1284   0.1228 -1.0458  0.2969\nsar1        0.6048   0.1723  3.5112  0.0006\nsar2       -0.3901   0.0885 -4.4067  0.0000\nsma1       -1.3803   0.1878 -7.3481  0.0000\nsma2        0.5664   0.1680  3.3726  0.0009\nconstant 4319.2878 366.7623 11.7768  0.0000\n\nsigma^2 estimated as 10081725505 on 194 degrees of freedom \n \nAIC = 26.06918  AICc = 26.07477  BIC = 26.2481 \n \n \n\n\n\n\n\nThe ACF plot reveals that most autocorrelation values are indeed within the confidence interval, indicating a good fit for all the models. The Q-Q plot suggest that the residuals are normally distributed for all the models.From the Ljung-Box Test, the p-values for all the models are typically below 0.05 suggesting that the residuals are not random, which may not be desirable in a good model fit. On comparing AIC, AICc, and BIC, model 3 has lower values indicating a better fit.\n\n\n\nModel 1Model 2Model 3\n\n\n\n\nView Code\nmodel_output <- capture.output(sarima(production_ts, 3,1,0,0,1,0,12))\n\n\n\n\n\n\n\nView Code\ncat(model_output[14:25], model_output[length(model_output)], sep = \"\\n\") \n\n\nconverged\n<><><><><><><><><><><><><><>\n \nCoefficients: \n    Estimate     SE t.value p.value\nar1  -0.2109 0.0701 -3.0085  0.0030\nar2  -0.1593 0.0707 -2.2513  0.0255\nar3  -0.1262 0.0699 -1.8070  0.0723\n\nsigma^2 estimated as 4607287504 on 200 degrees of freedom \n \nAIC = 25.12876  AICc = 25.12935  BIC = 25.19404 \n \n\n\n\n\n\n\nView Code\nmodel_output1 <- capture.output(sarima(production_ts, 1,1,0,0,1,0,12))\n\n\n\n\n\n\n\nView Code\ncat(model_output1[13:23], model_output1[length(model_output1)], sep = \"\\n\") \n\n\nconverged\n<><><><><><><><><><><><><><>\n \nCoefficients: \n    Estimate     SE t.value p.value\nar1  -0.1695 0.0695 -2.4399  0.0156\n\nsigma^2 estimated as 4767628473 on 202 degrees of freedom \n \nAIC = 25.14284  AICc = 25.14294  BIC = 25.17548 \n \n \n\n\n\n\n\n\nView Code\nmodel_output2 <- capture.output(sarima(production_ts, 0,1,1,1,1,1,12))\n\n\n\n\n\n\n\nView Code\ncat(model_output2[21:33], model_output1[length(model_output1)], sep = \"\\n\") \n\n\nconverged\n<><><><><><><><><><><><><><>\n \nCoefficients: \n     Estimate     SE t.value p.value\nma1   -0.2318 0.0806 -2.8754  0.0045\nsar1  -0.1148 0.0976 -1.1759  0.2410\nsma1  -0.6329 0.0785 -8.0600  0.0000\n\nsigma^2 estimated as 2988085206 on 200 degrees of freedom \n \nAIC = 24.73479  AICc = 24.73538  BIC = 24.80007 \n \n \n\n\n\n\n\nOn comparing, the ACF plot reveals that most autocorrelation values are indeed within the confidence interval, indicating a good fit for model 3. The Q-Q plot suggest that the residuals are normally distributed for all the models.From the Ljung-Box Test, the p-values for the model 3 are typically above 0.05 suggesting that the residuals are random, which is desirable in a good model fit. On comparing AIC, AICc, and BIC, model 3 has lower values indicating a better fit.\n\n\n\nModel 1Model 2\n\n\n\n\nView Code\nmodel_output <- capture.output(sarima(imports_ts, 1,1,0,0,1,0,12))\n\n\n\n\n\n\n\nView Code\ncat(model_output[13:23], model_output[length(model_output)], sep = \"\\n\") \n\n\nconverged\n<><><><><><><><><><><><><><>\n \nCoefficients: \n    Estimate     SE t.value p.value\nar1  -0.2793 0.0558  -5.009       0\n\nsigma^2 estimated as 379640070 on 298 degrees of freedom \n \nAIC = 22.60626  AICc = 22.60631  BIC = 22.63101 \n \n \n\n\n\n\n\n\nView Code\nmodel_output1 <- capture.output(sarima(imports_ts, 0,1,3,1,1,1,12))\n\n\n\n\n\n\n\nView Code\ncat(model_output1[24:37], model_output1[length(model_output1)], sep = \"\\n\") \n\n\nconverged\n<><><><><><><><><><><><><><>\n \nCoefficients: \n     Estimate     SE  t.value p.value\nma1   -0.3251 0.0578  -5.6290  0.0000\nma2   -0.0821 0.0629  -1.3051  0.1929\nma3   -0.1358 0.0633  -2.1467  0.0326\nsar1   0.1119 0.0780   1.4343  0.1526\nsma1  -0.8805 0.0621 -14.1862  0.0000\n\nsigma^2 estimated as 228590051 on 294 degrees of freedom \n \nAIC = 22.17843  AICc = 22.17912  BIC = 22.25269 \n \n\n\n\n\n\nOn comparing, the ACF plot reveals that most autocorrelation values are indeed within the confidence interval, indicating a good fit for model 2. The Q-Q plot suggest that the residuals are normally distributed for all the models.From the Ljung-Box Test, the p-values for the model 2 are typically above 0.05 suggesting that the residuals are random, which is desirable in a good model fit. On comparing AIC, AICc, and BIC, model 2 has lower values indicating a better fit.\n\n\n\nModel 1Model 2\n\n\n\n\nView Code\nmodel_output <- capture.output(sarima(exports_ts, 0,1,1,0,0,1,12))\n\n\n\n\n\n\n\nView Code\ncat(model_output[21:29], model_output[length(model_output)], sep = \"\\n\") \n\n\nCoefficients: \n          Estimate        SE t.value p.value\nma1        -0.2094    0.0563 -3.7215  0.0002\nsma1        0.4025    0.0482  8.3461  0.0000\nconstant 1770.9787 1188.6144  1.4900  0.1373\n\nsigma^2 estimated as 364702683 on 308 degrees of freedom \n \nAIC = 22.58516  AICc = 22.58541  BIC = 22.63326 \n \n\n\n\n\n\n\nView Code\nmodel_output1 <- capture.output(sarima(exports_ts, 0,1,1,1,0,0,12))\n\n\n\n\n\n\n\nView Code\ncat(model_output1[25:33], model_output1[length(model_output1)], sep = \"\\n\") \n\n\nCoefficients: \n          Estimate        SE t.value p.value\nma1        -0.1185    0.0600 -1.9752  0.0491\nsar1        0.5946    0.0511 11.6266  0.0000\nconstant 1602.9180 2072.9403  0.7733  0.4400\n\nsigma^2 estimated as 314262815 on 308 degrees of freedom \n \nAIC = 22.44622  AICc = 22.44647  BIC = 22.49432 \n \n\n\n\n\n\nOn comparing, the ACF plot reveals that most autocorrelation values are indeed within the confidence interval, indicating a good fit for both the models. The Q-Q plot suggest that the residuals are normally distributed for all the models.From the Ljung-Box Test, the p-values for both the models are typically below 0.05 suggesting that the residuals are not random, which may not be desirable in a good model fit. On comparing AIC, AICc, and BIC, model 2 has lower values indicating a better fit.\n\n\n\n\n\nCross-Validation\nIn this part, we’ll pinpoint the model that fits our data best by employing cross-validation and identifying the one with the lowest mean absolute error (MAE) and root mean square error (RMSE).\n\nPricesConsumptionProductionImportsExports\n\n\n\n\nView Code\nx=prices_ts\n  \nset.seed(123)\nk <- 144# first training set length \nn <- length(x) \n\n#n-k=168; 168/12=14; k=144\n\n# matrix(NA,(n-k)/4,4)\nres1 <- matrix(NA,(n-k)/4,12)\nres2 <- matrix(NA,(n-k)/4,12)\n\nmae1 <- matrix(NA, (n-k)/4,12)\nmae2 <- matrix(NA,(n-k)/4,12)\n\n\n\nst <- tsp(x)[1]+(k-1)/12\n\nfor(i in 1:(n-k)/12)\n{\n  #xtrain <- window(a10, start=st+(i-k+1)/4, end=st+i/4)\n  xtrain <- window(x, end=st + i-1)\n  xtest <- window(x, start=st + (i-1) + 1/12, end=st + i)\n  \n  fit <- Arima(xtrain, order=c(1,0,0), seasonal=list(order=c(0,1,1), period=12),\n               include.drift=TRUE, method=\"CSS\")\n  fcast <- forecast(fit, h=12)\n  \n  fit2 <- Arima(xtrain, order=c(1,1,1), seasonal=list(order=c(2,0,0), period=12),\n                include.drift=TRUE, method=\"CSS\")\n  fcast2 <- forecast(fit2, h=12)\n  \n  mae1[i,] <- abs(fcast$mean-xtest)\n  mae2[i,] <- abs(fcast2$mean-xtest)\n  \n  ####### to calculate RMSE ##########\n  res1[i,] <- (fcast$mean-xtest)^2\n  res2[i,] <- (fcast2$mean-xtest)^2\n  \n  \n}\n\nplot(1:12, colMeans(mae1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"MAE\")\nlines(1:12, colMeans(mae2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"Fit1: SARIMA(1,0,0)x(0,1,1)12\",\"Fit2:SARIMA(1,1,1)(2,0,0)12\"),col=2:4,lty=1)\n\n\n\n\n\n\n\nView Code\n########### RMSE ############\n\nrmse1 =sqrt(colMeans(res1,na.rm=TRUE))\nrmse2=sqrt(colMeans(res2,na.rm=TRUE))\n\n\nplot(1:12, rmse1, type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:12, rmse2, type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"Fit1: SARIMA(1,0,0)x(0,1,1)12\",\"Fit2:SARIMA(1,1,1)(2,0,0)12\"),col=2:4,lty=1)\n\n\n\n\n\nOverall, Fit 2 pulls ahead with slightly lower errors. So, it looks like Fit 2 is better.\n\n\n\n\nView Code\nx=consumption_ts\n  \nset.seed(123)\nk <- 144# first training set length \nn <- length(x) \n\n#n-k=168; 168/12=14; k=144\n\n# matrix(NA,(n-k)/4,4)\nres1 <- matrix(NA,(n-k)/4,12)\nres2 <- matrix(NA,(n-k)/4,12)\nres3 <- matrix(NA,(n-k)/4,12)\n\nmae1 <- matrix(NA, (n-k)/4,12)\nmae2 <- matrix(NA,(n-k)/4,12)\nmae3 <- matrix(NA,(n-k)/4,12)\n\n\n\nst <- tsp(x)[1]+(k-1)/12\n\nfor(i in 1:(n-k)/12)\n{\n  #xtrain <- window(a10, start=st+(i-k+1)/4, end=st+i/4)\n  xtrain <- window(x, end=st + i-1)\n  xtest <- window(x, start=st + (i-1) + 1/12, end=st + i)\n\n  fit <- Arima(xtrain, order=c(2,0,0), seasonal=list(order=c(0,1,2), period=12),\n               include.drift=TRUE,  method=\"CSS\")\n  fcast <- forecast(fit, h=12)\n  \n  fit2 <- Arima(xtrain, order=c(2,0,0), seasonal=list(order=c(0,1,1), period=12),\n                include.drift=TRUE,  method=\"CSS\")\n  fcast2 <- forecast(fit2, h=12)\n  \n  fit3 <- Arima(xtrain, order=c(2,0,3), seasonal=list(order=c(2,1,2), period=12),\n                include.drift=TRUE,  method=\"CSS\")\n  fcast3 <- forecast(fit3, h=12)\n  \n  mae1[i,] <- abs(fcast$mean-xtest)\n  mae2[i,] <- abs(fcast2$mean-xtest)\n  mae3[i,] <- abs(fcast3$mean-xtest)\n  \n  ####### to calculate RMSE ##########\n  res1[i,] <- (fcast$mean-xtest)^2\n  res2[i,] <- (fcast2$mean-xtest)^2\n  res3[i,] <- (fcast3$mean-xtest)^2\n  \n  \n}\n\nplot(1:12, colMeans(mae1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"MAE\")\nlines(1:12, colMeans(mae2,na.rm=TRUE), type=\"l\",col=3)\nlines(1:12, colMeans(mae3,na.rm=TRUE), type=\"l\",col=4)\nlegend(\"topleft\",legend=c(\"Fit1: SARIMA(2,0,0)x(0,1,2)12\",\"Fit2:SARIMA(2,0,0)x(0,1,1)12\",\"Fit3:SARIMA(2,0,3)x(2,1,2)12\"),col=2:5,lty=1)\n\n\n\n\n\n\n\nView Code\n########### RMSE ############\n\nrmse1 =sqrt(colMeans(res1,na.rm=TRUE))\nrmse2=sqrt(colMeans(res2,na.rm=TRUE))\nrmse3=sqrt(colMeans(res3,na.rm=TRUE))\n\n\nplot(1:12, rmse1, type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:12, rmse2, type=\"l\",col=3)\nlines(1:12, rmse3, type=\"l\",col=4)\nlegend(\"topleft\",legend=c(\"Fit1: SARIMA(2,0,0)x(0,1,2)12\",\"Fit2:SARIMA(2,0,0)x(0,1,1)12\",\"Fit3:SARIMA(2,0,3)x(2,1,2)12\"),col=2:5,lty=1)\n\n\n\n\n\nOverall, Fit 3 pulls ahead with slightly lower errors. So, it looks like Fit 3 is better.\n\n\n\n\nView Code\nx=production_ts\n  \nset.seed(123)\nk <- 144# first training set length \nn <- length(x) \n\n#n-k=168; 168/12=14; k=144\n\n# matrix(NA,(n-k)/4,4)\nres1 <- matrix(NA,(n-k)/4,12)\nres2 <- matrix(NA,(n-k)/4,12)\nres3 <- matrix(NA,(n-k)/4,12)\n\nmae1 <- matrix(NA, (n-k)/4,12)\nmae2 <- matrix(NA,(n-k)/4,12)\nmae3 <- matrix(NA,(n-k)/4,12)\n\n\n\nst <- tsp(x)[1]+(k-1)/12\n\nfor(i in 1:(n-k)/12)\n{\n  #xtrain <- window(a10, start=st+(i-k+1)/4, end=st+i/4)\n  xtrain <- window(x, end=st + i-1)\n  xtest <- window(x, start=st + (i-1) + 1/12, end=st + i)\n\n  fit <- Arima(xtrain, order=c(3,1,0), seasonal=list(order=c(0,1,0), period=12),\n               include.drift=TRUE,  method=\"CSS\")\n  fcast <- forecast(fit, h=12)\n\n  fit2 <- Arima(xtrain, order=c(1,1,0), seasonal=list(order=c(0,1,0), period=12),\n                include.drift=TRUE,  method=\"CSS\")\n  fcast2 <- forecast(fit2, h=12)\n\n  fit3 <- Arima(xtrain, order=c(0,1,1), seasonal=list(order=c(1,1,1), period=12),\n                include.drift=TRUE,  method=\"CSS\")\n  fcast3 <- forecast(fit3, h=12)\n  \n  mae1[i,] <- abs(fcast$mean-xtest)\n  mae2[i,] <- abs(fcast2$mean-xtest)\n  mae3[i,] <- abs(fcast3$mean-xtest)\n  \n  ####### to calculate RMSE ##########\n  res1[i,] <- (fcast$mean-xtest)^2\n  res2[i,] <- (fcast2$mean-xtest)^2\n  res3[i,] <- (fcast3$mean-xtest)^2\n  \n  \n}\n\nplot(1:12, colMeans(mae1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"MAE\")\nlines(1:12, colMeans(mae2,na.rm=TRUE), type=\"l\",col=3)\nlines(1:12, colMeans(mae3,na.rm=TRUE), type=\"l\",col=4)\nlegend(\"topleft\",legend=c(\"Fit1: SARIMA(3,1,0)x(0,1,0)12\",\"Fit2:SARIMA(1,1,0)x(0,1,0)12\",\"Fit3:SARIMA(0,1,1)x(1,1,1)12\"),col=2:5,lty=1)\n\n\n\n\n\n\n\nView Code\n########### RMSE ############\n\nrmse1 =sqrt(colMeans(res1,na.rm=TRUE))\nrmse2=sqrt(colMeans(res2,na.rm=TRUE))\nrmse3=sqrt(colMeans(res3,na.rm=TRUE))\n\n\nplot(1:12, rmse1, type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:12, rmse2, type=\"l\",col=3)\nlines(1:12, rmse3, type=\"l\",col=4)\nlegend(\"topleft\",legend=c(\"Fit1: SARIMA(3,1,0)x(0,1,0)12\",\"Fit2:SARIMA(1,1,0)x(0,1,0)12\",\"Fit3:SARIMA(0,1,1)x(1,1,1)12\"),col=2:5,lty=1)\n\n\n\n\n\nOverall, Fit 3 pulls ahead with slightly lower errors. So, it looks like Fit 3 is better.\n\n\n\n\nView Code\nx=imports_ts\n  \nset.seed(123)\nk <- 51# first training set length \nn <- length(x) \n\n#n-k=168; 168/12=14; k=144\n\n# matrix(NA,(n-k)/4,4)\nres1 <- matrix(NA,(n-k)/4,12)\nres2 <- matrix(NA,(n-k)/4,12)\n\nmae1 <- matrix(NA, (n-k)/4,12)\nmae2 <- matrix(NA,(n-k)/4,12)\n\n\n\nst <- tsp(x)[1]+(k-1)/12\n\nfor(i in 1:(n-k)/12)\n{\n  #xtrain <- window(a10, start=st+(i-k+1)/4, end=st+i/4)\n  xtrain <- window(x, end=st + i-1)\n  xtest <- window(x, start=st + (i-1) + 1/12, end=st + i)\n\n  fit <- Arima(xtrain, order=c(1,1,0), seasonal=list(order=c(0,1,0), period=12),\n               include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=12)\n\n  fit2 <- Arima(xtrain, order=c(0,1,3), seasonal=list(order=c(1,1,1), period=12),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=12)\n  \n  mae1[i,] <- abs(fcast$mean-xtest)\n  mae2[i,] <- abs(fcast2$mean-xtest)\n  \n  ####### to calculate RMSE ##########\n  res1[i,] <- (fcast$mean-xtest)^2\n  res2[i,] <- (fcast2$mean-xtest)^2\n  \n  \n}\n\nplot(1:12, colMeans(mae1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"MAE\")\nlines(1:12, colMeans(mae2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"Fit1: SARIMA(1,1,0)x(0,1,0)12\",\"Fit2:SARIMA(0,1,3)x(1,1,1)12\"),col=2:4,lty=1)\n\n\n\n\n\n\n\nView Code\n########### RMSE ############\n\nrmse1 =sqrt(colMeans(res1,na.rm=TRUE))\nrmse2=sqrt(colMeans(res2,na.rm=TRUE))\n\n\nplot(1:12, rmse1, type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:12, rmse2, type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"Fit1: SARIMA(1,1,0)x(0,1,0)12\",\"Fit2:SARIMA(0,1,3)x(1,1,1)12\"),col=2:4,lty=1)\n\n\n\n\n\nOverall, Fit 2 pulls ahead with slightly lower errors. So, it looks like Fit 2 is better.\n\n\n\n\nView Code\nx=exports_ts\n  \nset.seed(123)\nk <- 144# first training set length \nn <- length(x) \n\n#n-k=168; 168/12=14; k=144\n\n# matrix(NA,(n-k)/4,4)\nres1 <- matrix(NA,(n-k)/4,12)\nres2 <- matrix(NA,(n-k)/4,12)\n\nmae1 <- matrix(NA, (n-k)/4,12)\nmae2 <- matrix(NA,(n-k)/4,12)\n\n\n\nst <- tsp(x)[1]+(k-1)/12\n\nfor(i in 1:(n-k)/12)\n{\n  #xtrain <- window(a10, start=st+(i-k+1)/4, end=st+i/4)\n  xtrain <- window(x, end=st + i-1)\n  xtest <- window(x, start=st + (i-1) + 1/12, end=st + i)\n  \n  fit <- Arima(xtrain, order=c(0,1,1), seasonal=list(order=c(0,0,1), period=12),\n               include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=12)\n  \n  fit2 <- Arima(xtrain, order=c(0,1,1), seasonal=list(order=c(1,0,0), period=12),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=12)\n  \n  mae1[i,] <- abs(fcast$mean-xtest)\n  mae2[i,] <- abs(fcast2$mean-xtest)\n  \n  ####### to calculate RMSE ##########\n  res1[i,] <- (fcast$mean-xtest)^2\n  res2[i,] <- (fcast2$mean-xtest)^2\n  \n  \n}\n\nplot(1:12, colMeans(mae1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"MAE\")\nlines(1:12, colMeans(mae2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"Fit1: SARIMA(0,1,1)x(0,0,1)12\",\"Fit2:SARIMA(0,1,1)x(1,0,0)12\"),col=2:4,lty=1)\n\n\n\n\n\n\n\nView Code\n########### RMSE ############\n\nrmse1 =sqrt(colMeans(res1,na.rm=TRUE))\nrmse2=sqrt(colMeans(res2,na.rm=TRUE))\n\n\nplot(1:12, rmse1, type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:12, rmse2, type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"Fit1: SARIMA(0,1,1)x(0,0,1)12\",\"Fit2:SARIMA(0,1,1)x(1,0,0)12\"),col=2:4,lty=1)\n\n\n\n\n\nOverall, Fit 2 pulls ahead with slightly lower errors. So, it looks like Fit 2 is better.\n\n\n\n\n\nFinal Model Fitting\nIn this part, we’re going to fit the best model from our earlier analysis and jot down its equation.\n\nPricesConsumptionProductionImportsExports\n\n\n\n\nView Code\nfit1 <- Arima(prices_ts, order=c(1,1,1), seasonal=list(order=c(2,0,0),period=12))\nsummary(fit1)\n\n\nSeries: prices_ts \nARIMA(1,1,1)(2,0,0)[12] \n\nCoefficients:\n          ar1     ma1     sar1     sar2\n      -0.9618  0.9071  -0.1570  -0.1649\ns.e.   0.0728  0.1003   0.0865   0.0996\n\nsigma^2 = 0.6418:  log likelihood = -255.84\nAIC=521.68   AICc=521.96   BIC=538.53\n\nTraining set error measures:\n                       ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set -0.008181362 0.7918204 0.5223794 -1.36221 11.23393 0.3694543\n                     ACF1\nTraining set -0.001772395\n\n\nModel equation:\n\\[\n(1 - 0.9618B)(1-B)(1-0.1570B^{12}-0.1649B^{24})Y_t =(1+0.9071B)w_t\n\\]\n\n\n\n\nView Code\nfit2 <- Arima(consumption_ts, order=c(2,0,3), seasonal=list(order=c(2,1,2),period=12),method=\"ML\")\nsummary(fit2)\n\n\nSeries: consumption_ts \nARIMA(2,0,3)(2,1,2)[12] \n\nCoefficients:\n         ar1     ar2     ma1      ma2      ma3    sar1     sar2     sma1\n      0.0163  0.9800  0.3435  -0.8232  -0.2451  0.6047  -0.4146  -1.4154\ns.e.  0.0237  0.0238  0.0748   0.0714   0.0781  0.1625   0.0835   0.1879\n        sma2\n      0.5988\ns.e.  0.1635\n\nsigma^2 = 1.071e+10:  log likelihood = -2651.09\nAIC=5322.18   AICc=5323.32   BIC=5355.36\n\nTraining set error measures:\n                  ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 10818.3 98342.46 73926.48 0.3489177 3.235926 0.6323878\n                     ACF1\nTraining set -0.007852977\n\n\nModel equation:\n\\[\n(1 - 0.0163B - 0.9800B^2)(1 - 0.6047B^{12} + 0.4146B^{24})(1 - B^{12})^1Y_t = (1 + 0.3435B - 0.8232B^2 - 0.2451B^3)(1 - 1.4154B^{12} + 0.5988B^{24})w_t\n\\]\n\n\n\n\nView Code\nfit3 <- Arima(production_ts, order=c(0,1,1), seasonal=list(order=c(1,1,1),period=12))\nsummary(fit3)\n\n\nSeries: production_ts \nARIMA(0,1,1)(1,1,1)[12] \n\nCoefficients:\n          ma1     sar1     sma1\n      -0.2318  -0.1148  -0.6329\ns.e.   0.0806   0.0976   0.0785\n\nsigma^2 = 3.033e+09:  log likelihood = -2506.58\nAIC=5021.16   AICc=5021.36   BIC=5034.41\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE      MASE      ACF1\nTraining set 2289.614 52995.22 34932.59 0.07971598 1.319548 0.2680922 0.0231852\n\n\nModel equation:\n\\[\n(1 - B)(1 - B^{12}) Y_t = (1−0.2318B) (1−0.1148B^{12}) (1−0.6329B^{12}) w_t\n\\]\n\n\n\n\nView Code\nfit4 <- Arima(imports_ts, order=c(0,1,1), seasonal=list(order=c(1,0,0),period=12))\nsummary(fit4)\n\n\nSeries: imports_ts \nARIMA(0,1,1)(1,0,0)[12] \n\nCoefficients:\n          ma1    sar1\n      -0.3213  0.6779\ns.e.   0.0637  0.0403\n\nsigma^2 = 316104207:  log likelihood = -3487.41\nAIC=6980.83   AICc=6980.91   BIC=6992.05\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 63.52825 17693.64 14186.81 -0.2091907 4.959858 0.6342818\n                   ACF1\nTraining set 0.02733407\n\n\nModel equation:\n\\[\n(1 - B)Y_t = (1−0.3213B) (1−0.6779B^{12})w_t\n\\]\n\n\n\n\nView Code\nfit5 <- Arima(exports_ts, order=c(0,1,0), seasonal=list(order=c(0,1,3),period=12))\nsummary(fit5)\n\n\nSeries: exports_ts \nARIMA(0,1,0)(0,1,3)[12] \n\nCoefficients:\n         sma1    sma2    sma3\n      -0.5912  0.1420  0.0031\ns.e.   0.0588  0.0829  0.0955\n\nsigma^2 = 280386033:  log likelihood = -3332.9\nAIC=6673.8   AICc=6673.94   BIC=6688.6\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 88.11593 16309.73 9630.989 -0.3729319 9.009833 0.3492677\n                    ACF1\nTraining set -0.08097367\n\n\nModel equation:\n\\[\n(1 - B)(1 - B^{12}) Y_t = (1 + 0.5912 B^{12} - 0.1420 B^{24} - 0.0031 B^{36}) w_t\n\\]\n\n\n\n\n\nSARIMA vs. Benchmark methods\nNow, we are going to compare the best model we chose above with the benchmark methods\n\nPricesConsumptionProductionImportsExports\n\n\n\n\nView Code\nautoplot(prices_ts) +\n  autolayer(meanf(prices_ts, h=12),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(prices_ts, h=12),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(prices_ts, h=12),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(prices_ts, h=12, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit1,12), \n            series=\"fit\",PI=FALSE) +theme_bw() + \n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\nComparing Accuracy Metrics\n\n\nView Code\noutput1 <- accuracy(forecast(fit1,12))\noutput2 <- accuracy(meanf(prices_ts, h=12))\noutput3 <- accuracy(naive(prices_ts, h=12))\noutput4 <- accuracy(rwf(prices_ts, drift=TRUE, h=12))\n\noutput_list <- list(output1, output2, output3, output4)\n\n# Put into df\noutput_df <- do.call(rbind, output_list)\n\n# Set row names for the dataframe\nrow.names(output_df) <- c(\"SARIMA fit\", \"Meanf\", \"Naive\", \"Drift\")\n\noutput_df\n\n\n                      ME      RMSE       MAE        MPE     MAPE      MASE\nSARIMA fit -8.181362e-03 0.7918204 0.5223794  -1.362210 11.23393 0.3694543\nMeanf       1.517356e-16 2.3123213 1.7996103 -23.725893 46.03433 1.2727795\nNaive      -2.883721e-03 0.8138242 0.5097674  -1.144999 10.63560 0.3605345\nDrift      -2.895201e-17 0.8138191 0.5096467  -1.065391 10.62714 0.3604491\n                   ACF1\nSARIMA fit -0.001772395\nMeanf       0.936691654\nNaive      -0.076051915\nDrift      -0.076051915\n\n\nThe Seasonal Naive method’s trend line looked a lot like last year’s. The SARIMA model has lower RMSE so it can be said that it did better than most of the other methods we checked out.\n\n\n\n\nView Code\nautoplot(consumption_ts) +\n  autolayer(meanf(consumption_ts, h=12),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(consumption_ts, h=12),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(consumption_ts, h=12),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(consumption_ts, h=12, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit2,12), \n            series=\"fit\",PI=FALSE) +theme_bw() + \n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\nComparing Accuracy Metrics\n\n\nView Code\noutput1 <- accuracy(forecast(fit2,12))\noutput2 <- accuracy(meanf(consumption_ts, h=12))\noutput3 <- accuracy(naive(consumption_ts, h=12))\noutput4 <- accuracy(rwf(consumption_ts, drift=TRUE, h=12))\n\noutput_list <- list(output1, output2, output3, output4)\n\n# Put into df\noutput_df <- do.call(rbind, output_list)\n\n# Set row names for the dataframe\nrow.names(output_df) <- c(\"SARIMA fit\", \"Meanf\", \"Naive\", \"Drift\")\n\noutput_df\n\n\n                      ME      RMSE       MAE        MPE      MAPE      MASE\nSARIMA fit  1.081830e+04  98342.46  73926.48  0.3489177  3.235926 0.6323878\nMeanf       2.244115e-10 475539.86 388753.66 -4.5394828 18.158360 3.3255073\nNaive       3.786088e+03 301268.55 241294.60 -0.7161268 10.686221 2.0641013\nDrift      -3.142357e-11 301244.76 241185.00 -0.8957034 10.688195 2.0631638\n                   ACF1\nSARIMA fit -0.007852977\nMeanf       0.784939243\nNaive       0.350347620\nDrift       0.350347620\n\n\nThe SARIMA model did better than most of the other methods we checked out. It picked up on the seasonal swings that a lot of the other methods missed. The Seasonal Naive method’s trend line looked a lot like last year’s, but the SARIMA model smoothed out the ups and downs a bit more. Moreover, the RMSE of the SARIMA fit is the lowest.\n\n\n\n\nView Code\nautoplot(production_ts) +\n  autolayer(meanf(production_ts, h=12),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(production_ts, h=12),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(production_ts, h=12),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(production_ts, h=12, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit3,12), \n            series=\"fit\",PI=FALSE) +theme_bw() + \n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\nComparing Accuracy Metrics\n\n\nView Code\noutput1 <- accuracy(forecast(fit3,12))\noutput2 <- accuracy(meanf(production_ts, h=12))\noutput3 <- accuracy(naive(production_ts, h=12))\noutput4 <- accuracy(rwf(production_ts, drift=TRUE, h=12))\n\noutput_list <- list(output1, output2, output3, output4)\n\n# Put into df\noutput_df <- do.call(rbind, output_list)\n\n# Set row names for the dataframe\nrow.names(output_df) <- c(\"SARIMA fit\", \"Meanf\", \"Naive\", \"Drift\")\n\noutput_df\n\n\n                      ME      RMSE       MAE         MPE      MAPE      MASE\nSARIMA fit  2.289614e+03  52995.22  34932.59  0.07971598  1.319548 0.2680922\nMeanf      -1.546615e-10 545658.73 456839.53 -4.14357445 17.582835 3.5060416\nNaive       7.998926e+03 136882.41 103582.25  0.15193387  3.955783 0.7949480\nDrift       1.494482e-10 136648.50 103540.46 -0.16214898  3.957111 0.7946273\n                 ACF1\nSARIMA fit  0.0231852\nMeanf       0.9562452\nNaive      -0.6011544\nDrift      -0.6011544\n\n\nThe SARIMA model did better than most of the other methods we checked out. It picked up on the seasonal swings that a lot of the other methods missed. The Seasonal Naive method’s trend line looked a lot like last year’s, but the SARIMA model smoothed out the ups and downs a bit more. Moreover, the RMSE of the SARIMA fit is the lowest.\n\n\n\n\nView Code\nautoplot(imports_ts) +\n  autolayer(meanf(imports_ts, h=12),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(imports_ts, h=12),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(imports_ts, h=12),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(imports_ts, h=12, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit4,12), \n            series=\"fit\",PI=FALSE) +theme_bw() + \n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\nComparing Accuracy Metrics\n\n\nView Code\noutput1 <- accuracy(forecast(fit4,12))\noutput2 <- accuracy(meanf(imports_ts, h=12))\noutput3 <- accuracy(naive(imports_ts, h=12))\noutput4 <- accuracy(rwf(imports_ts, drift=TRUE, h=12))\n\noutput_list <- list(output1, output2, output3, output4)\n\n# Put into df\noutput_df <- do.call(rbind, output_list)\n\n# Set row names for the dataframe\nrow.names(output_df) <- c(\"SARIMA fit\", \"Meanf\", \"Naive\", \"Drift\")\n\noutput_df\n\n\n                      ME     RMSE      MAE        MPE      MAPE      MASE\nSARIMA fit  6.352825e+01 17693.64 14186.81 -0.2091907  4.959858 0.6342818\nMeanf      -1.794955e-11 54476.42 46530.12 -3.7116764 16.899929 2.0803278\nNaive       4.711254e+01 25084.96 19464.36 -0.3595737  6.791850 0.8702374\nDrift       1.225908e-11 25084.91 19464.21 -0.3766501  6.792339 0.8702307\n                  ACF1\nSARIMA fit  0.02733407\nMeanf       0.89426354\nNaive      -0.18972707\nDrift      -0.18972707\n\n\nThe SARIMA model did better than most of the other methods we checked out. It picked up on the seasonal swings that a lot of the other methods missed. The Seasonal Naive method’s trend line looked a lot like last year’s, but the SARIMA model smoothed out the ups and downs a bit more. Moreover, the RMSE of the SARIMA fit is the lowest.\n\n\n\n\nView Code\nautoplot(exports_ts) +\n  autolayer(meanf(exports_ts, h=12),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(exports_ts, h=12),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(exports_ts, h=12),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(exports_ts, h=12, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit5,12), \n            series=\"fit\",PI=FALSE) +theme_bw() + \n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\nComparing Accuracy Metrics\n\n\nView Code\noutput1 <- accuracy(forecast(fit5,12))\noutput2 <- accuracy(meanf(exports_ts, h=12))\noutput3 <- accuracy(naive(exports_ts, h=12))\noutput4 <- accuracy(rwf(exports_ts, drift=TRUE, h=12))\n\noutput_list <- list(output1, output2, output3, output4)\n\n# Put into df\noutput_df <- do.call(rbind, output_list)\n\n# Set row names for the dataframe\nrow.names(output_df) <- c(\"SARIMA fit\", \"Meanf\", \"Naive\", \"Drift\")\n\noutput_df\n\n\n                      ME      RMSE        MAE          MPE       MAPE      MASE\nSARIMA fit  8.811593e+01  16309.73   9630.989   -0.3729319   9.009833 0.3492677\nMeanf       1.042567e-11 162271.31 124173.292 -213.0207422 241.607017 4.5031422\nNaive       1.881955e+03  22476.64  12884.714    0.1542360  10.800990 0.4672639\nDrift      -2.593253e-12  22397.71  12829.335   -3.5443285  11.476894 0.4652556\n                  ACF1\nSARIMA fit -0.08097367\nMeanf       0.97738243\nNaive      -0.34599428\nDrift      -0.34599428\n\n\nThe SARIMA model did better than most of the other methods we checked out. It picked up on the seasonal swings that a lot of the other methods missed. The Seasonal Naive method’s trend line looked a lot like last year’s, but the SARIMA model smoothed out the ups and downs a bit more. Moreover, the RMSE of the SARIMA fit is the lowest.\n\n\n\n\n\nForecasting\nFinally, we’ll forecast and plot the forecast for the next 3 years using our final fit.\n\nPricesConsumptionProductionImportsExports\n\n\n\n\nView Code\n# forecast next three years\nfit1 %>% forecast(h=36) %>% autoplot(main = \"Natual Gas Prices Prediction\",fcol = '#92c54b') +\n  ylab(\"Prices\") + xlab(\"Time\")+ theme_bw() \n\n\n\n\n\n\n\n\n\nView Code\n# forecast next three years\nfit2 %>% forecast(h=36) %>% autoplot(main = \"Natual Gas Consumption Prediction\",fcol = '#92c54b') +\n  ylab(\"Consumption\") + xlab(\"Time\")+ theme_bw() \n\n\n\n\n\n\n\n\n\nView Code\n# forecast next three years\nfit3 %>% forecast(h=36) %>% autoplot(main = \"Natual Gas Production Prediction\",fcol = '#92c54b') +\n  ylab(\"Production\") + xlab(\"Time\")+ theme_bw() \n\n\n\n\n\n\n\n\n\nView Code\n# forecast next three years\nfit4 %>% forecast(h=36) %>% autoplot(main = \"Natual Gas Imports Prediction\",fcol = '#92c54b') +\n  ylab(\"Imports\") + xlab(\"Time\")+ theme_bw() \n\n\n\n\n\n\n\n\n\nView Code\n# forecast next three years\nfit5 %>% forecast(h=36) %>% autoplot(main = \"Natual Gas Exports Prediction\",fcol = '#92c54b') +\n  ylab(\"Exports\") + xlab(\"Time\")+ theme_bw() \n\n\n\n\n\n\n\n\nThe forecasted values come from the model’s parameters, which we figured out by looking at past data. To see how accurate these forecasts are, we can use cross-validation. This means we split the data into a part for training and another part for testing. Then we see how well the model’s predictions stack up against the real numbers in the test set. This step is key for spotting any issues with the model and tweaking the parameters to get things just right.\n\n\nSeasonal Cross-Validation\n\nPricesConsumptionProductionImportsExports\n\n\n\n12-step ahead CV1-step ahead CV\n\n\n\n\nView Code\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,1),seasonal=c(2,0,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(prices_ts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point(color=\"#92c54b\")+geom_line(color=\"#92c54b\")+ggtitle(\"12 Step Ahead Cross Validation\")+ theme_bw()\n\n\n\n\n\n\n\n\n\nView Code\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,1),seasonal=c(2,0,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(prices_ts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point(color=\"#92c54b\")+geom_line(color=\"#92c54b\")+ggtitle(\"1 Step Ahead Cross Validation\")+theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n12-step ahead CV1-step ahead CV\n\n\n\n\nView Code\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(2,0,3),seasonal=c(2,1,2)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(consumption_ts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point(color=\"#92c54b\")+geom_line(color=\"#92c54b\")+ggtitle(\"12 Step Ahead Cross Validation\")+theme_bw()\n\n\n\n\n\n\n\n\n\nView Code\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(2,0,3),seasonal=c(2,1,2)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(consumption_ts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point(color=\"#92c54b\")+geom_line(color=\"#92c54b\")+ggtitle(\"1 Step Ahead Cross Validation\")+theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n12-step ahead CV1-step ahead CV\n\n\n\n\nView Code\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,1),seasonal=c(1,1,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(production_ts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point(color=\"#92c54b\")+geom_line(color=\"#92c54b\")+ggtitle(\"12 Step Ahead Cross Validation\")+theme_bw()\n\n\n\n\n\n\n\n\n\nView Code\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,1),seasonal=c(1,1,1)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(production_ts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point(color=\"#92c54b\")+geom_line(color=\"#92c54b\")+ggtitle(\"1 Step Ahead Cross Validation\")+theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n12-step ahead CV1-step ahead CV\n\n\n\n\nView Code\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,1),seasonal=c(1,0,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(imports_ts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point(color=\"#92c54b\")+geom_line(color=\"#92c54b\")+ggtitle(\"12 Step Ahead Cross Validation\")+theme_bw()\n\n\n\n\n\n\n\n\n\nView Code\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,1),seasonal=c(1,0,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(imports_ts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point(color=\"#92c54b\")+geom_line(color=\"#92c54b\")+ggtitle(\"1 Step Ahead Cross Validation\")+theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n12-step ahead CV1-step ahead CV\n\n\n\n\nView Code\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0),seasonal=c(0,1,3)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(exports_ts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point(color=\"#92c54b\")+geom_line(color=\"#92c54b\")+ggtitle(\"12 Step Ahead Cross Validation\")+theme_bw()\n\n\n\n\n\n\n\n\n\nView Code\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0),seasonal=c(0,1,3)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(exports_ts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point(color=\"#92c54b\")+geom_line(color=\"#92c54b\")+ggtitle(\"1 Step Ahead Cross Validation\")+theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nThe MSE tends to rise by 12 steps at each step of cross-validation it suggests that there’s a pattern of increasing error which indicates that the model becomes less accurate at predicting values one year ahead. However, the one-step-ahead forecasting plots show that the MSE seems to be stable. Indicating that our model’s predictions are consistently close to the actual values for each subsequent time point in the series."
  },
  {
    "objectID": "dl.html",
    "href": "dl.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "On this page, I will be exploring deep learning methodologies for time series analysis. I’m going to implement various deep learning models like RNN, GRUs and LSTMs, with and without L2 regularization using structured approach outlined on this page. Finally, I’ll compare the results of deep learning models with traditional TS models used across this project."
  },
  {
    "objectID": "intro.html#introduction",
    "href": "intro.html#introduction",
    "title": "Analysis of Natural Gas Industry in the US",
    "section": "Introduction",
    "text": "Introduction\nIn the big picture of America’s energy scene, natural gas is like a star player – it’s affordable, does a good job, and is kinder to the environment compared to old-school fuels. They call it a “bridge fuel,” helping us move from the old ways of powering things to a cleaner, greener future. It’s a practical choice that keeps the lights on while we figure out the renewable energy stuff. And natural gas isn’t just about making power; it’s woven into everything, influencing how we do business, sparking new ideas, and even playing a role in how countries work together. So, when we talk about natural gas, it’s not just about keeping the energy flowing – it’s about shaping how we live and where we’re headed.\nZooming in on the U.S. natural gas scene, you realize it’s not just about drilling and pipelines. It’s part of everyday life, from heating our homes to fueling the machines that make stuff. But its impact goes even deeper, affecting jobs, technology, and even how countries relate to each other. Natural gas isn’t just a player in the energy game; it’s like a backstage VIP, pulling strings that touch everything from local businesses to global affairs. So, as the way we power up changes, natural gas keeps holding its own, being a key player in the U.S. story – past, present, and future."
  },
  {
    "objectID": "intro.html#literature-review",
    "href": "intro.html#literature-review",
    "title": "Analysis of Natural Gas Industry in the US",
    "section": "Literature review",
    "text": "Literature review\nNowhere is the influence of natural gas more pronounced than in the United States. As both the world’s largest producer and consumer of natural gas, the nation relies heavily on this resource. Reflecting its significance, natural gas claims a substantial 33% share in the U.S. energy consumption landscape, making it a cornerstone in the nation’s energy portfolio. Source\n\n\n\n\n\nThe applications of natural gas are diverse, ranging from electricity generation to fueling vehicles, heating homes, and cooking. Significantly, it is used for heating, electricity generation, and industrial processes in the U.S., highlighting its pivotal role in the nation’s power infrastructure. Source\n\n\n\n\n\nBeyond its energy applications, the natural gas industry is an economic powerhouse. It supports 10.3 million jobs and contributes nearly 8% to the U.S. Gross Domestic Product (GDP), playing a significant role in the nation’s economic fabric. Source\nRecent Record-Breaking Consumption\n\n\n\nimage by: GETTY IMAGES/ISTOCKPHOTO\n\n\nIn a remarkable turn of events, the severe U.S. winter weather in mid-January 2024 set new records for natural gas consumption. On January 16, 2024, consumption reached an unprecedented monthly high while the prices for the heating fuel have dropped to their lowest since April. This real-world event offers a tangible backdrop to the broader exploration, showcasing how the industry responds to and is influenced by external factors. Source"
  },
  {
    "objectID": "intro.html#explore-the-big-picture",
    "href": "intro.html#explore-the-big-picture",
    "title": "Analysis of Natural Gas Industry in the US",
    "section": "Explore the big picture",
    "text": "Explore the big picture"
  },
  {
    "objectID": "intro.html#scope-of-the-analysis",
    "href": "intro.html#scope-of-the-analysis",
    "title": "Analysis of Natural Gas Industry in the US",
    "section": "Scope of the Analysis",
    "text": "Scope of the Analysis\nThis comprehensive analysis embarks on a multifaceted journey. It seeks to unravel the correlation between natural gas industry dynamics and key economic indicators. A focus on GDP and the employment rate will illuminate the industry’s influence on the broader economic health of the nation.\n\n\n\nImage source: American Petroleum Institute\n\n\nDelving into Price Dynamics, the study investigates the impact of natural gas prices on consumption and production patterns across sectors. Understanding the intricate relationship between pricing mechanisms and consumer/producer behavior will add depth to the analysis.\nExtending into the Financial Markets, the analysis scrutinizes the stock market performance of major energy companies involved in natural gas production. This facet provides insights into the industry’s financial resilience and responsiveness to market dynamics.\n\n\n\n\n\nA crucial facet of the exploration lies in understanding the Environmental Impact of natural gas consumption. By correlating natural gas usage with carbon emissions, the analysis contributes to the ongoing discourse on sustainable energy practices.\n\n\n\n\n\nTrade dynamics are also a pivotal element of this study, analyzing the trade relationships related to natural gas contributes a comprehensive understanding of the industry’s impact on global economic interactions."
  },
  {
    "objectID": "intro.html#research-questions",
    "href": "intro.html#research-questions",
    "title": "Analysis of Natural Gas Industry in the US",
    "section": "Research questions",
    "text": "Research questions\n\nHow does natural gas consumption correlate with changes in GDP, and what economic implications does this correlation have for the overall economic health of the nation?\nIn what ways does natural gas consumption influence the employment rate, and what patterns or trends emerge when analyzing the relationship between natural gas industry dynamics and employment levels?\nHow do shifts in natural gas pricing mechanisms affect consumer behavior, particularly in terms of energy consumption patterns?\nHow do pricing mechanisms influence producer behavior and what are the potential long-term effects on the overall economic health of the nation?\nHow do environmental factors, including weather conditions, impact natural gas consumption, and how can this impact be visualized to inform sustainable energy practices?\nWhat insights can be gained from analyzing the stock market performance of major energy companies involved in natural gas production, and how does this performance reflect broader trends in the industry?\nWhat key considerations impact investment trends related to natural gas infrastructure, and how do these investments contribute to the long-term sustainability of the industry?\nHow does the relationship between natural gas consumption and trade dynamics vary among regions, and what insights can be gained about the geopolitical implications of these variations?\nWhat role do import/export dynamics play in the natural gas industry, and how do these factors influence global economic interactions?\nHow can Deep Learning for Time Series enhance the ability predict future trends in natural gas consumption?"
  },
  {
    "objectID": "financial.html",
    "href": "financial.html",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "",
    "text": "On this page, I’ll gather stock data for key players in the natural gas sector. Then, I’ll dive into modeling and forecasting future return volatility, using what we call conditional heteroscedastic models. This analysis isn’t just about numbers; it’s about understanding the financial well-being and stability of top natural gas companies. It’s a tool for spotting trends and patterns in the industry, helping us identify when the market’s smooth sailing and when it’s choppy waters.\nLet’s talk about the companies I’ve picked for this analysis. First up, we’ve got Chevron Corp (NYSE:CVX), a major player in the energy sector with a significant focus on natural gas production. Known for its global presence and strong financial performance, Chevron is a cornerstone in the industry. Then there’s BP (LON:BP), another heavyweight in the energy game. With a diversified portfolio that includes natural gas operations, BP brings its own unique perspective to the table. Together, these companies offer a comprehensive view of the natural gas industry’s landscape, making them perfect candidates for our analysis. Know more"
  },
  {
    "objectID": "financial.html#financial-time-series-plots",
    "href": "financial.html#financial-time-series-plots",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Financial Time Series Plots",
    "text": "Financial Time Series Plots\n\nChevron CorpBP\n\n\n\n\nView Code\ng4<- ggplot(cvx, aes(x=Date)) +\n  geom_line(aes(y=CVX.Adjusted, colour=\"CVX\"))+\n   labs(title = paste(\"Stock Prices of Chevron Corp\"),x = \"Date\", y = \"Adjusted Closing Prices\")+\n    scale_color_manual(values = c(\"CVX\" = \"#92c54b\"),name = \"Ticker Symbol\")+\n    theme_bw()+ \n    theme(plot.title = element_text(hjust = 0.5))\n\nggplotly(g4) %>%\n  layout(hovermode = \"x\")\n\n\n\n\n\n\nForm the plot, there’s a pronounced upward trend starting from 2000 to around 2008, where it peaks before experiencing a sharp decline. This decline is likely linked to the 2008 financial crisis, which affected most stock prices globally. After 2008, the stock price recovers, with some volatility but maintains an upward trajectory until around 2014, after which it appears to enter a period of decline or correction. This could be due the significant drop in global oil prices that made 2014-2015 a challenging year for Chevron and the entire oil and natural gas industry, reducing earnings across the sector. The trend from around 2016 shows recovery and continued growth, with a notable surge starting from 2020 onward after COVID-19 pandemic. Overall, despite periods of decline and volatility, the long-term trend for Chevron Corp’s stock appears to be upward.\n\n\nView Code\ncvx$SMA_50 <- as.numeric(SMA(Cl(cvx),n=50))\ncvx$SMA_200 <- as.numeric(SMA(Cl(cvx),n=200))\n\n#candle stick plot\nfig <- cvx %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~CVX.Open, close = ~CVX.Close,\n          high = ~CVX.High, low = ~CVX.Low, name = \"Candle Stick\") \n\n# MA plot\n\nfig <-fig %>%  add_lines(x = ~Date, y = ~SMA_50, name = \"50-MA\",\n            line = list(color = '#501d4a', width = 0.7),\n            hoverinfo = \"none\", inherit = F)  %>% add_lines(x = ~Date, y = ~SMA_200, name = \"200-MA\",\n            line = list(color = '#ff4d00', width = 0.7),\n            hoverinfo = \"none\", inherit = F) \n\nfig <- fig %>% layout(yaxis = list(title = \"Stock Price\"))\n\n\n# define colors for increasing and decreasing\nfor (i in 1:length(cvx[,1])) {\n  if (cvx$CVX.Close[i] >= cvx$CVX.Open[i]) {\n      cvx$direction[i] = 'Increasing'\n  } else {\n      cvx$direction[i] = 'Decreasing'\n  }\n}\n\n# Volumne plot\nfig2 <- cvx  %>% plot_ly(x=~Date, y=~CVX.Volume, type='bar', name = \"Volume\",\n          color = ~direction, colors = c('#3d9970','#ff4136'), showlegend = FALSE) %>% layout(yaxis = list(title = \"Volume\"))\n\n# combine both the plots\nfig <-subplot(fig, fig2, nrows = 2, heights = c(0.7, 0.2), shareX = TRUE, titleY = TRUE)\n\nfig <- fig %>% layout(title = paste(\"CVX Candle Stick Plot With MA And Volume\"))%>% \n  layout(hovermode = \"x\")\nfig\n\n\n\n\n\n\nThe candlestick plot of Chevron Corp’s (CVX) stock price, augmented with a 50-day moving average (MA) and a 200-day MA, which help identify short-term and long-term trends, respectively. The initial segment of the chart shows a relatively stable increase in price from about 2000 until a peak around 2008, followed by a significant drop that is likely due to the global financial crisis. Post-crisis, the stock price recovers, with fluctuations around the 50-day MA until approximately 2014, suggesting short-term volatility but a generally stable trend. From 2014 to 2020, the price exhibits more volatility, as indicated by the wider swings around the moving averages and the increased distance between the candlesticks and the MAs. This period includes sharp drops and rapid recoveries, which could correspond to market responses to oil price changes, corporate actions, or broader economic events. Around 2020, there’s another sharp decline followed by a strong recovery, with the price rising steeply and consistently staying above the 50-day and 200-day MAs, suggesting a bullish outlook for this period. The volume plot shows trading volume over time. Notably high volume spikes often coincide with significant price movements, providing clues about market sentiment and potential trend reversals. Overall, the trend across this time frame is marked by periods of growth and retraction with a recent sharp upward trend, reflecting a bullish sentiment in the market for CVX.\n\n\n\n\nView Code\ng4<- ggplot(bp, aes(x=Date)) +\n  geom_line(aes(y=BP.Adjusted, colour=\"BP\"))+\n   labs(title = paste(\"Stock Prices of BP\"),x = \"Date\", y = \"Adjusted Closing Prices\")+\n    scale_color_manual(values = c(\"BP\" = \"#92c54b\"),name = \"Ticker Symbol\")+\n    theme_bw()+ \n    theme(plot.title = element_text(hjust = 0.5))\n\nggplotly(g4) %>%\n  layout(hovermode = \"x\")\n\n\n\n\n\n\nThe plot shows a trend with notable fluctuations over the two-decade period. From an initial glance, there is no clear, consistent upward or downward trend over the entire period. Instead, there are periods of both inclines and declines. A more detailed analysis could potentially reveal shorter bullish or bearish phases. There is a significant drop in the stock price visible around the year 2010, which could correspond to the Deepwater Horizon oil spill that occurred in the Gulf of Mexico, a major environmental disaster that impacted BP’s stock value heavily. Post-2010, the stock seems to recover, trending upwards for a few years before experiencing other periods of volatility. Around 2020, there’s another sharp decline which could be related to the economic impact of the COVID-19 pandemic. Overall, while there are periods of recovery and growth, the long-term trend doesn’t show a sustained upward trajectory. Instead, the stock has experienced significant volatility with periods of both appreciation and depreciation in value.\n\n\nView Code\nbp$SMA_50 <- as.numeric(SMA(Cl(bp),n=50))\nbp$SMA_200 <- as.numeric(SMA(Cl(bp),n=200))\n\n#candle stick plot\nfig <- bp %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~BP.Open, close = ~BP.Close,\n          high = ~BP.High, low = ~BP.Low, name = \"Candle Stick\") \n\n# MA plot\n\nfig <-fig %>%  add_lines(x = ~Date, y = ~SMA_50, name = \"50-MA\",\n             line = list(color = '#501d4a', width = 0.7),\n            hoverinfo = \"none\", inherit = F)  %>% add_lines(x = ~Date, y = ~SMA_200, name = \"200-MA\",\n            line = list(color = '#ff4d00', width = 0.7),\n            hoverinfo = \"none\", inherit = F) \n\nfig <- fig %>% layout(yaxis = list(title = \"Stock Price\"))\n\n\n# define colors for increasing and decreasing\nfor (i in 1:length(bp[,1])) {\n  if (bp$BP.Close[i] >= bp$BP.Open[i]) {\n      bp$direction[i] = 'Increasing'\n  } else {\n      bp$direction[i] = 'Decreasing'\n  }\n}\n\n# Volumne plot\nfig2 <- bp  %>% plot_ly(x=~Date, y=~BP.Volume, type='bar', name = \"Volume\",\n          color = ~direction, colors = c('#3d9970','#ff4136'), showlegend = FALSE) %>% layout(yaxis = list(title = \"Volume\"))\n\n# combine both the plots\nfig <-subplot(fig, fig2, nrows = 2, heights = c(0.7, 0.2), shareX = TRUE, titleY = TRUE)\n\nfig <- fig %>% layout(title = paste(\"BP Candle Stick Plot With MA And Volume\"))%>% \n  layout(hovermode = \"x\")\nfig\n\n\n\n\n\n\nThe candlestick graph of BP’s stock prices along with the 50-day and 200-day moving averages are used to determine the stock’s trend direction over time. The 50-day moving average (50-MA) is used to gauge the intermediate trend. In this chart, it fluctuates frequently above and below the 200-day moving average (200-MA), colored red, indicating no clear dominance of either a bullish or bearish trend over the examined period. The 200-day moving average is used to identify the long-term trend. If prices are above the 200-MA, it’s generally considered a bullish signal, and if they’re below, it’s considered bearish. Here, we see the stock price cross this average several times, reflecting a market that lacks a definitive long-term trend and suggests periods of uncertainty or market correction. The volume bars highlight trading activity. Spikes in volume, especially those corresponding with significant price movements, can suggest key events mentioned below the first graph. In summary, the trend over this period shows volatility with no persistent upward or downward direction, characterized by fluctuations around the moving averages. The latter part of the chart shows a recovery with the stock price moving above both moving averages, potentially signaling a change to a bullish phase."
  },
  {
    "objectID": "financial.html#returns-of-financial-time-series",
    "href": "financial.html#returns-of-financial-time-series",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Returns of Financial Time Series",
    "text": "Returns of Financial Time Series\n\nChevron CorpBP\n\n\n\n\nView Code\ncvx_ts <- ts(cvx$CVX.Adjusted, start=decimal_date(as.Date(\"2000-01-01\")), frequency = 365.25)\nreturns_cvx = diff(log(cvx_ts))\nautoplot(returns_cvx, color=\"#92c54b\")+ggtitle(\"Chevron Corp Returns\")+ theme_bw()\n\n\n\n\n\nThe graph suggests the presence of volatility clustering, a common phenomenon in financial time series. Volatility clustering means that periods of high volatility, where returns show significant fluctuations from one period to the next, tend to be followed by more high volatility periods. Conversely, periods of low volatility tend to cluster together as well. In the chart, we can observe that there are several instances where the returns are tightly packed together near the zero line, indicating relatively stable periods with small daily returns. In contrast, there are also clusters of large spikes extending above and below the zero line, indicating turbulent periods where the stock price experienced significant changes from one day to the next. This behavior is particularly evident around the large negative spike, where a period of high volatility seems to have been followed by more volatility. Similarly, after calmer periods, the chart doesn’t immediately jump to high volatility; it transitions into it, which is also a characteristic of volatility clustering. Understanding this can be crucial for financial risk management and necessitates models that can account for such changing volatility, like GARCH models, rather than those assuming constant volatility.\n\n\n\n\nView Code\nbp_ts <- ts(bp$BP.Adjusted, start=decimal_date(as.Date(\"2000-01-01\")), frequency = 365.25)\nreturns_bp = diff(log(bp_ts))\nautoplot(returns_bp, color=\"#92c54b\")+ggtitle(\"BP Returns\")+ theme_bw()\n\n\n\n\n\nLike the previous Chevron Corp graph, this graph for “BP Returns” demonstrates volatility clustering, where the variance in returns appears to be dependent on previous variance, resulting in periods where high volatility is concentrated and followed by more high volatility, and vice versa for low volatility. These clusters of spikes suggest a non-random pattern, indicating that the returns are not evenly spread out but rather occur in streaks, which is typical in financial markets and particularly relevant for entities engaged in trading or risk management."
  },
  {
    "objectID": "financial.html#acf-and-pacf-of-returns",
    "href": "financial.html#acf-and-pacf-of-returns",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "ACF and PACF of returns",
    "text": "ACF and PACF of returns\n\nChevron CorpBP\n\n\n\n\nView Code\nggAcf(returns_cvx,40)+ ggtitle(\"ACF Plot for CVX Stock Price Returns\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\n\n\nView Code\nggPacf(returns_cvx,40)+ ggtitle(\"PACF Plot for CVX Stock Price Returns\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\nFrom the plots, we can observe that most of the autocorrelation coefficients fall within the confidence interval boundaries, indicating that there is very little linear relationship between the stock returns and their past values beyond what would be expected by random chance. This lack of significant autocorrelation at almost all lags suggests that the CVX stock returns are stationary, meaning that their statistical properties, such as mean, variance, and autocorrelation, do not depend on the time at which the series is observed.\n\n\n\n\nView Code\nggAcf(returns_bp,40)+ ggtitle(\"ACF Plot for BP Stock Price Returns\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\n\n\nView Code\nggPacf(returns_bp,40)+ ggtitle(\"PACF Plot for BP Stock Price Returns\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\nIn the ACF plot, you can see that most autocorrelations are within the confidence bounds, which could suggest that there is no significant autocorrelation at most lags. However, there are some spikes outside of the confidence bounds at certain lags, which could indicate some periodic effects or other forms of autocorrelation that could affect stationarity.\nThe PACF plot has significant spikes at the first few lags might suggest an AR component in the time series model. However, the significance of the autocorrelation at the first lag alone in the PACF could also be indicative of a non-stationary series."
  },
  {
    "objectID": "financial.html#acf-and-pacf-of-absolute-returns",
    "href": "financial.html#acf-and-pacf-of-absolute-returns",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "ACF and PACF of absolute returns",
    "text": "ACF and PACF of absolute returns\n\nChevron CorpBP\n\n\n\n\nView Code\nggAcf(abs(returns_cvx),40)+ ggtitle(\"ACF Plot for CVX Stock Price Absolute Returns\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\n\n\nView Code\nggPacf(abs(returns_cvx),40)+ ggtitle(\"PACF Plot for CVX Stock Price Absolute Returns\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\nThe ACF plot of the absolute returns shows significant positive autocorrelation at multiple lags, which gradually decreases as the lags increase. This is indicative of volatility clustering, where large changes in stock prices (regardless of direction) tend to be followed by large changes, and small changes tend to be followed by small changes.\nThe PACF plot shows a few initial significant spikes, particularly at the first lag, but these drop off more quickly than in the ACF plot. The significance in the first lag and the rapid decline in the following lags can indicate a potential AR(1) process in the volatility.\n\n\n\n\nView Code\nggAcf(abs(returns_bp),40)+ ggtitle(\"ACF Plot for BP Stock Price Absolute Returns\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\n\n\nView Code\nggPacf(abs(returns_bp),40)+ ggtitle(\"PACF Plot for BP Stock Price Absolute Returns\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\nThe ACF and PACF plots for the absolute returns of BP (British Petroleum) stock show significant autocorrelations that extend across many lags in the ACF plot. This suggests a strong presence of volatility clustering—large changes tend to be followed by large changes, and small by small, over an extended period. The fact that these correlations remain positive and slowly decay as the lags increase is indicative of persistent volatility.\nThe PACF plot shows significant partial autocorrelations at a few lags near the beginning but these become less significant as the lag increases. This pattern in the PACF could imply an autoregressive conditional heteroskedasticity (ARCH) effect, where past volatility impacts future volatility."
  },
  {
    "objectID": "financial.html#acf-and-pacf-of-squared-returns",
    "href": "financial.html#acf-and-pacf-of-squared-returns",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "ACF and PACF of squared returns",
    "text": "ACF and PACF of squared returns\n\nChevron CorpBP\n\n\n\n\nView Code\nggAcf(returns_cvx^2,40)+ ggtitle(\"ACF Plot for CVX Stock Price Squared Returns\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\n\n\nView Code\nggPacf(returns_cvx^2,40)+ ggtitle(\"PACF Plot for CVX Stock Price Squared Returns\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\nThe ACF and PACF plots for the squared returns of CVX (Chevron Corporation) stock are indicative of the characteristics of the volatility, rather than the price returns themselves. The ACF plot displays a slowly declining positive correlation that persists across several lags, a hallmark of volatility clustering and a common feature in financial time series. This persistence suggests that large changes in returns are likely to be followed by large changes, regardless of the direction, which implies a certain level of predictability in the volatility pattern.\nThe PACF plot for the squared returns shows significant correlations at a few early lags, but these correlations quickly become insignificant as the lags increase, suggesting that the autoregressive part of the volatility is short-lived.\n\n\n\n\nView Code\nggAcf(returns_bp^2,40)+ ggtitle(\"ACF Plot for BP Stock Price Squared Returns\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\n\n\nView Code\nggPacf(returns_bp^2,40)+ ggtitle(\"PACF Plot for BP Stock Price Squared Returns\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\nThe ACF and PACF plots for the squared returns of BP (British Petroleum) stock show the autocorrelation pattern of volatility. In the ACF plot, the gradually decreasing positive correlations across lags again suggest the presence of volatility clustering, with the initial lags displaying a higher degree of correlation which is characteristic of financial time series. This indicates that periods of high volatility are likely to be followed by high volatility, and low by low.\nThe PACF plot shows a few significant lags near the start, but these quickly decrease and mostly become insignificant, which might suggest a short-term autoregressive effect on volatility."
  },
  {
    "objectID": "financial.html#arch-test",
    "href": "financial.html#arch-test",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "ARCH Test",
    "text": "ARCH Test\nThe ARCH LM test is used to detect the presence of ARCH effects in a time series, which is a form of time-varying volatility often found in financial data.\n\nChevron CorpBP\n\n\n\n\nView Code\nArchTest(returns_cvx, lags=7, demean=TRUE)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  returns_cvx\nChi-squared = 1517.7, df = 7, p-value < 2.2e-16\n\n\nGiven that the p-value < 2.2e-16, we can reject the null hypothesis with high confidence. Therefore, the test suggests that there are significant ARCH effects present in the CVX stock returns, indicating that the volatility of the stock returns is changing over time and should be modeled accordingly, potentially with an ARCH or GARCH model.\n\n\n\n\nView Code\nArchTest(returns_bp, lags=11, demean=TRUE)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  returns_bp\nChi-squared = 1205, df = 11, p-value < 2.2e-16\n\n\nGiven that the p-value < 2.2e-16, we can reject the null hypothesis with high confidence. Therefore, the test suggests that there are significant ARCH effects present in the BP stock returns, indicating that the volatility of the stock returns is changing over time and should be modeled accordingly, potentially with an ARCH or GARCH model."
  },
  {
    "objectID": "financial.html#arima-model-fitting",
    "href": "financial.html#arima-model-fitting",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "ARIMA Model Fitting",
    "text": "ARIMA Model Fitting\n\nACF AND PACF Plots\n\nChevron CorpBP\n\n\nLog Transformation\n\n\nView Code\nggAcf(log(cvx_ts),40) + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\n\n\nView Code\nggPacf(log(cvx_ts),40) + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\nDifferenced Transformation\n\n\nView Code\nggAcf(diff(cvx_ts),40) + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\n\n\nView Code\nggPacf(diff(cvx_ts),40) + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\nDifferenced Log Transformation\n\n\nView Code\nggAcf(diff(log(cvx_ts)),40) + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\n\n\nView Code\nggPacf(diff(log(cvx_ts)),40) + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\nChevron Corporation’s stock data only reaches weak stationarity when we apply two transformations: differencing and taking the log of the stock prices. Unaltered, the data tracks closely with a random walk, aligning with an ARIMA(0,1,0) model. For an accurate model of the log-transformed data, we should use an ARIMA model with one level of differencing, represented as ARIMA(p,1,d).\n\n\nLog Transformation\n\n\nView Code\nggAcf(log(bp_ts),40) + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\n\n\nView Code\nggPacf(log(bp_ts),40) + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\nDifferenced Transformation\n\n\nView Code\nggAcf(diff(bp_ts),40) + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\n\n\nView Code\nggPacf(diff(bp_ts),40) + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\nDifferenced Log Transformation\n\n\nView Code\nggAcf(diff(log(bp_ts)),40) + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\n\n\nView Code\nggPacf(diff(log(bp_ts)),40) + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\n\n\n\n\nSimilarly, British Petroleum’s stock data only reaches weak stationarity when we apply two transformations: differencing and taking the log of the stock prices. Unaltered, the data tracks closely with a random walk, aligning with an ARIMA(0,1,0) model. For an accurate model of the log-transformed data, we should use an ARIMA model with one level of differencing, represented as ARIMA(p,1,d).\n\n\n\n\n\nManual ARIMA Model Selectoin\n\nChevron CorpBP\n\n\nSelected Parameters\np:4 d:1 q:4\n\n\nView Code\n######################## Check for different combinations ########\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*32),nrow=32) # roughly nrow = 3x4x2\n\n\nfor (p in 1:4)# p=1,2,3\n{\n  for(q in 1:4)# q=1,2,3\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(log(cvx_ts),order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp[which.min(temp$AIC),]\n\n\n   p d q       AIC       BIC     AICc\n23 2 0 3 -30329.82 -30276.52 -30329.8\n\n\n\n\nView Code\ntemp[which.min(temp$BIC),]\n\n\n   p d q       AIC       BIC      AICc\n10 1 1 0 -30321.56 -30301.57 -30321.56\n\n\n\n\nView Code\ntemp[which.min(temp$AICc),]\n\n\n   p d q       AIC       BIC     AICc\n23 2 0 3 -30329.82 -30276.52 -30329.8\n\n\nAccording to lowest AIC, BIC and AICc, the optimal AIRMA models are as follows:\nModel 1 (Best Model): ARIMA(2,0,3)\nModel 2 (Second Best Model): ARIMA(1,1,0)\n\n\nSelected Parameters\np:4 d:1 q:4\n\n\nView Code\n######################## Check for different combinations ########\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*32),nrow=32) # roughly nrow = 3x4x2\n\n\nfor (p in 1:4)# p=1,2,3\n{\n  for(q in 1:4)# q=1,2,3\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(log(bp_ts),order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp[which.min(temp$AIC),]\n\n\n   p d q       AIC       BIC      AICc\n21 2 0 2 -29331.93 -29285.28 -29331.91\n\n\n\n\nView Code\ntemp[which.min(temp$BIC),]\n\n\n  p d q       AIC       BIC      AICc\n2 0 1 0 -29309.22 -29295.89 -29309.22\n\n\n\n\nView Code\ntemp[which.min(temp$AICc),]\n\n\n   p d q       AIC       BIC      AICc\n21 2 0 2 -29331.93 -29285.28 -29331.91\n\n\nAccording to lowest AIC, BIC and AICc, the optimal AIRMA models are as follows:\nModel 1 (Best Model): ARIMA(2,0,2)\nModel 2 (Second Best Model): ARIMA(0,1,0)\n\n\n\n\n\nAutomated ARIMA Model Selection\n\nChevron CorpBP\n\n\n\n\nView Code\nauto.arima(log(cvx_ts))\n\n\nSeries: log(cvx_ts) \nARIMA(0,1,1) with drift \n\nCoefficients:\n         ma1  drift\n      -0.088  4e-04\ns.e.   0.013  2e-04\n\nsigma^2 = 0.00031:  log likelihood = 15163.71\nAIC=-30321.43   AICc=-30321.42   BIC=-30301.44\n\n\nModel 3 (Best Auto Arima Model): ARIMA(0,1,1)\n\n\n\n\nView Code\nauto.arima(log(bp_ts))\n\n\nSeries: log(bp_ts) \nARIMA(0,1,0) \n\nsigma^2 = 0.0003692:  log likelihood = 14656.54\nAIC=-29311.07   AICc=-29311.07   BIC=-29304.41\n\n\nModel 3 (Best Auto Arima Model): ARIMA(0,1,0)\n\n\n\n\n\nResidual Diagnostics\n\nChevron CorpBP\n\n\n\nModel 1Model 2Model 3\n\n\n\n\nView Code\nmodel_output1 <-capture.output(sarima(log(cvx_ts), 2,0,3))\n\n\n\n\n\n\n\nView Code\ncat(model_output1[49:64], model_output1[length(model_output1)], sep = \"\\n\")\n\n\nconverged\n<><><><><><><><><><><><><><>\n \nCoefficients: \n      Estimate     SE t.value p.value\nar1     0.2258 0.0804  2.8072  0.0050\nar2     0.7740 0.0804  9.6258  0.0000\nma1     0.6881 0.0812  8.4710  0.0000\nma2    -0.0629 0.0174 -3.6070  0.0003\nma3     0.0272 0.0136  2.0045  0.0451\nxmean   3.8843 0.8707  4.4609  0.0000\n\nsigma^2 estimated as 0.000309608 on 5781 degrees of freedom \n \nAIC = -5.238504  AICc = -5.238501  BIC = -5.230444 \n \n \n\n\n\n\n\n\nView Code\nmodel_output2 <-capture.output(sarima(log(cvx_ts), 1,1,0))\n\n\n\n\n\n\n\nView Code\ncat(model_output2[13:21], model_output2[length(model_output2)], sep = \"\\n\")\n\n\nCoefficients: \n         Estimate     SE t.value p.value\nar1       -0.0883 0.0131 -6.7413  0.0000\nconstant   0.0004 0.0002  1.8653  0.0622\n\nsigma^2 estimated as 0.0003098505 on 5784 degrees of freedom \n \nAIC = -5.240505  AICc = -5.240505  BIC = -5.237051 \n \n \n\n\n\n\n\n\nView Code\nmodel_output3 <-capture.output(sarima(log(cvx_ts), 0,1,1))\n\n\n\n\n\n\n\nView Code\ncat(model_output3[13:21], model_output3[length(model_output3)], sep = \"\\n\")\n\n\nCoefficients: \n         Estimate     SE t.value p.value\nma1       -0.0880 0.0130 -6.7607  0.0000\nconstant   0.0004 0.0002  1.8791  0.0603\n\nsigma^2 estimated as 0.0003098579 on 5784 degrees of freedom \n \nAIC = -5.240481  AICc = -5.240481  BIC = -5.237027 \n \n \n\n\n\n\n\nThe diagnostic comparisons suggest that the ARIMA(1,1,0) model is a solid choice, primarily due to its lower AIC score of -5.240506, which outperforms the auto.arima recommendation. Despite not achieving the lower AIC score of the ARIMA(2,0,3) model, we lean towards the ARIMA(1,1,0) for its simplicity, in line with the principle of parsimony.\nEvaluating the standardized residuals plot, it mostly aligns with the expected mean of zero, though there’s evident variance, including volatility clustering. This pattern hints at the potential benefit of incorporating an ARCH/GARCH model to address the error variance.\nThe ACF plot for residuals not showing significant lags is promising, suggesting that the residuals do not have autocorrelation issues. Meanwhile, the normal Q-Q plot shows a fairly normal distribution of residuals with minor deviations in the tails. The Ljung-Box test results, with many p-values above the 0.05 threshold, affirm the model’s adequacy, as we prefer to accept the null hypothesis that the residuals are independent and randomly distributed.\nIn summary, despite the variance indicated by the residuals plot advocating for a volatility model like ARCH/GARCH, the overall diagnostic evidence supports the appropriateness of the selected ARIMA model.\n\n\n\nModel 1Model 2\n\n\n\n\nView Code\nmodel_output1 <-capture.output(sarima(log(bp_ts), 2,0,2))\n\n\n\n\n\n\n\nView Code\ncat(model_output1[90:110], model_output1[length(model_output1)], sep = \"\\n\")\n\n\nconverged\n<><><><><><><><><><><><><><>\n \nCoefficients: \n      Estimate     SE t.value p.value\nar1     0.1993 0.0792  2.5146  0.0119\nar2     0.7950 0.0789 10.0724  0.0000\nma1     0.7825 0.0802  9.7567  0.0000\nma2    -0.0446 0.0141 -3.1574  0.0016\nxmean   3.1134 0.0730 42.6547  0.0000\n\nsigma^2 estimated as 0.0003674774 on 5782 degrees of freedom \n \nAIC = -5.068033  AICc = -5.068031  BIC = -5.061124 \n \nNA\nNA\nNA\nNA\nNA\nNA\n \n\n\n\n\n\n\nView Code\nmodel_output2 <-capture.output(sarima(log(bp_ts), 0,1,0))\n\n\n\n\n\n\n\nView Code\ncat(model_output2[11:18], model_output2[length(model_output2)], sep = \"\\n\")\n\n\nCoefficients: \n         Estimate    SE t.value p.value\nconstant    1e-04 3e-04  0.3794  0.7044\n\nsigma^2 estimated as 0.0003692233 on 5785 degrees of freedom \n \nAIC = -5.065541  AICc = -5.065541  BIC = -5.063237 \n \n \n\n\n\n\n\nThe diagnostic comparisons suggest that the ARIMA(0,1,0) model is a solid choice, primarily due to its lower AIC score of -5.240506.\nEvaluating the standardized residuals plot, it mostly aligns with the expected mean of zero, though there’s evident variance, including volatility clustering. This pattern hints at the potential benefit of incorporating an ARCH/GARCH model to address the error variance.\nThe ACF plot for residuals not showing significant lags is promising, suggesting that the residuals do not have autocorrelation issues. Meanwhile, the normal Q-Q plot shows a fairly normal distribution of residuals with minor deviations in the tails. The Ljung-Box test results, with many p-values above the 0.05 threshold, affirm the model’s adequacy, as we prefer to accept the null hypothesis that the residuals are independent and randomly distributed.\nIn summary, despite the variance indicated by the residuals plot advocating for a volatility model like ARCH/GARCH, the overall diagnostic evidence supports the appropriateness of the selected ARIMA model."
  },
  {
    "objectID": "financial.html#squared-residuals-from-the-best-fitting-arima-model",
    "href": "financial.html#squared-residuals-from-the-best-fitting-arima-model",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Squared Residuals from the Best-Fitting ARIMA Model",
    "text": "Squared Residuals from the Best-Fitting ARIMA Model\n\nChevron CorpBP\n\n\n\n\nView Code\narima.fit_cvx<-Arima(log(cvx_ts),order=c(1,1,0),include.drift = TRUE)\narima.res_cvx<-arima.fit_cvx$residuals\nacf(arima.res_cvx)\n\n\n\n\n\n\n\nView Code\nacf(arima.res_cvx^2)\n\n\n\n\n\n\n\nView Code\npacf(arima.res_cvx^2)\n\n\n\n\n\n\n\n\n\nView Code\narima.fit_bp<-Arima(log(bp_ts),order=c(0,1,0),include.drift = TRUE)\narima.res_bp<-arima.fit_bp$residuals\nacf(arima.res_bp)\n\n\n\n\n\n\n\nView Code\nacf(arima.res_bp^2)\n\n\n\n\n\n\n\nView Code\npacf(arima.res_bp^2)\n\n\n\n\n\n\n\n\nThe consistent pattern of volatility clustering revealed by the plots of squared residuals suggests that typical ARIMA models may not fully capture the dynamic nature of the data.Given the presence of volatility clustering, as evidenced by the ACF and PACF plots of squared residuals, incorporating a model that can handle varying volatility over time is necessary. Both ARCH and GARCH models are designed to capture this feature.\nA GARCH model, as opposed to a simple ARCH model, would likely be more appropriate in this scenario because it encompasses a broader range of data behaviors by accounting for both long-term and short-term variance in the data. It is generally more efficient for financial series where volatility can persist over time, not only at recent time points."
  },
  {
    "objectID": "financial.html#fitting-garch-model-on-arima-residuals",
    "href": "financial.html#fitting-garch-model-on-arima-residuals",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Fitting GARCH Model on ARIMA Residuals",
    "text": "Fitting GARCH Model on ARIMA Residuals\n\nChevron CorpBP\n\n\n\n\nView Code\nmodel <- list() ## set counter\ncc <- 1\nfor (p in 1:8) {\n  for (q in 1:8) {\n  \nmodel[[cc]] <- garch(arima.res_cvx,order=c(q,p),trace=F)\ncc <- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC <- sapply(model, AIC) ## model with lowest AIC is the best\nwhich(GARCH_AIC == min(GARCH_AIC))\n\n\n[1] 1\n\n\n\n\nView Code\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]]\n\n\n\nCall:\ngarch(x = arima.res_cvx, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         b1  \n4.216e-06  8.501e-02  8.997e-01  \n\n\n\n\n\n\nView Code\nmodel <- list() ## set counter\ncc <- 1\nfor (p in 1:8) {\n  for (q in 1:8) {\n  \nmodel[[cc]] <- garch(arima.res_bp,order=c(q,p),trace=F)\ncc <- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC <- sapply(model, AIC) ## model with lowest AIC is the best\nwhich(GARCH_AIC == min(GARCH_AIC))\n\n\n[1] 1\n\n\n\n\nView Code\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]]\n\n\n\nCall:\ngarch(x = arima.res_bp, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         b1  \n3.939e-06  7.672e-02  9.130e-01  \n\n\n\n\n\n\nModel Selection\n\nChevron CorpBP\n\n\n\nGARCH(1,1)GARCH(2,1)GARCH(1,2)\n\n\n\n\nView Code\nsummary(garchFit(~garch(1,1), arima.res_cvx,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = arima.res_cvx, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x7fbf5956fc90>\n [data = arima.res_cvx]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1  \n3.9862e-06  4.2137e-06  8.5029e-02  8.9972e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     3.986e-06   1.702e-04    0.023    0.981    \nomega  4.214e-06   6.734e-07    6.257 3.92e-10 ***\nalpha1 8.503e-02   7.035e-03   12.086  < 2e-16 ***\nbeta1  8.997e-01   7.979e-03  112.765  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 16302.18    normalized:  2.817035 \n\nDescription:\n Thu May  2 22:40:41 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic      p-Value\n Jarque-Bera Test   R    Chi^2  896.99850 0.000000e+00\n Shapiro-Wilk Test  R    W             NA           NA\n Ljung-Box Test     R    Q(10)   36.58388 6.678108e-05\n Ljung-Box Test     R    Q(15)   39.28869 5.801794e-04\n Ljung-Box Test     R    Q(20)   53.33025 7.266309e-05\n Ljung-Box Test     R^2  Q(10)   15.75336 1.069137e-01\n Ljung-Box Test     R^2  Q(15)   17.10344 3.127184e-01\n Ljung-Box Test     R^2  Q(20)   22.57366 3.102100e-01\n LM Arch Test       R    TR^2    15.82513 1.993775e-01\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.632687 -5.628082 -5.632688 -5.631085 \n\n\n\n\n\n\nView Code\nsummary(garchFit(~garch(2,1), arima.res_cvx,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(2, 1), data = arima.res_cvx, trace = F) \n\nMean and Variance Equation:\n data ~ garch(2, 1)\n<environment: 0x7fbf48386038>\n [data = arima.res_cvx]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1      alpha2       beta1  \n3.9862e-06  4.3821e-06  7.6912e-02  1.0524e-02  8.9669e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     3.986e-06   1.702e-04    0.023    0.981    \nomega  4.382e-06   7.372e-07    5.944 2.78e-09 ***\nalpha1 7.691e-02   1.380e-02    5.575 2.48e-08 ***\nalpha2 1.052e-02   1.549e-02    0.680    0.497    \nbeta1  8.967e-01   9.297e-03   96.453  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 16302.6    normalized:  2.817108 \n\nDescription:\n Thu May  2 22:40:41 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic      p-Value\n Jarque-Bera Test   R    Chi^2  881.42714 0.000000e+00\n Shapiro-Wilk Test  R    W             NA           NA\n Ljung-Box Test     R    Q(10)   37.12951 5.374696e-05\n Ljung-Box Test     R    Q(15)   39.77941 4.895957e-04\n Ljung-Box Test     R    Q(20)   53.76092 6.276206e-05\n Ljung-Box Test     R^2  Q(10)   14.66896 1.446053e-01\n Ljung-Box Test     R^2  Q(15)   16.09666 3.756567e-01\n Ljung-Box Test     R^2  Q(20)   21.54511 3.656953e-01\n LM Arch Test       R    TR^2    14.90946 2.464253e-01\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.632488 -5.626731 -5.632490 -5.630485 \n\n\n\n\n\n\nView Code\nsummary(garchFit(~garch(1,2), arima.res_cvx,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = arima.res_cvx, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x7fbf485b5548>\n [data = arima.res_cvx]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2  \n3.9862e-06  4.2176e-06  8.5076e-02  8.9965e-01  1.0000e-08  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     3.986e-06   1.703e-04    0.023    0.981    \nomega  4.218e-06   8.566e-07    4.923 8.51e-07 ***\nalpha1 8.508e-02   1.434e-02    5.933 2.97e-09 ***\nbeta1  8.996e-01   1.901e-01    4.732 2.22e-06 ***\nbeta2  1.000e-08   1.756e-01    0.000    1.000    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 16302.38    normalized:  2.817069 \n\nDescription:\n Thu May  2 22:40:41 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic      p-Value\n Jarque-Bera Test   R    Chi^2  896.94747 0.000000e+00\n Shapiro-Wilk Test  R    W             NA           NA\n Ljung-Box Test     R    Q(10)   36.52761 6.829038e-05\n Ljung-Box Test     R    Q(15)   39.21844 5.944111e-04\n Ljung-Box Test     R    Q(20)   53.25078 7.464976e-05\n Ljung-Box Test     R^2  Q(10)   15.73454 1.074879e-01\n Ljung-Box Test     R^2  Q(15)   17.09309 3.133300e-01\n Ljung-Box Test     R^2  Q(20)   22.58792 3.094783e-01\n LM Arch Test       R    TR^2    15.80556 2.003051e-01\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.632411 -5.626653 -5.632412 -5.630408 \n\n\n\n\n\nAlthough AIC/BIC of GACH(1,1) is slightly higher. The GARCH(1,1) model still stands out with all the significant coefficients, which isn’t the case for the GARCH(1,2) and GARCH(2,1) model. Thus, it’s clear that the optimal choice for our CVX series would be ARIMA(1,1,0) + GARCH(1,1)\n\n\n\nGARCH(1,1)GARCH(2,1)GARCH(1,2)\n\n\n\n\nView Code\nsummary(garchFit(~garch(1,1), arima.res_bp,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = arima.res_bp, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x7fbf485cd778>\n [data = arima.res_bp]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1  \n5.0817e-06  3.9363e-06  7.6723e-02  9.1301e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     5.082e-06   1.850e-04    0.027    0.978    \nomega  3.936e-06   6.966e-07    5.651  1.6e-08 ***\nalpha1 7.672e-02   6.793e-03   11.294  < 2e-16 ***\nbeta1  9.130e-01   7.584e-03  120.389  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 15726.63    normalized:  2.71758 \n\nDescription:\n Thu May  2 22:40:41 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                  Statistic   p-Value\n Jarque-Bera Test   R    Chi^2  2883.397831 0.0000000\n Shapiro-Wilk Test  R    W               NA        NA\n Ljung-Box Test     R    Q(10)     6.997074 0.7257211\n Ljung-Box Test     R    Q(15)    10.054453 0.8163008\n Ljung-Box Test     R    Q(20)    19.488757 0.4902905\n Ljung-Box Test     R^2  Q(10)     5.011861 0.8903843\n Ljung-Box Test     R^2  Q(15)     7.652859 0.9369450\n Ljung-Box Test     R^2  Q(20)    12.632011 0.8926089\n LM Arch Test       R    TR^2      7.294044 0.8375859\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.433777 -5.429171 -5.433778 -5.432175 \n\n\n\n\n\n\nView Code\nsummary(garchFit(~garch(2,1), arima.res_bp,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(2, 1), data = arima.res_bp, trace = F) \n\nMean and Variance Equation:\n data ~ garch(2, 1)\n<environment: 0x7fbf484e5078>\n [data = arima.res_bp]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1      alpha2       beta1  \n5.0817e-06  3.9366e-06  7.6708e-02  1.0000e-08  9.1301e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     5.082e-06   1.851e-04    0.027    0.978    \nomega  3.937e-06   7.534e-07    5.225 1.74e-07 ***\nalpha1 7.671e-02   1.425e-02    5.384 7.29e-08 ***\nalpha2 1.000e-08   1.635e-02    0.000    1.000    \nbeta1  9.130e-01   8.832e-03  103.372  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 15726.75    normalized:  2.7176 \n\nDescription:\n Thu May  2 22:40:42 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                  Statistic   p-Value\n Jarque-Bera Test   R    Chi^2  2885.330180 0.0000000\n Shapiro-Wilk Test  R    W               NA        NA\n Ljung-Box Test     R    Q(10)     7.001523 0.7253012\n Ljung-Box Test     R    Q(15)    10.058253 0.8160597\n Ljung-Box Test     R    Q(20)    19.510851 0.4888774\n Ljung-Box Test     R^2  Q(10)     5.013148 0.8902980\n Ljung-Box Test     R^2  Q(15)     7.649806 0.9370541\n Ljung-Box Test     R^2  Q(20)    12.641040 0.8922489\n LM Arch Test       R    TR^2      7.285403 0.8381910\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.433472 -5.427715 -5.433473 -5.431469 \n\n\n\n\n\n\nView Code\nsummary(garchFit(~garch(1,2), arima.res_bp,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = arima.res_bp, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x7fbf48ab7ad8>\n [data = arima.res_bp]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2  \n5.0817e-06  4.2914e-06  8.4895e-02  7.8096e-01  1.2298e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     5.082e-06   1.850e-04    0.027    0.978    \nomega  4.291e-06   8.387e-07    5.117 3.11e-07 ***\nalpha1 8.489e-02   1.140e-02    7.446 9.64e-14 ***\nbeta1  7.810e-01   1.462e-01    5.340 9.29e-08 ***\nbeta2  1.230e-01   1.365e-01    0.901    0.368    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 15727.16    normalized:  2.717671 \n\nDescription:\n Thu May  2 22:40:42 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                  Statistic   p-Value\n Jarque-Bera Test   R    Chi^2  2852.539905 0.0000000\n Shapiro-Wilk Test  R    W               NA        NA\n Ljung-Box Test     R    Q(10)     7.030586 0.7225544\n Ljung-Box Test     R    Q(15)    10.052870 0.8164012\n Ljung-Box Test     R    Q(20)    19.538334 0.4871214\n Ljung-Box Test     R^2  Q(10)     5.283927 0.8714224\n Ljung-Box Test     R^2  Q(15)     7.939335 0.9261918\n Ljung-Box Test     R^2  Q(20)    12.846504 0.8838723\n LM Arch Test       R    TR^2      7.584854 0.8166736\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.433613 -5.427856 -5.433615 -5.431610 \n\n\n\n\n\nAlthough AIC/BIC of GACH(1,1) is slightly higher. The GARCH(1,1) model still stands out with all the significant coefficients, which isn’t the case for the GARCH(1,2) and GARCH(2,1) model. Thus, it’s clear that the optimal choice for our BP series would be ARIMA(0,1,0) + GARCH(1,1)."
  },
  {
    "objectID": "financial.html#final-model-fitting",
    "href": "financial.html#final-model-fitting",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Final Model Fitting",
    "text": "Final Model Fitting\n\nChevron CorpBP\n\n\n\nARIMA(1,1,0)GARCH(1,1)BOX-LJUNG TEST\n\n\n\n\nView Code\nsummary(arima.fit_cvx<-Arima(log(cvx_ts),order=c(1,1,0),include.drift = TRUE))\n\n\nSeries: log(cvx_ts) \nARIMA(1,1,0) with drift \n\nCoefficients:\n          ar1  drift\n      -0.0883  4e-04\ns.e.   0.0131  2e-04\n\nsigma^2 = 0.00031:  log likelihood = 15163.78\nAIC=-30321.56   AICc=-30321.56   BIC=-30301.57\n\nTraining set error measures:\n                       ME       RMSE        MAE          MPE      MAPE\nTraining set 3.986174e-07 0.01760109 0.01188014 -0.001502839 0.3157812\n                   MASE          ACF1\nTraining set 0.05103762 -0.0002988665\n\n\n\n\n\n\nView Code\nsummary(final.fit_cvx <- garchFit(~garch(1,1), arima.res_cvx,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = arima.res_cvx, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x7fbf4839c390>\n [data = arima.res_cvx]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1  \n3.9862e-06  4.2137e-06  8.5029e-02  8.9972e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     3.986e-06   1.702e-04    0.023    0.981    \nomega  4.214e-06   6.734e-07    6.257 3.92e-10 ***\nalpha1 8.503e-02   7.035e-03   12.086  < 2e-16 ***\nbeta1  8.997e-01   7.979e-03  112.765  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 16302.18    normalized:  2.817035 \n\nDescription:\n Thu May  2 22:40:42 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic      p-Value\n Jarque-Bera Test   R    Chi^2  896.99850 0.000000e+00\n Shapiro-Wilk Test  R    W             NA           NA\n Ljung-Box Test     R    Q(10)   36.58388 6.678108e-05\n Ljung-Box Test     R    Q(15)   39.28869 5.801794e-04\n Ljung-Box Test     R    Q(20)   53.33025 7.266309e-05\n Ljung-Box Test     R^2  Q(10)   15.75336 1.069137e-01\n Ljung-Box Test     R^2  Q(15)   17.10344 3.127184e-01\n Ljung-Box Test     R^2  Q(20)   22.57366 3.102100e-01\n LM Arch Test       R    TR^2    15.82513 1.993775e-01\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.632687 -5.628082 -5.632688 -5.631085 \n\n\n\n\n\n\nView Code\nsummary(garch(arima.res_cvx, order = c(1,1),trace = F))\n\n\n\nCall:\ngarch(x = arima.res_cvx, order = c(1, 1), trace = F)\n\nModel:\nGARCH(1,1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-7.03414 -0.59555  0.02795  0.62210  5.62234 \n\nCoefficient(s):\n    Estimate  Std. Error  t value Pr(>|t|)    \na0 4.216e-06   4.822e-07    8.744   <2e-16 ***\na1 8.501e-02   4.684e-03   18.148   <2e-16 ***\nb1 8.997e-01   5.733e-03  156.944   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n    Jarque Bera Test\n\ndata:  Residuals\nX-squared = 896.22, df = 2, p-value < 2.2e-16\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.0077157, df = 1, p-value = 0.93\n\n\n\n\nView Code\ncheckresiduals(garch(arima.res_cvx, order = c(1,1),trace = F), theme = theme_bw())\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals\nQ* = 716.41, df = 730.5, p-value = 0.6382\n\nModel df: 0.   Total lags used: 730.5\n\n\n\n\n\nThe Ljung-Box test results a p-value of 0.6381, suggesting the model’s residuals are random and free of autocorrelation, confirmed by the equal degrees of freedom to the lags used, indicating a well-specified model.\nThe plot of the residuals over time does not seem to show any obvious patterns or trends, which is consistent with the idea of randomness in the residuals. The ACF plot of residuals also shows that most autocorrelation coefficients are within the confidence bounds, suggesting no significant autocorrelation at various lags. Finally, the distribution plot of the residuals appears to have a bell shape and closely follows the normal distribution, indicating that the residuals are well-behaved and the model has done a good job capturing the structure of the data.\nOverall, the Ljung-Box test results and the diagnostic plots collectively suggest that the current model is adequately specified and there may not be a need for further modeling of the residuals.\n\n\n\nARIMA(0,1,0)GARCH(1,1)BOX-LJUNG TEST\n\n\n\n\nView Code\nsummary(arima.fit_bp<-Arima(log(bp_ts),order=c(0,1,0),include.drift = TRUE))\n\n\nSeries: log(bp_ts) \nARIMA(0,1,0) with drift \n\nCoefficients:\n      drift\n      1e-04\ns.e.  3e-04\n\nsigma^2 = 0.0003693:  log likelihood = 14656.61\nAIC=-29309.22   AICc=-29309.22   BIC=-29295.89\n\nTraining set error measures:\n                       ME       RMSE        MAE          MPE      MAPE\nTraining set 5.081731e-07 0.01921356 0.01287856 -0.002161193 0.4219117\n                   MASE       ACF1\nTraining set 0.06194278 -0.0179258\n\n\n\n\n\n\nView Code\nsummary(final.fit_bp <- garchFit(~garch(1,1), arima.res_bp,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = arima.res_bp, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x7fbf4d7a10b0>\n [data = arima.res_bp]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1  \n5.0817e-06  3.9363e-06  7.6723e-02  9.1301e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     5.082e-06   1.850e-04    0.027    0.978    \nomega  3.936e-06   6.966e-07    5.651  1.6e-08 ***\nalpha1 7.672e-02   6.793e-03   11.294  < 2e-16 ***\nbeta1  9.130e-01   7.584e-03  120.389  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 15726.63    normalized:  2.71758 \n\nDescription:\n Thu May  2 22:40:43 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                  Statistic   p-Value\n Jarque-Bera Test   R    Chi^2  2883.397831 0.0000000\n Shapiro-Wilk Test  R    W               NA        NA\n Ljung-Box Test     R    Q(10)     6.997074 0.7257211\n Ljung-Box Test     R    Q(15)    10.054453 0.8163008\n Ljung-Box Test     R    Q(20)    19.488757 0.4902905\n Ljung-Box Test     R^2  Q(10)     5.011861 0.8903843\n Ljung-Box Test     R^2  Q(15)     7.652859 0.9369450\n Ljung-Box Test     R^2  Q(20)    12.632011 0.8926089\n LM Arch Test       R    TR^2      7.294044 0.8375859\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.433777 -5.429171 -5.433778 -5.432175 \n\n\n\n\n\n\nView Code\nsummary(garch(arima.res_bp, order = c(1,1),trace = F))\n\n\n\nCall:\ngarch(x = arima.res_bp, order = c(1, 1), trace = F)\n\nModel:\nGARCH(1,1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-8.89387 -0.57388  0.01751  0.59118  7.75504 \n\nCoefficient(s):\n    Estimate  Std. Error  t value Pr(>|t|)    \na0 3.939e-06   4.127e-07    9.545   <2e-16 ***\na1 7.672e-02   3.099e-03   24.759   <2e-16 ***\nb1 9.130e-01   4.020e-03  227.125   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n    Jarque Bera Test\n\ndata:  Residuals\nX-squared = 2882.6, df = 2, p-value < 2.2e-16\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.26039, df = 1, p-value = 0.6099\n\n\n\n\nView Code\ncheckresiduals(garch(arima.res_bp, order = c(1,1),trace = F), theme = theme_bw())\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals\nQ* = 780.64, df = 730.5, p-value = 0.09674\n\nModel df: 0.   Total lags used: 730.5\n\n\n\n\n\nThe residuals plot shows no obvious patterns or trends, which is good—it suggests that the model has captured the data’s behavior fairly well. However, some outliers are visible, which could be a sign of extreme values that the model didn’t account for.\nThe ACF plot of the residuals displays bars within the confidence bounds for most lags, indicating that there is no significant autocorrelation in the residuals at those lags. This is a positive sign, as it implies the model’s errors are random and do not exhibit predictable structure over time.\nThe Ljung-Box test p-value of 0.09673 is above the common alpha level of 0.05, which means we fail to reject the null hypothesis of independence in the residuals. This result supports the absence of autocorrelation seen in the ACF plot.\nThe histogram of residuals, with the overlaid normal distribution curve, suggests that the residuals are approximately normally distributed, which is an assumption for many statistical models.\nTaking all the results into account, the diagnostic plots and the Ljung-Box test suggest that the model is fitting well, as there is no significant evidence of autocorrelation in the residuals, and they appear to be normally distributed. The model may not need further refinement, although the presence of outliers could be explored further to determine if they have an undue influence on the model."
  },
  {
    "objectID": "financial.html#forecasting",
    "href": "financial.html#forecasting",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Forecasting",
    "text": "Forecasting\n\nChevron CorpBP\n\n\n\n\nView Code\npredict(final.fit_cvx, n.ahead = 100, plot=TRUE)\n\n\n\n\n\nFrom the plot, it looks like the forecasted values maintain the series’ mean close to zero, which could mean the model predicts the series to continue as it has been—assuming no trend or seasonal component is present. The consistency in the variability of the residuals and the predictions implies that the model has captured the data’s behavior well.\n\n\n\n\nView Code\npredict(final.fit_bp, n.ahead = 100, plot=TRUE)\n\n\n\n\n\nFrom the plot, the data appears to fluctuate around a consistent mean, and the forecast extends this pattern, suggesting the model expects this behavior to continue. The width of the confidence intervals reflects the model’s estimation of forecast uncertainty, with wider intervals indicating less certainty. Overall, the plot suggests a well-fitted model with reasonable prediction intervals for the given time series."
  },
  {
    "objectID": "financial.html#volitality-plots",
    "href": "financial.html#volitality-plots",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Volitality Plots",
    "text": "Volitality Plots\n\nChevron CorpBP\n\n\n\n\nView Code\nht <- final.fit_cvx@h.t \ncvx=data.frame(cvx)\ncvx <- data.frame(cvx,rownames(cvx))\ncolnames(cvx)[7] = \"date\"\ncvx$date<-as.Date(cvx$date,\"%Y-%m-%d\")\n\ndata_cvx = data.frame(ht, cvx$date)\n\ng1<- ggplot(data_cvx, aes(y = ht, x = cvx.date)) + geom_line(col = \"#92c54b\") + ylab('Conditional Variance') + xlab('Date')+ggtitle(\"Volatality plot\") + theme_bw()\n\n# Convert to plotly\nggplotly(g1)\n\n\n\n\n\n\nThe graph indicates periods of relative calm interspersed with sharp spikes where volatility has surged. Such spikes could correspond to external events or periods of market stress that have increased uncertainty. Particularly, the graph shows two noticeable large spikes near 2008 to 2009 as a result of The Great Recession and 2020 as a result of COVID-19 pandemic that led to increased volatility during those times.\n\n\n\n\nView Code\nht <- final.fit_bp@h.t \n\ncolnames(bp)[7] = \"date\"\ndata_bp = data.frame(ht,bp$date)\n\ng2<-ggplot(data_bp, aes(y = ht, x = bp.date)) + geom_line(col = \"#92c54b\") + ylab('Conditional Variance') + xlab('Date')+ggtitle(\"Volatality plot\") + theme_bw()\n\n# Convert to plotly\nggplotly(g2)\n\n\n\n\n\n\nWe observe periods of relatively low volatility interrupted by significant spikes, suggesting moments of high market uncertainty or specific events impacting volatility. These pronounced spikes, especially the ones around 2008-2009 as a result of The Great Recession and 2010 due to the largest oil spill in the history of marine oil drilling operations and again around 2020 as a result of COVID-19 pandemic."
  },
  {
    "objectID": "financial.html#model-equation",
    "href": "financial.html#model-equation",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Model Equation",
    "text": "Model Equation\n\nChevron CorpBP\n\n\nARIMA(0,1,1) + GARCH(1,1):\n\\(r_t = \\phi r_{t-1} + \\epsilon_t + \\theta \\epsilon_{t-1}\\)\nwhere \\(\\phi\\) = autoregressive parameters for the conditional mean of the time series, \\(\\theta\\) = moving average parameters for the conditional mean of the time series, \\(\\epsilon_t\\) = standardized white noise process with mean 0 and variance 1.\nThe conditional variance of the time series, \\(\\sigma_t^2\\), is modeled as a GARCH(1,1) process as:\n\\(\\sigma_t^2 = a_0 + \\alpha_1 \\epsilon_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2\\)\nwhere \\(\\alpha_1\\) = autoregressive parameters for the squared residuals \\(r_{t-1}^2\\) and the conditional variances \\(\\sigma_{t-1}^2\\), \\(\\beta_1\\) = moving average parameters for the squared residuals \\(r_{t-1}^2\\) and the conditional variances \\(\\sigma_{t-1}^2\\), \\(a_0\\) = constant variance term.\n\n\nARIMA(0,1,0) + GARCH(1,1):\n\\(r_t = \\phi r_{t-1} + \\epsilon_t\\)\nwhere \\(\\phi\\) = autoregressive parameters for the conditional mean of the time series, \\(\\theta\\) = moving average parameters for the conditional mean of the time series, \\(\\epsilon_t\\) = standardized white noise process with mean 0 and variance 1.\nThe conditional variance of the time series, \\(\\sigma_t^2\\), is modeled as a GARCH(1,1) process as:\n\\(\\sigma_t^2 = a_0 + \\alpha_1 \\epsilon_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2\\)\nwhere \\(\\alpha_1\\) = autoregressive parameters for the squared residuals \\(r_{t-1}^2\\) and the conditional variances \\(\\sigma_{t-1}^2\\), \\(\\beta_1\\) = moving average parameters for the squared residuals \\(r_{t-1}^2\\) and the conditional variances \\(\\sigma_{t-1}^2\\), \\(a_0\\) = constant variance term."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This section explores an exploratory data analysis of various time series datasets relevant to this project. It involves identifying components, determining whether they align with multiplicative or additive models, generating lag plots, exploring autocorrelation, assessing stationarity, and applying differencing and detrending techniques."
  },
  {
    "objectID": "eda.html#time-series-plots",
    "href": "eda.html#time-series-plots",
    "title": "Exploratory Data Analysis",
    "section": "Time Series Plots",
    "text": "Time Series Plots\nlet’s start by visually examining each of the time series datasets. This initial step is crucial because it allows us to quickly spot any patterns, such as seasonality, or determine whether the data follows a multiplicative or additive trend.\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 emissions\n\n\n\n\n\n\n\nView Code\nprices<-read.csv(\"datasets/eda/Henry_Hub_Natural_Gas_Spot_Price.csv\")\nprices$Date <- as.Date(prices$Month, format = \"%m/%d/%y\")\nprices <- subset(prices, select = -Month)\nnames(prices)[1] <- \"prices\"\nprices <- prices[c(\"Date\", \"prices\")]\nprices_ts = ts(prices$prices, start = 2005, end = c(2022,12), frequency =12 )\n\nfig = autoplot(prices_ts, ts.geom = \"line\", main = \"Natural Gas Henry Hub Prices in the U.S.\", xlab = \"Time\", ylab = \"Prices (Dollars per Million Btu)\", colour = \"#92c54b\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\nThe graph illustrates the historical prices of natural gas at the Henry Hub in the U.S. from 2005, displaying significant volatility with sharp peaks and troughs, particularly notable around 2008, 2014, and a sharp rise post-2020. The pattern of fluctuations, where changes are proportionally larger during high-price periods, suggests a multiplicative seasonality in the series, indicating that external factors impacting prices have a stronger effect when prices are high. This behavior points to a multiplicative time series model, are described by the equation \\(Series = Trend \\times Seasonal \\times Random\\).\n\n\n\n\nView Code\nconsumption<-read.csv(\"datasets/eda/Consumption.csv\")\n\n# Convert \"Month\" column to Date type\nconsumption$Date <- as.Date(consumption$Month, format = \"%Y-%m-%d\")\n\nconsumption_ts = ts(consumption$Consumption, start = 2005,end = c(2022,12),frequency = 12)\nfig = autoplot(consumption_ts, ts.geom = \"line\", main = \"U.S. Natural Gas Total Consumption\", xlab = \"Date\", ylab = \"Consumption (Million Cubic Feet)\", colour = \"#92c54b\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\nLooking at this graph, it’s easy to spot that when we use more natural gas overall, the high-usage times in winter—when heaters are on full blast—really stick out. That means we’re dealing with a additive series. Basically, as we use more gas in general, those winter peaks get even higher. This shows that how much more gas we use in the cold months is tied to how much we’re using all year round. The more gas we need overall, the bigger the jumps in use when it’s cold and everyone’s trying to keep warm.\n\n\n\n\nView Code\nproduction<-read.csv(\"datasets/eda/Production.csv\")\n\n# Convert \"Month\" column to Date type\nproduction$Date <- as.Date(production$Month, format = \"%Y-%m-%d\")\n\nproduction_ts = ts(production$Production, start = 2005,end = c(2022,12),frequency = 12)\nfig = autoplot(production_ts, ts.geom = \"line\", main = \"U.S. Natural Gas Total Production\", xlab = \"Date\", ylab = \"Production (Million Cubic Feet)\", colour = \"#92c54b\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\nThe graph lays it out pretty simply, U.S. is producing more natural gas than ever before. You can see the numbers going up over the years. Sure, there are some dips and bumps that might be about the time of year, but the big picture is that production keeps climbing. These ups and downs get more noticeable as production gets higher, which hints that we’re looking at a multiplicative pattern. That’s when the seasonal changes get bigger as the overall numbers do. But to really nail down if that’s the case, or if we’ve got an additive situation where those changes stay the same size no matter what, we’d have to dig a little deeper into the details.\n\n\n\n\nView Code\nimports<-read.csv(\"datasets/eda/Imports.csv\")\n\n# Convert \"Month\" column to Date type\nimports$Date <- as.Date(imports$Month, format = \"%Y-%m-%d\")\n\nimports_ts = ts(imports$Imports, start = 1997,end = c(2022,12),frequency = 12)\nfig = autoplot(imports_ts, ts.geom = \"line\", main = \"U.S. Natural Gas Imports\", xlab = \"Date\", ylab = \"Imports (Million Cubic Feet)\", colour = \"#92c54b\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\nThe graph indicates that the U.S. has been reducing its natural gas imports steadily since 1997, which could be due to advancements in domestic production or a shift in import needs. There’s a significant drop in imports shown around 2020, likely connected to the disruptions in the energy market caused by the COVID-19 pandemic, which would have influenced trade and demand patterns. The data also reflects seasonal patterns, with consistent fluctuations that hint at a recurring cycle within each year. The changing size of these seasonal peaks and troughs could point to a multiplicative series, where the scale of the fluctuations is linked to the overall volume of imports. A more thorough analysis would be required to confirm the exact nature of the series.\n\n\n\n\nView Code\nexports<-read.csv(\"datasets/eda/Exports.csv\")\n\n# Convert \"Month\" column to Date type\nexports$Date <- as.Date(exports$Month, format = \"%Y-%m-%d\")\n\nexports_ts = ts(exports$Exports, start = 1997,end = c(2022,12),frequency = 12)\nfig = autoplot(exports_ts, ts.geom = \"line\", main = \"U.S. Natural Gas Exports\", xlab = \"Date\", ylab = \"Exports (Million Cubic Feet)\", colour = \"#92c54b\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\nThe graph shows a pronounced upward trend in U.S. natural gas exports, particularly after 2010, with a steep increase leading up to 2022. The series displays fluctuations that may not seem consistent in amplitude over time, suggesting an multiplicative series. Around the early 2020s, there is visible volatility which may correlate with the onset of the COVID-19 pandemic. This period is marked by sharp fluctuations which could be attributed to changes in demand or supply chain disruptions in exports in response to the global crisis. The pandemic’s impact is evident as it deviates from the otherwise steady trend of growth.\n\n\n\n\nView Code\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\ntickers = c(\"CVX\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2000-01-02\",\n             to = \"2022-12-29\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\ncvx_stk <- data.frame(CVX$CVX.Adjusted)\n\ncvx_stk <- data.frame(cvx_stk,rownames(cvx_stk))\ncolnames(cvx_stk) <- append(tickers,'Dates')\n\ncvx_stk$Dates<-as.Date(cvx_stk$Dates,\"%Y-%m-%d\")\n\ncvx_stk_ts <- ts(cvx_stk$CVX, start = decimal_date(as.Date(\"2000-01-02\")), frequency = 365.25)\npe<-autoplot(cvx_stk_ts, ts.geom = \"line\",xlab = \"Time\", ylab = \"Price\", colour = \"#92c54b\")+ggtitle('Time Series Plot For Chevron Corp Stock')+theme_bw()\nggplotly(pe)\n\n\n\n\n\n\nThe graph shows a noticeable trend of growth particularly after a period of more stable prices. The fluctuations do not exhibit a consistent seasonal pattern, and the variations in price don’t seem to depend on the level of the series, indicating that the series might be multiplicative. However, stock prices are typically influenced by a multitude of factors, and seasonality is not always a primary characteristic in such time series data.\n\n\n\n\nView Code\nco2_emissions<-read.csv(\"datasets/eda/co2_emissions.csv\")\n# Convert \"Month\" column to Date type\nco2_emissions$Date <- as.Date(co2_emissions$Date, format = \"%Y-%m-%d\")\n\nco2_ts = ts(co2_emissions$co2_value, start = 2000,end = c(2023,11),frequency = 12)\n\nfig = autoplot(co2_ts, ts.geom = \"line\", main = \"Carbon Dioxide Emissions from U.S. Natural Gas Consumption\", xlab = \"Date\", ylab = \"CO2 Emissions (million metric tons of CO2)\", colour = \"#92c54b\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\nThe graph shows us that CO2 emissions from using natural gas in the U.S. go up and down in a pattern every year suggesting seasonality. The emissions go up when heating demand is high. The ups and downs get bigger as the years go on that’s a sure sign we’re looking at a multiplicative series and underscoring the whole “more usage, more emissions” vibe each year."
  },
  {
    "objectID": "eda.html#lag-plots",
    "href": "eda.html#lag-plots",
    "title": "Exploratory Data Analysis",
    "section": "Lag Plots",
    "text": "Lag Plots\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 emissions\n\n\n\n\nView Code\ngglagplot(prices_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Price ($ per Million Btu) \")+ggtitle(\"Lag Plot for Natural Gas Henry Hub Prices\")+theme(axis.text.x=element_text(angle=45, hjust=1)) + theme_bw()\n\n\n\n\n\nLooking at natural gas prices for U.S. homes, you’ll notice that the price in any month often mirrors the price from the month before—this is what we call a strong lag 1 autocorrelation. As you look further, though, at lag 4 and lag 5, the connection starts to weaken. The other plots show very weak positive autocorrelation.\n\n\n\n\nView Code\np<-gglagplot(consumption_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Consumption (Million Cubic Feet)\")+ggtitle(\"Lag Plot for U.S. Natural gas Consumption\") + theme_bw()\np+ theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\nFor U.S. natural gas consumption, there’s a strong, positive autocorrelation from month to month, especially noticeable at lag 1, reflecting a month-to-month consistency. Moreover, there’s a significant yearly pattern, as seen with the strong correlations at lags 11, 12, and 13, which represent the annual cycle in usage. Other time lags don’t show much of a pattern at all, with the correlations being pretty weak.\n\n\n\n\nView Code\nq<-gglagplot(production_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Production (Million Cubic Feet)\")+ggtitle(\"Lag Plot for U.S. Natural Gas Production\") + theme_bw()\nq+ theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\nIn U.S. natural gas production, there’s a strong pattern where the output from one month is closely linked to the next—that’s what we see with the strong positive autocorrelation at lag 1 and lag 2. As for the rest of the plots, they’re also showing a positive autocorrelation.\n\n\n\n\nView Code\nr<-gglagplot(imports_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Imports (Million Cubic Feet)\")+ggtitle(\"Lag Plot for Natural gas Imports\")+ theme_bw()\nr+ theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\nIn U.S. natural gas imports, there’s a strong link between what happens in one month and the next, shown by the strong positive autocorrelation at lag 1. After that, the connection starts to weaken gradually, and by the time it hits lags 13 to 16, the autocorrelation is pretty weak.\n\n\n\n\nView Code\ns<-gglagplot(exports_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Exports (Million Cubic Feet)\")+ggtitle(\"Lag Plot for Natural gas Exports\")+ theme_bw()\ns+ theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\nIn the U.S. natural gas exports, there’s a clear pattern where what happens this month is a good indicator of what’s next, especially at lag 1 and lag 2, showing a strong connection. But as you move further out, from lag 3 onwards, this link starts to fade. By the time you reach lags 15 and 16, there’s hardly any correlation left, meaning what happened over a year ago doesn’t really tell us much about what’s happening now.\n\n\n\n\nView Code\ngglagplot(cvx_stk_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Price\")+ggtitle(\"Lag Plot for Chevron Corp Stocks\")+theme(axis.text.x=element_text(angle=45, hjust=1)) + theme_bw()\n\n\n\n\n\nFor CVX stocks, there’s a strong positive link in their movements from one day to the next, especially noticeable in the first three lags. This connection starts to lessen as you look further out in time, and by the time you get to lags 13 to 16, the correlation is pretty weak, indicating that past stock movements don’t offer much insight into the stock’s behavior at these points.\n\n\n\n\nView Code\ngglagplot(co2_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"CO2 Emissions (million metric tons of CO2)\")+ggtitle(\"Lag Plot for CO2 Emissions from Natural Gas\")+theme(axis.text.x=element_text(angle=45, hjust=1)) + theme_bw()\n\n\n\n\n\nFor CO2 emissions, there’s a notable strong positive correlation at lag 13. At other lags, the correlation is much weaker, indicating that month-to-month changes are not as predictably linked."
  },
  {
    "objectID": "eda.html#decomposition-plots",
    "href": "eda.html#decomposition-plots",
    "title": "Exploratory Data Analysis",
    "section": "Decomposition Plots",
    "text": "Decomposition Plots\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 emissions\n\n\n\n\nView Code\ndecomposed <- decompose(prices_ts, \"multiplicative\")\nautoplot(decomposed, main = \"Decomposition Plot For Natural Gas Henry Hub Prices\")+theme_bw()+geom_line(colour = \"#92c54b\")\n\n\n\n\n\nTend: The trend line shows that prices were higher around 2005, fell to a lower stable trend until about 2020, and then began to rise again. Seasonal: Seasonal variations are quite regular, indicating higher prices during certain times of the year, likely winter, consistent with increased heating demand. Remainder: The remainder component shows occasional spikes, suggesting external shocks or unique events affecting prices.\n\n\n\n\nView Code\ndecomposed <- decompose(consumption_ts, \"additive\")\nautoplot(decomposed, main = \"Decomposition Plot For Natural Gas Consumption\",ts.colour = 'blue')+theme_bw()+geom_line(colour = \"#92c54b\")\n\n\n\n\n\nThe plot shows clear seasonal peaks and troughs in natural gas consumption, which repeat with a regular pattern every year. Trend: Here we see a generally increasing trend in natural gas consumption over the years. Seasonal: The plot reveals the seasonal component, highlighting the recurring pattern within each year. The pronounced and regular oscillations correspond to the periods of higher and lower consumption, likely related to heating and cooling needs during winter and summer months, respectively. Remainder: This plot includes irregularities or noise in the data after the trend and seasonal components have been removed.\n\n\n\n\nView Code\ndecomposed <- decompose(production_ts, \"multiplicative\")\nautoplot(decomposed, main = \"Decomposition Plot For Natural Gas Production\")+theme_bw()+geom_line(colour = \"#92c54b\")\n\n\n\n\n\nThe data plot suggests a stable and slightly increasing production trend with regular fluctuations. Trend: This plot indicates a generally stable production level in the earlier years with a slight but noticeable increase starting around 2010 and continuing to rise. Seasonal: The plot shows the cyclical pattern in the data. This plot suggests a complex pattern, hinting at seasonal effects that are not as regular as in other seasonal data, such as consumption. Remainder: This plot includes irregularities or noise in the data after the trend and seasonal components have been removed.\n\n\n\n\nView Code\ndecomposed <- decompose(imports_ts, \"multiplicative\")\nautoplot(decomposed, main = \"Decomposition Plot For Natural Gas Imports\")+theme_bw()+geom_line(colour = \"#92c54b\")\n\n\n\n\n\nOverall, the plot suggests that natural gas imports have declined over the analyzed period while exhibiting a strong seasonal pattern and some random noise.\n\n\n\n\nView Code\ndecomposed <- decompose(exports_ts, \"multiplicative\")\nautoplot(decomposed, main = \"Decomposition Plot For Natural Gas Exports\")+theme_bw()+geom_line(colour = \"#92c54b\")\n\n\n\n\n\nOverall, the decomposition suggests that natural gas exports from this dataset are characterized by a growing trend over the years, with clear seasonal patterns and some random fluctuations that don’t follow a predictable pattern.\n\n\n\n\nView Code\ndecomposed <- decompose(cvx_stk_ts, \"multiplicative\")\nautoplot(decomposed, main = \"Decomposition Plot For Chevron Corp Stock\")+theme_bw()+geom_line(colour = \"#92c54b\")\n\n\n\n\n\nOverall, the plot suggests that Chevron’s stock has experienced a long-term upward trend, with no distinct seasonal pattern, and some short-term irregular movements, particularly in recent years.\n\n\n\n\nView Code\ndecomposed <- decompose(co2_ts, \"multiplicative\")\nautoplot(decomposed, main = \"Decomposition Plot for CO2 Emissions from Natural Gas Consumption\")+theme_bw()+geom_line(colour = \"#92c54b\")\n\n\n\n\n\nThe sharp and regular seasonal pattern suggests that the consumption of natural gas, and therefore the CO2 emissions, has clear seasonal drivers. The stable trend implies that, over the years, there has not been a significant long-term increase or decrease in the level of emissions after accounting for the seasonal effects."
  },
  {
    "objectID": "eda.html#acf-and-pacf-plots",
    "href": "eda.html#acf-and-pacf-plots",
    "title": "Exploratory Data Analysis",
    "section": "ACF and PACF Plots",
    "text": "ACF and PACF Plots\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 emissions\n\n\n\n\nView Code\npricesacf <- ggAcf(prices_ts)+ggtitle(\"ACF Plot for Natural Gas Henry Hub Prices \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \npricespacf <- ggPacf(prices_ts)+ggtitle(\"PACF Plot for Natural Gas Henry Hub Prices \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \ngrid.arrange(pricesacf, pricespacf, nrow=2)\n\n\n\n\n\nThe data on Natural Gas Henry Hub Prices reveals a tight relationship in the short term, with the strongest autocorrelation at lags 1 through 24 in the Autocorrelation Function (ACF) plot. In the Partial Autocorrelation Function (PACF) plot, shows up lag at 1. Seeing this kind of pattern, points to the data being non-stationary.\n\n\n\n\nView Code\nconsumptionacf <- ggAcf(consumption_ts)+ggtitle(\"ACF Plot for Natural Gas Consumption \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \nconsumptionpacf <- ggPacf(consumption_ts)+ggtitle(\"PACF Plot for Natural Gas Consumption \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \ngrid.arrange(consumptionacf, consumptionpacf, nrow=2)\n\n\n\n\n\nThe Natural Gas Consumption data presents a clear pattern that the immediate past consumption (at lags 1 and 2) is strongly connected to the current consumption, as shown by the high autocorrelation values at these lags in the ACF plot. Similarly, the PACF plot confirms this relationship at the first two lags. Here, the strong connections at the first few lags suggest our Natural Gas Consumption numbers change over time, indicating non-stationarity.\n\n\n\n\nView Code\nproductionacf <- ggAcf(production_ts)+ggtitle(\"ACF Plot for Natural Gas Production \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \nproductionpacf <- ggPacf(production_ts)+ggtitle(\"PACF Plot for Natural Gas Production \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \ngrid.arrange(productionacf, productionpacf, nrow=2)\n\n\n\n\n\nThe data on Natural Gas Production reveals a tight relationship in the short term, with strong autocorrelation at lags 1 through 24 in the Autocorrelation Function (ACF) plot. In the Partial Autocorrelation Function (PACF) plot, this close relationship shows up at lags 1 and 2. Seeing this kind of pattern, points to the data being non-stationary.\n\n\n\n\nView Code\nimportsacf <- ggAcf(imports_ts)+ggtitle(\"ACF Plot for Natural Gas Imports \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \nimportspacf <- ggPacf(imports_ts)+ggtitle(\"PACF Plot for Natural Gas Imports \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \ngrid.arrange(importsacf, importspacf, nrow=2)\n\n\n\n\n\nThe data on Natural Gas Imports reveals a tight relationship in the short term, with strong autocorrelation at lags 1 through 24 in the Autocorrelation Function (ACF) plot. In the Partial Autocorrelation Function (PACF) plot, this close relationship shows up at lags 1. Seeing this kind of pattern, points to the data being non-stationary.\n\n\n\n\nView Code\nexportsacf <- ggAcf(exports_ts)+ggtitle(\"ACF Plot for Natural Gas Exports \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \nexportspacf <- ggPacf(exports_ts)+ggtitle(\"PACF Plot for Natural Gas Exports \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \ngrid.arrange(exportsacf, exportspacf, nrow=2)\n\n\n\n\n\nThe data on Natural Gas Exports reveals a tight relationship in the short term, with strong autocorrelation at lags 1 through 24 in the Autocorrelation Function (ACF) plot. In the Partial Autocorrelation Function (PACF) plot, this close relationship shows up at lags 1. Seeing this kind of pattern, points to the data being non-stationary.\n\n\n\n\nView Code\nstocksacf <- ggAcf(cvx_stk_ts,100)+ggtitle(\"ACF Plot for Chevron Corp Stock \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\nstockspacf <- ggPacf(cvx_stk_ts,100)+ggtitle(\"PACF Plot for Chevron Corp Stock \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\ngrid.arrange(stocksacf, stockspacf, nrow=2)\n\n\n\n\n\nThe data on Natural Gas Stocks reveals a tight relationship in the short term, with strong autocorrelation at lags 1 through 100 in the Autocorrelation Function (ACF) plot. In the Partial Autocorrelation Function (PACF) plot, this close relationship shows up at lags 1. Seeing this kind of pattern, points to the data being non-stationary.\n\n\n\n\nView Code\nco2acf <- ggAcf(co2_ts)+ggtitle(\"ACF Plot for CO2 Emissions from Natural Gas Consumption \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \nco2pacf <- ggPacf(co2_ts)+ggtitle(\"PACF Plot for CO2 Emissions from Natural Gas Consumption\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \ngrid.arrange(co2acf, co2pacf, nrow=2)\n\n\n\n\n\nThe data on CO2 Emissions reveals a week relationship, with little autocorrelation at lags 13 in the Autocorrelation Function (ACF) plot. In the Partial Autocorrelation Function (PACF) plot, this close relationship shows up at lag 9, 10, 11, 12 and 13. Seeing this kind of pattern, points to the data being stationary."
  },
  {
    "objectID": "eda.html#dickey-fuller-test",
    "href": "eda.html#dickey-fuller-test",
    "title": "Exploratory Data Analysis",
    "section": "Dickey-Fuller Test",
    "text": "Dickey-Fuller Test\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 emissions\n\n\n\n\nView Code\nadf.test(prices_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  prices_ts\nDickey-Fuller = -2.5138, Lag order = 5, p-value = 0.3605\nalternative hypothesis: stationary\n\n\nSince the p value is more than 0.05, we fail to reject the null hypothesis. Suggesting that the series is non-stationary validating earlier observation.\n\n\n\n\nView Code\nadf.test(consumption_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  consumption_ts\nDickey-Fuller = -11.489, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\nSince the p value is less than 0.05, we have sufficient evidence to reject the null hypothesis.Suggesting that the series is stationary contradicting earlier observation.\n\n\n\n\nView Code\nadf.test(production_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  production_ts\nDickey-Fuller = -2.345, Lag order = 5, p-value = 0.4313\nalternative hypothesis: stationary\n\n\nSince the p value is more than 0.05, we fail to reject the null hypothesis. Suggesting that the series is non-stationary validating earlier observation.\n\n\n\n\nView Code\nadf.test(imports_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  imports_ts\nDickey-Fuller = -2.8553, Lag order = 6, p-value = 0.2158\nalternative hypothesis: stationary\n\n\nSince the p value is more than 0.05, we fail to reject the null hypothesis. Suggesting that the series is non-stationary validating earlier observation.\n\n\n\n\nView Code\nadf.test(exports_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  exports_ts\nDickey-Fuller = -0.35869, Lag order = 6, p-value = 0.9879\nalternative hypothesis: stationary\n\n\nSince the p value is more than 0.05, we fail to reject the null hypothesis. Suggesting that the series is non-stationary validating earlier observation.\n\n\n\n\nView Code\nadf.test(cvx_stk_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  cvx_stk_ts\nDickey-Fuller = -2.2655, Lag order = 17, p-value = 0.4659\nalternative hypothesis: stationary\n\n\nSince the p value is more than 0.05, we fail to reject the null hypothesis. Suggesting that the series is non-stationary validating earlier observation.\n\n\n\n\nView Code\nadf.test(co2_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  co2_ts\nDickey-Fuller = -10.018, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\nSince the p value is less than 0.05, we have sufficient evidence to reject the null hypothesis.Suggesting that the series is stationary validating earlier observation."
  },
  {
    "objectID": "eda.html#detrend-differencing",
    "href": "eda.html#detrend-differencing",
    "title": "Exploratory Data Analysis",
    "section": "Detrend & Differencing",
    "text": "Detrend & Differencing\nNext, we need to detrend and difference the data to make it stationary except for the CO2 emissions data-set. As seen above from ACF plot, PCF plot and validated from the Dickey-Fuller test that the CO2 emissions data-set is already stationary.\n\nPricesConsumptionProductionImportsExportsCVX Stocks\n\n\n\n\nView Code\n# Generate model\nfit_prices = lm(prices_ts~time(prices_ts), na.action=NULL)\n\nplot1<-autoplot(resid(fit_prices), main=\"Detrended\", colour = \"#92c54b\") +theme_bw()\nplot2<-autoplot(diff(prices_ts), main=\"First Difference\", colour = \"#92c54b\") +theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nView Code\nplot1 <- ggAcf(prices_ts, 48) + ggtitle(\"Original Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\nplot2 <- ggAcf(resid(fit_prices), 48) + ggtitle(\"Detrended Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\nplot3 <- ggAcf(diff(prices_ts), 48) + ggtitle(\"First Difference Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\n\nView Code\n# Generate model\nfit_consump = lm(consumption_ts~time(consumption_ts), na.action=NULL)\n\nplot1<-autoplot(resid(fit_consump), main=\"Detrended\", colour = \"#92c54b\") +theme_bw()\nplot2<-autoplot(diff(consumption_ts), main=\"First Difference\", colour = \"#92c54b\") +theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nView Code\nplot1 <- ggAcf(consumption_ts, 48) + ggtitle(\"Original Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\nplot2 <- ggAcf(resid(fit_consump), 48) + ggtitle(\"Detrended Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\nplot3 <- ggAcf(diff(consumption_ts), 48) + ggtitle(\"First Difference Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\n\nView Code\n# Generate model\nfit_prod = lm(production_ts~time(production_ts), na.action=NULL)\n\nplot1<-autoplot(resid(fit_prod), main=\"Detrended\", colour = \"#92c54b\") +theme_bw()\nplot2<-autoplot(diff(production_ts), main=\"First Difference\", colour = \"#92c54b\") +theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nView Code\nplot1 <- ggAcf(production_ts, 48) + ggtitle(\"Original Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\nplot2 <- ggAcf(resid(fit_prod), 48) + ggtitle(\"Detrended Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\nplot3 <- ggAcf(diff(production_ts), 48) + ggtitle(\"First Difference Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\n\nView Code\n# Generate model\nfit_imports = lm(imports_ts~time(imports_ts), na.action=NULL)\n\nplot1<-autoplot(resid(fit_imports), main=\"Detrended\", colour = \"#92c54b\") +theme_bw()\nplot2<-autoplot(diff(imports_ts), main=\"First Difference\", colour = \"#92c54b\") +theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nView Code\nplot1 <- ggAcf(imports_ts, 48) + ggtitle(\"Original Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\nplot2 <- ggAcf(resid(fit_imports), 48) + ggtitle(\"Detrended Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\nplot3 <- ggAcf(diff(imports_ts), 48) + ggtitle(\"First Difference Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\n\nView Code\n# Generate model\nfit_exports = lm(exports_ts~time(exports_ts), na.action=NULL)\n\nplot1<-autoplot(resid(fit_exports), main=\"Detrended\", colour = \"#92c54b\") +theme_bw()\nplot2<-autoplot(diff(exports_ts), main=\"First Difference\", colour = \"#92c54b\") +theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nView Code\nplot1 <- ggAcf(exports_ts, 48) + ggtitle(\"Original Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\nplot2 <- ggAcf(resid(fit_exports), 48) + ggtitle(\"Detrended Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\nplot3 <- ggAcf(diff(exports_ts), 48) + ggtitle(\"First Difference Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\n\nView Code\n# Generate model\nfit_stk = lm(cvx_stk_ts~time(cvx_stk_ts), na.action=NULL)\n\nplot1<-autoplot(resid(fit_stk), main=\"Detrended\", colour = \"#92c54b\") +theme_bw()\nplot2<-autoplot(diff(cvx_stk_ts), main=\"First Difference\", colour = \"#92c54b\") +theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nView Code\nplot1 <- ggAcf(cvx_stk_ts, 48) + ggtitle(\"Original Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\nplot2 <- ggAcf(resid(fit_stk), 48) + ggtitle(\"Detrended Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\nplot3 <- ggAcf(diff(cvx_stk_ts), 48) + ggtitle(\"First Difference Data\")+theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \n\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\nOverall, detrending and differencing was used to prepare the data for further analysis/modeling. The detrended data suggests that the trend component has been removed, revealing the underlying seasonal effects, while the differenced data shows little to no autocorrelation, suggesting that the data may have been made stationary."
  },
  {
    "objectID": "eda.html#first-vs-second-difference",
    "href": "eda.html#first-vs-second-difference",
    "title": "Exploratory Data Analysis",
    "section": "First Vs Second Difference",
    "text": "First Vs Second Difference\n\nPricesConsumptionProductionImportsExportsCVX Stocks\n\n\n\n\nView Code\nplot_1<-ggAcf(diff(prices_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \nplot_2<-ggAcf(diff(diff(prices_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\ngrid.arrange(plot_1, plot_2,nrow=2)\n\n\n\n\n\nAfter performing First Order Differencing, we can see no significant correlation in the ACF and PACF plots, which makes the data stationary. However, Second Order Differencing, makes the data look more stationary than it actually is, and may lead to inaccurate estimates of parameters and forecasts.\n\n\n\n\nView Code\nplot_1<-ggAcf(diff(consumption_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \nplot_2<-ggAcf(diff(diff(consumption_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\ngrid.arrange(plot_1, plot_2,nrow=2)\n\n\n\n\n\nAfter performing First Order Differencing, we can see no significant correlation in the ACF and PACF plots, which makes the data stationary. However, Second Order Differencing, makes the data look more stationary than it actually is, and may lead to inaccurate estimates of parameters and forecasts.\n\n\n\n\nView Code\nplot_1<-ggAcf(diff(production_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \nplot_2<-ggAcf(diff(diff(production_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\ngrid.arrange(plot_1, plot_2,nrow=2)\n\n\n\n\n\nAfter performing First Order Differencing, we can see no significant correlation in the ACF and PACF plots, which makes the data stationary. However, Second Order Differencing, makes the data look more stationary than it actually is, and may lead to inaccurate estimates of parameters and forecasts.\n\n\n\n\nView Code\nplot_1<-ggAcf(diff(imports_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \nplot_2<-ggAcf(diff(diff(imports_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\ngrid.arrange(plot_1, plot_2,nrow=2)\n\n\n\n\n\nAfter performing First Order Differencing, we can see no significant correlation in the ACF and PACF plots, which makes the data stationary. So, there is no need for Second Order Differencing as it makes the data look more stationary than it actually is, and may lead to inaccurate estimates of parameters and forecasts.\n\n\n\n\nView Code\nplot_1<-ggAcf(diff(exports_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \nplot_2<-ggAcf(diff(diff(exports_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\ngrid.arrange(plot_1, plot_2,nrow=2)\n\n\n\n\n\nAfter performing First Order Differencing, we can see no significant correlation in the ACF and PACF plots, which makes the data stationary. So, there is no need for Second Order Differencing as it makes the data look more stationary than it actually is, and may lead to inaccurate estimates of parameters and forecasts.\n\n\n\n\nView Code\nplot_1<-ggAcf(diff(cvx_stk_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\") \nplot_2<-ggAcf(diff(diff(cvx_stk_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#92c54b\") +\n    geom_hline(yintercept = 0, color = \"#92c54b\")\ngrid.arrange(plot_1, plot_2,nrow=2)\n\n\n\n\n\nAfter performing First Order Differencing, we can see no significant correlation in the ACF and PACF plots, which makes the data stationary. So, there is no need for Second Order Differencing as it makes the data look more stationary than it actually is, and may lead to inaccurate estimates of parameters and forecasts."
  },
  {
    "objectID": "eda.html#adjusted-dickey-fuller-test",
    "href": "eda.html#adjusted-dickey-fuller-test",
    "title": "Exploratory Data Analysis",
    "section": "Adjusted Dickey-Fuller Test",
    "text": "Adjusted Dickey-Fuller Test\n\nPricesConsumptionProductionImportsExportsCVX Stocks\n\n\n\n\nView Code\nadf.test(diff(diff(prices_ts)))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(diff(prices_ts))\nDickey-Fuller = -9.9148, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nView Code\nadf.test(diff(consumption_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(consumption_ts)\nDickey-Fuller = -7.984, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nView Code\nadf.test(diff(production_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(production_ts)\nDickey-Fuller = -7.9263, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nView Code\nadf.test(diff(imports_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(imports_ts)\nDickey-Fuller = -8.4953, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nView Code\nadf.test(diff(exports_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(exports_ts)\nDickey-Fuller = -10.846, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nView Code\nadf.test(diff(cvx_stk_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(cvx_stk_ts)\nDickey-Fuller = -18.023, Lag order = 17, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nOverall, after first-order/second-order differencing the the p-value for each test is below 0.05 suggesting that we can reject the null hypothesis. Thus, ensuring all the data series has become stationary."
  },
  {
    "objectID": "eda.html#moving-average-smoothing",
    "href": "eda.html#moving-average-smoothing",
    "title": "Exploratory Data Analysis",
    "section": "Moving Average Smoothing",
    "text": "Moving Average Smoothing\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 emissions\n\n\n\n\nView Code\nma_3 = ma(prices_ts, 3)\nma_6 = ma(prices_ts, 6)\nma_12 = ma(prices_ts, 12)\nma_24 = ma(prices_ts, 24)\n\nautoplot(prices_ts, color = \"grey\", alpha = 0.5) + \n  autolayer(ma_3, series = \"3-MA\")+\n  autolayer(ma_6, series = \"6-MA\")+\n  autolayer(ma_12, series = \"12-MA\")+\n  autolayer(ma_24, series = \"24-MA\")+ \n  labs(\n    title = \"Time Series Plot For Natural Gas Prices Delivered to Residential Consumers with Moving Averages\",\n    x = \"Date\",\n    y = \"Prices (Dollars per Thousand Cubic Feet)\",\n    color = \"Moving Average Series\"\n  )+\n  theme_bw()\n\n\n\n\n\nThe moving averages are calculated for 3-MA, 6-MA, 12-MA, and 24-MA periods. The 3-MA, is closest to the raw data and shows a smoother version of the short-term fluctuations. The 6-MA, smooths out the data over a slightly longer period, reducing the noise further compared to the 3-MA. The 12-MA, provides an annual perspective, showing the underlying trend within each year. The 24-MA, is the smoothest line, likely indicating the overall trend over two years, which helps in identifying the longer-term direction of the prices. The natural gas price data is highly seasonal, as indicated by the regular peaks and troughs. This seasonality could be attributed to the increased demand for heating during the winter months. The plot also reveals a general upward trend in prices over time, particularly noticeable in the 24-MA.\n\n\n\n\nView Code\nma_3 = ma(consumption_ts, 3)\nma_6 = ma(consumption_ts, 6)\nma_12 = ma(consumption_ts, 12)\nma_24 = ma(consumption_ts, 24)\n\nautoplot(consumption_ts, color = \"grey\", alpha = 0.5) + \n  autolayer(ma_3, series = \"3-MA\")+\n  autolayer(ma_6, series = \"6-MA\")+\n  autolayer(ma_12, series = \"12-MA\")+\n  autolayer(ma_24, series = \"24-MA\")+ \n  labs(\n    title = \"Time Series Plot For Natural Gas Consumption with Moving Averages\",\n    x = \"Date\",\n    y = \"Consumption (Million Cubic Feet)\",\n    color = \"Moving Average Series\"\n  )+\n  theme_bw()\n\n\n\n\n\nThe moving averages are calculated for 3-MA, 6-MA, 12-MA, and 24-MA periods. The raw data shows strong seasonal patterns in natural gas consumption. The 12-MA likely reflects the annual cycle, while the 24-MA shows a longer-term trend, likely indicating a gradual increase in natural gas consumption over the period displayed.Especially the 24-MA, highlight a growth trend in consumption, despite the regular fluctuations.\n\n\n\n\nView Code\nma_3 = ma(production_ts, 3)\nma_6 = ma(production_ts, 6)\nma_12 = ma(production_ts, 12)\nma_24 = ma(production_ts, 24)\n\nautoplot(production_ts, color = \"grey\", alpha = 0.5) + \n  autolayer(ma_3, series = \"3-MA\")+\n  autolayer(ma_6, series = \"6-MA\")+\n  autolayer(ma_12, series = \"12-MA\")+\n  autolayer(ma_24, series = \"24-MA\")+ \n  labs(\n    title = \"Time Series Plot For Natural Gas Production with Moving Averages\",\n    x = \"Date\",\n    y = \"Production (Million Cubic Feet)\",\n    color = \"Moving Average Series\"\n  )+\n  theme_bw()\n\n\n\n\n\nThe moving averages are calculated for 3-MA, 6-MA, 12-MA, and 24-MA periods. The 3-MA and 6-MA closely track the original data, smoothing out some of the volatility. The 12-MA, shows less variability and indicates a clearer upward trend. The 24-MA smoothest line suggests the long-term trend, further reducing noise and clearly highlighting the general increase in natural gas production over the years.\n\n\n\n\nView Code\nma_3 = ma(imports_ts, 3)\nma_6 = ma(imports_ts, 6)\nma_12 = ma(imports_ts, 12)\nma_24 = ma(imports_ts, 24)\n\nautoplot(imports_ts, color = \"grey\", alpha = 0.5) + \n  autolayer(ma_3, series = \"3-MA\")+\n  autolayer(ma_6, series = \"6-MA\")+\n  autolayer(ma_12, series = \"12-MA\")+\n  autolayer(ma_24, series = \"24-MA\")+ \n  labs(\n    title = \"Time Series Plot For Natural Gas Imports with Moving Averages\",\n    x = \"Date\",\n    y = \"Imports (Million Cubic Feet)\",\n    color = \"Moving Average Series\"\n  )+\n  theme_bw()\n\n\n\n\n\nThe moving averages are calculated for 3-MA, 6-MA, 12-MA, and 24-MA periods.The 3-MA and 6-MA follow the original data closely, reflecting short-term trends and seasonal fluctuations. The 12-MA smooths out the data to an extent where yearly patterns may be observed. The 24-MA provides the smoothest curve, revealing the long-term trend of the import data. The original data shows that natural gas imports have experienced significant volatility, with a general declining trend particularly noticeable from a peak around 2008 onwards. The moving averages confirm this downward trend, with the 24-MA indicating a gradual decrease over the observed period.\n\n\n\n\nView Code\nma_3 = ma(exports_ts, 3)\nma_6 = ma(exports_ts, 6)\nma_12 = ma(exports_ts, 12)\nma_24 = ma(exports_ts, 24)\n\nautoplot(exports_ts, color = \"grey\", alpha = 0.5) + \n  autolayer(ma_3, series = \"3-MA\")+\n  autolayer(ma_6, series = \"6-MA\")+\n  autolayer(ma_12, series = \"12-MA\")+\n  autolayer(ma_24, series = \"24-MA\")+ \n  labs(\n    title = \"Time Series Plot For Natural Gas Exports with Moving Averages\",\n    x = \"Date\",\n    y = \"Exports (Million Cubic Feet)\",\n    color = \"Moving Average Series\"\n  )+\n  theme_bw()\n\n\n\n\n\nThe moving averages are calculated for 3-MA, 6-MA, 12-MA, and 24-MA periods. The moving averages trace closely together, suggesting consistency in the underlying trend, which is a significant, steady increase in natural gas exports over time. The 3-MA and 6-MA reveal some short-term variations, while the 12-MA, and 24-M smooth out these fluctuations and clarify the upward trajectory.\n\n\n\n\nView Code\nma_3 = ma(cvx_stk_ts, 3)\nma_6 = ma(cvx_stk_ts, 6)\nma_12 = ma(cvx_stk_ts, 12)\nma_24 = ma(cvx_stk_ts, 24)\n\nautoplot(cvx_stk_ts, color = \"grey\", alpha = 0.5) + \n  autolayer(ma_3, series = \"3-MA\")+\n  autolayer(ma_6, series = \"6-MA\")+\n  autolayer(ma_12, series = \"12-MA\")+\n  autolayer(ma_24, series = \"24-MA\")+ \n  labs(\n    title = \"Time Series Plot For Chevron Corp Stock with Moving Averages\",\n    x = \"Date\",\n    y = \"Price\",\n    color = \"Moving Average Series\"\n  )+\n  theme_bw()\n\n\n\n\n\nThe moving averages are calculated for 3-MA, 6-MA, 12-MA, and 24-MA periods. The moving averages each follow the actual price closely. A sharp increase in the stock price is visible around 2015, which is reflected across all the moving averages, suggesting a strong market during that time.\n\n\n\n\nView Code\nma_3 = ma(co2_ts, 3)\nma_6 = ma(co2_ts, 6)\nma_12 = ma(co2_ts, 12)\nma_24 = ma(co2_ts, 24)\n\nautoplot(co2_ts, color = \"grey\", alpha = 0.5) + \n  autolayer(ma_3, series = \"3-MA\")+\n  autolayer(ma_6, series = \"6-MA\")+\n  autolayer(ma_12, series = \"12-MA\")+\n  autolayer(ma_24, series = \"24-MA\")+ \n  labs(\n    title = \"Time Series Plot For CO2 Emissions from Natural Gas Consumption\",\n    x = \"Date\",\n    y = \"Million Metric Tons of CO2\",\n    color = \"Moving Average Series\"\n  )+\n  theme_bw()\n\n\n\n\n\nThe moving averages are calculated for 3-MA, 6-MA, 12-MA, and 24-MA periods. These moving averages, smooth out the series and reveal underlying trends by dampening the volatility and noise present in the original data. The seasional nature of the emissions data could be reflective of seasonal patterns in natural gas consumption, likely with higher emissions during winter months due to increased heating requirements. The 12-MA and 24-MA, typically shows this seasonality with less pronounced fluctuations. The overall trend suggests a repeating pattern without a clear long-term increase or decrease in emissions over the years. This could imply that while seasonality is strong, the overall emissions from natural gas have remained relatively stable."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Analysis of Natural Gas Industry in the US",
    "section": "Introduction",
    "text": "Introduction\nIn the big picture of America’s energy scene, natural gas is like a star player – it’s affordable, does a good job, and is kinder to the environment compared to old-school fuels. They call it a “bridge fuel,” helping us move from the old ways of powering things to a cleaner, greener future. It’s a practical choice that keeps the lights on while we figure out the renewable energy stuff. And natural gas isn’t just about making power; it’s woven into everything, influencing how we do business, sparking new ideas, and even playing a role in how countries work together. So, when we talk about natural gas, it’s not just about keeping the energy flowing – it’s about shaping how we live and where we’re headed.\nZooming in on the U.S. natural gas scene, you realize it’s not just about drilling and pipelines. It’s part of everyday life, from heating our homes to fueling the machines that make stuff. But its impact goes even deeper, affecting jobs, technology, and even how countries relate to each other. Natural gas isn’t just a player in the energy game; it’s like a backstage VIP, pulling strings that touch everything from local businesses to global affairs. So, as the way we power up changes, natural gas keeps holding its own, being a key player in the U.S. story – past, present, and future.\nWatch how natural gas has our backs. It provides energy when renewables don’t have enough power stored, to help us keep up with our modern lifestyles.\n\nvideo source",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#literature-review",
    "href": "index.html#literature-review",
    "title": "Analysis of Natural Gas Industry in the US",
    "section": "Literature review",
    "text": "Literature review\nNowhere is the influence of natural gas more pronounced than in the United States. As both the world’s largest producer and consumer of natural gas, the nation relies heavily on this resource.\nTalking about natural gas production, significant sources include basins such as the Marcellus and Utica Shale formations in Appalachia, as well as the Haynesville Shale formation in Louisiana and East Texas. The U.S. Geological Survey estimates that the Appalachian formations contain approximately 214 trillion cubic feet of undiscovered, technically recoverable continuous natural gas resources. Thanks to advancements in drilling and extraction technologies, it’s now possible to produce large quantities of natural gas safely and cost-effectively, providing a reliable local supply for consumers. Source\n\nConsumption of natural gas in the U.S. has grown alongside production, it claims a substantial 33% share in the U.S. energy consumption landscape, making it a cornerstone in the nation’s energy portfolio.\n\n\n\n\n\n\nNatural gas consumption actually grew 2% in April 2020 vs. a year earlier.\n\n\n\nThis came during the same month that overall U.S. energy consumption fell to its lowest level in more than 30 years. Likely affected by stay-at-home orders, residential consumption of natural gas was 15% higher in April 2020 vs. a year earlier.\n\n\n\n\n\n\n\n\nMore than 100 coal-fired plants have been replaced or converted to natural gas in the U.S. since 2011.\n\n\n\nThe decision for plants to switch from coal to natural gas has been driven by stricter emission standards, low natural gas prices, and more efficient natural gas turbine technology.\n\n\nSource\n\n\n\n\n\nThe applications of natural gas are diverse, ranging from electricity generation to fueling vehicles, heating homes, and cooking. Significantly, it is used for heating, electricity generation, and industrial processes in the U.S., highlighting its pivotal role in the nation’s power infrastructure.\n\n\n\n\n\n\nNatural gas-fired power generation in the Lower 48 jumped 9% in the first half of 2020 vs. a year earlier.\n\n\n\nNatural gas generators now make up largest share of U.S. electricity generation capacity.\n\n\nSource\n\n\n\n\n\nBeyond its energy applications, the natural gas industry is an economic powerhouse. Rising natural gas production has historically correlated directly with economic growth in the country – with the past decade-plus being a prime example.\nAs natural gas production skyrocketed in the United States over the past decade-plus, the U.S. enjoyed an unprecedented 11 consecutive years of gross domestic product (GDP) growth prior to the COVID-19 pandemic. The natural gas boom has been credited for 8% of GDP growth since the Great Recession. Source1 Source2\nThis economic prosperity can largely be explained by the fact that strong domestic natural gas production dramatically reduces foreign imports, keeping billions of dollars here in our own economy rather than exporting them oversees. Our newfound energy bounty has also proven the equivalent of a massive tax cut, lowering energy costs dramatically. In fact, the Department of Energy reported in 2018 that average U.S. energy costs fell 34 percent from 2008 to 2016, dropping from record-high levels to a “record-low energy expenditure share” in less than a decade. Source\nThe U.S.natural gas industry also continues to support more than 10.3 million jobs across the United States, with direct natural gas industry jobs paying an average salary double the private sector average ($112,712 per year). Source\nEnergy is the one thing that all Americans use. And with that fundamental reality in mind, we have literally been able to drill our way to lower energy prices and economic prosperity over the past decade-plus, all while reducing greenhouse gas emissions. Source\n Source\nThe United States has reduced carbon dioxide emissions more than any other country this century. And although it may surprise casual observers, experts agree that the primary reason for these emission declines is fuel-switching to clean-burning natural gas, which has been made possible by industry innovation. That’s right: the same technologies that have allowed us to emerge as the leading oil and natural gas producer in the world are also largely responsible for our status as the world leader in greenhouse gas emission reductions. The decoupling trend is unprecedented and shows that we don’t have to choose between the economy and environment. The emissions reductions have been driven by fuel-switching in the electricity generation sector, with the Energy Information Administration estimating that natural gas is actually responsible for 58 percent more power sector CO2 emissions reductions than renewables and other non-carbon electricity generation sources. Source\nRecent Record-Breaking Consumption\n\n\n\nimage by: GETTY IMAGES/ISTOCKPHOTO\n\n\nIn a remarkable turn of events, the severe U.S. winter weather in mid-January 2024 set new records for natural gas consumption. On January 16, 2024, consumption reached an unprecedented monthly high while the prices for the heating fuel have dropped to their lowest since April. This real-world event offers a tangible backdrop to the broader exploration, showcasing how the industry responds to and is influenced by external factors. Source",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#explore-the-big-picture",
    "href": "index.html#explore-the-big-picture",
    "title": "Analysis of Natural Gas Industry in the US",
    "section": "Explore the big picture",
    "text": "Explore the big picture",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#scope-of-the-analysis",
    "href": "index.html#scope-of-the-analysis",
    "title": "Analysis of Natural Gas Industry in the US",
    "section": "Scope of the Analysis",
    "text": "Scope of the Analysis\nThis project aims to deepen understanding of the natural gas sector’s crucial role within the broader energy matrix by analyzing economic, environmental, demand and supply dynamics, stock market interactions, and trade impacts. It will highlight current market conditions and forecast future trends.\n\nEconomic Impact\n\n\n\nImage source: Shutterstock/FOTOGRIN\n\n\nThis comprehensive analysis embarks on a multifaceted journey to unravel the correlation between natural gas industry dynamics and key economic indicators such as GDP and unemployment rates. The analysis primarily focuses on how these economic factors impact natural gas prices, illuminating the industry’s influence on the broader economic health of the nation.\n\n\nDemand/Supply Dynamics\n\n\n\n\n\nThis analysis delves into the dynamics of demand and supply, exploring how factors such as natural gas prices, storage, and temperature influence consumption, as well as how supply-side dynamics, including imports and production, affect natural gas prices. A thorough understanding of these demand and supply behaviors will enrich the analysis.\n\n\nEnvironmental Impact\n\n\n\n\n\nA crucial facet of the exploration lies in understanding the Environmental Impact of natural gas consumption and production. By correlating natural gas usage and production with carbon emissions, the analysis contributes to the ongoing discourse on sustainable energy practices.\n\n\nMarket Performance\n\n\n\nimage by: Torsten Asmus/iStock via Getty Images\n\n\nExtending into the Financial Markets, the analysis scrutinizes the stock market performance of major energy companies involved in natural gas production. This facet provides insights into the industry’s financial resilience and responsiveness to market dynamics.\n\n\nTrade Dynamics\n\n\n\n\n\nTrade dynamics form a crucial part of this analysis, examining how natural gas trade relationships contribute to a broader understanding of the industry’s global economic influence. This analysis specifically explores how U.S. natural gas consumption, exchange rates, and the economic health of importing country affect natural gas exports."
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Analysis of Natural Gas Industry in the US",
    "section": "Research questions",
    "text": "Research questions\n\nHow can we anticipate changes in natural gas prices based on economic indicators like GDP and unemployment rates?\nHow can we optimize natural gas consumption in response to fluctuating production levels, and external factors like temperature and storage capacities?\nHow do domestic consumption, exchange rates, and the economic health of importing countries like the Netherlands affect US natural gas export volumes?\nHow can changes in natural gas consumption and production levels be expected to influence CO2 emissions?\nHow can understanding the relationship between supply dynamics, such as changes in imports and production, help optimize pricing strategies and inventory management for natural gas at Henry Hub?\nHow do stock market trends of major natural gas companies reflect the overall health of the natural gas industry?\nWhat are the future trends in natural gas prices based on historical price patterns?\nHow can we use historical consumption patterns to predict future demand for natural gas, and how might this influence US procurement and distribution strategies?\nCan we anticipate the future levels of natural gas imports and exports based on historical data?\nCan we predict future trends in CO2 emissions based solely on their historical patterns?Based on trends in CO2 emissions from natural gas usage, how can companies adjust their practices to better meet environmental standards?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Simple Lightbox Example",
    "section": "",
    "text": "#1: Oil and Gas Has Improved Both Quality and Length of Life\n\n\n\n\n\nAnother Lovely Image\n\n\n\n\n\nThe Last Lovely Image"
  },
  {
    "objectID": "datasrc.html",
    "href": "datasrc.html",
    "title": "Data Sources",
    "section": "",
    "text": "This page provides a brief overview of the datasets utilized, each contributing valuable insights to various aspects of our analysis. It includes the data sources I tapped into for information gathering, and you also have the option to download the datasets.",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "datasrc.html#eia-data",
    "href": "datasrc.html#eia-data",
    "title": "Data Sources",
    "section": "EIA Data",
    "text": "EIA Data\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset contains the average price of natural gas delivered to residential and commercial consumers in the USA.\nThe data spans from the year 1973 to 2023.\nPrices are measured in dollars per thousand cubic feet.\n\nDownload Natural gas Residential Price data Download Natural gas Commercial Price data\n\nThis dataset contains carbon dioxide emissions from energy consumption of natural gas in the USA.\nThe data spans from the year 2000 to 2023.\nThe unit of measurement is millions of metric tons of carbon dioxide.\n\nDownload Natural gas CO2 Emissions data"
  },
  {
    "objectID": "datasrc.html#opec-data",
    "href": "datasrc.html#opec-data",
    "title": "Data Sources",
    "section": "OPEC Data",
    "text": "OPEC Data\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset provides information on the exports and imports of natural gas worldwide, categorized by country.\nThe data covers the years 1975 to 2022.\nThe unit of analysis is million standard cubic meters (M standard cu m).\n\nDownload Export data  Download Import data"
  },
  {
    "objectID": "datasrc.html#fred-data",
    "href": "datasrc.html#fred-data",
    "title": "Data Sources",
    "section": "FRED Data",
    "text": "FRED Data\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset presents employment figures for all employees in Oil and Gas Extraction industry in the United States.\nThe data spans from the year 2000 to 2022.\nThe unit of analysis is Thousands of Persons, Not Seasonally Adjusted.\n\nDownload Employment data"
  },
  {
    "objectID": "datasrc.html#statista-data",
    "href": "datasrc.html#statista-data",
    "title": "Data Sources",
    "section": "Statista Data",
    "text": "Statista Data\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset provides information on natural gas production, consumption, and consumption by sector in the United States.\nData for natural gas production spans from 1998 to 2022, measured in billion cubic meters. For natural gas consumption, the dataset covers the years 1995 to 2022, with measurements in trillion cubic feet. Additionally, data on natural gas consumption by sector is available from 2005 to 2022, presented in billion cubic feet.\n\nDownload Production data  Download Consumption data  Download Counsumption by sector data"
  },
  {
    "objectID": "datasrc.html#yahoo-finance-data",
    "href": "datasrc.html#yahoo-finance-data",
    "title": "Data Sources",
    "section": "Yahoo Finance Data",
    "text": "Yahoo Finance Data\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis data represents the opening price, highest price, lowest price, closing price, adjusted closing price, and trading volume for Chevron Corp (NYSE:CVX) on different dates.\nIt will be used in financial analysis to track the performance and volatility of the stock prices for top 15 natural gas producing companies over time.\n\nDownload Chevron Corporation (CVX) stocks data"
  },
  {
    "objectID": "datasrc.html#iea50-data",
    "href": "datasrc.html#iea50-data",
    "title": "Data Sources",
    "section": "IEA50 Data",
    "text": "IEA50 Data\n\n\n\n\n\nclick image for data source\n\n\n\n\nContains an extensive selection of Greenhouse gas emissions data from the energy sector for over 190 countries and regions.\nThe data included in this file with the following indicators:\n\nEnergy related greenhouse gas emissions\nFugitive greenhouse gas emissions\nExtended time series, starting in 1751 of CO2 emissions from fuel combustion.\n\n\nDownload GHG emissions data"
  },
  {
    "objectID": "datasrc.html#u.s.-bureau-of-labor-statistics-data",
    "href": "datasrc.html#u.s.-bureau-of-labor-statistics-data",
    "title": "Data Sources",
    "section": "U.S. Bureau of Labor Statistics Data",
    "text": "U.S. Bureau of Labor Statistics Data\n\n\n\n\n\nclick image for data source\n\n\n\n\nContains information of the workforce in the U.S. natural gas distribution industry by location, wage, gender and age from 2014 to 2021\n\nDownload employment by location data Download employment by wage data Download employment by gender and age data"
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "This project explored various aspects of the natural gas industry in the United States, utilizing multiple datasets and employing a combination of univariate, multivariate, and financial time series models, alongside cutting-edge deep learning approaches. My initial efforts focused on data visualization and exploratory data analysis to discern patterns and trends within the market. Through univariate analysis, I gained a broader understanding of time series analysis related to natural gas prices, consumption, production, imports, exports, CO2 emissions, and CVX stock movements. This served as a foundational step before advancing to more complex multivariate and financial time series analyses. Building on this foundation with a thorough review of existing literature, I developed sophisticated multivariate time series models. These models were instrumental in examining the effects of GDP and unemployment on natural gas prices, the influences of natural gas production, storage, and temperature on consumption, and the impacts of production and consumption on CO2 emissions. I also explored how domestic consumption, exchange rates, and the GDP of importing countries affect U.S. natural gas exports and investigated the interdependencies among production, import, and natural gas prices. Furthermore, I analyzed and forecasted the stock prices of leading natural gas production companies using financial time series models to assess their responsiveness to changes in the U.S. economy. This comprehensive analysis not only reinforced the understanding of the natural gas sector’s complexities but also underscored the efficacy of time-series modeling in predicting its economic implications.\nTo better understand the nuances of the natural gas sector, let’s examine detailed conclusions across its various dimensions:"
  },
  {
    "objectID": "multivariate.html",
    "href": "multivariate.html",
    "title": "Multivariate TS Models (ARIMAX/SARIMAX/VAR)",
    "section": "",
    "text": "This section delves into the use of ARIMAX, SARIMAX, and VAR modeling techniques for the time series datasets relevant to my project.\nThe ARIMAX (AutoRegressive Integrated Moving Average with eXogenous variables) and SARIMAX (Seasonal ARIMAX) models are particularly useful when you’re looking to predict the behavior of one primary variable based on its past values, while also considering the impact of other external variables. These models are invaluable for scenarios where external factors significantly drive changes in the primary variable, but the influence isn’t bidirectional.\nOn the other hand, VAR (Vector Autoregression) models are pivotal when analyzing systems where variables are interdependent. These models allow us to explore how each variable in the dataset influences others over time, providing a comprehensive view of the dynamic interactions within the data."
  },
  {
    "objectID": "multivariate.html#literature-review",
    "href": "multivariate.html#literature-review",
    "title": "Multivariate TS Models (ARIMAX/SARIMAX/VAR)",
    "section": "Literature Review",
    "text": "Literature Review\nMacroeconomic Factors Affecting Natural Gas Export Management: The paper demonstrates that domestic consumption, exchange rates, and GDP per capita of the importing country significantly influence natural gas export volumes both in the short and long term. This suggests that these variables are critical predictors for modeling and forecasting the volume of natural gas exports\nLow US Storage Levels Point to Winter Natural Gas Price Spike Risk: The article highlights how low gas storage levels significantly impact natural gas prices, suggesting that storage could be a critical dependent variable in models predicting these prices, especially during high-demand winter periods. This relationship also implies that storage levels could serve as a dependent variable in predicting natural gas demand, reflecting its role in balancing supply and demand seasonally.\nShort-Term Energy Outlook Supplement: Market Drivers and Other Factors Affecting Natural Gas Prices: This report provides a detailed analysis of various factors influencing natural gas prices. The report’s emphasis on the direct impacts of consumption and production on natural gas pricing. The report’s discussion on macroeconomic trends affecting industrial activity and natural gas consumption, suggesting that broader economic conditions significantly impact natural gas demand and, consequently, prices.\nImpacts of long-term temperature change and variability on electricity investments: This paper highlights how temperature increases significantly impact electric capacity and investment needs in the U.S. Analyzing climate and socioeconomic scenarios, it shows that higher temperatures boost electricity demand and capital investments, especially during peak times. This suggests that temperature is a crucial factor in predicting natural gas demand, as it is integral to electricity generation.\nDrivers of the US CO2 emissions 1997–2013: This paper uses structural decomposition analysis (SDA) to identify the key factors influencing CO2 emissions in the United States. It reveals that increases in consumption drove emissions up during 1997-2007, while reductions in consumption and changes in production structures helped decrease emissions from 2007-2013. These findings highlights the importance of targeting consumption patterns and production structures in CO2 emissions management."
  },
  {
    "objectID": "multivariate.html#datasets-models",
    "href": "multivariate.html#datasets-models",
    "title": "Multivariate TS Models (ARIMAX/SARIMAX/VAR)",
    "section": "Datasets & Models",
    "text": "Datasets & Models\n\nNatural gas demandEconometric DynamicsExportsCO2 EmissionsSupply Influence\n\n\n(SARIMAX) Consumption ~ Temperature + Production + Storage\nI chose Temperature, Production, and Storage as exogenous variables in the SARIMAX model for predicting natural gas demand because Temperature directly influences consumption due to heating and cooling needs, varying with the seasons. Production levels impact market supply, which can affect consumption by altering availability and prices. Storage levels act as a buffer for supply fluctuations and have implications on consumption through their effect on market dynamics.\n\n\n\n\n\nView Code\nconsumption<-read.csv(\"datasets/eda/Consumption.csv\")\nconsumption$Date <- as.Date(consumption$Month, format = \"%m/%d/%y\")\nconsumption <- subset(consumption, select = -Month)\n\ntemperature<-read.csv(\"datasets/eda/temperature.csv\")\ntemperature$Date <- as.Date(temperature$moonth, format = \"%m/%d/%y\")\ntemperature <- subset(temperature, select = c(Date,avg_temp))\nnames(temperature)[2] <- \"temperature\"\n\nproduction<-read.csv(\"datasets/eda/Production.csv\")\nproduction$Date <- as.Date(production$Month, format = \"%m/%d/%y\")\nproduction <- subset(production, select = -Month)\n\nstorage<-read.csv(\"datasets/eda/Storage.csv\")\nstorage$Date <- as.Date(storage$Month, format = \"%m/%d/%y\")\nstorage <- subset(storage, select = -Month)\nnames(storage)[1] <- \"storage\"\n\ndemand <- consumption %>%\n  left_join(temperature, by = \"Date\") %>%\n  left_join(production, by = \"Date\") %>%\n  left_join(storage, by = \"Date\")\n\ndemand <- demand[, c(2, 1, 3:ncol(demand))]\n\ndemand_ts = ts(demand, start = 2005, end = c(2022,12), frequency =12 )\nhead_demand<-head(demand)\nknitr::kable(head_demand, caption = \"Natural Gas Demand in the US Dataset\")\n\n\n\nNatural Gas Demand in the US Dataset\n\n\nDate\nConsumption\ntemperature\nProduction\nstorage\n\n\n\n\n2005-01-01\n2561858\n33.4\n2035036\n6199291\n\n\n2005-02-01\n2242986\n37.9\n1870546\n5768939\n\n\n2005-03-01\n2205787\n42.3\n2080504\n5484332\n\n\n2005-04-01\n1724877\n52.3\n1979474\n5699060\n\n\n2005-05-01\n1522613\n59.6\n2010625\n6075521\n\n\n2005-06-01\n1534122\n69.2\n1972975\n6398738\n\n\n\n\n\n\n\n(ARIMAX) Prices(Henry Hub) ~ GDP + Unemployment\nI chose GDP and unemployment rates as exogenous variables for this analysis because they are key indicators of economic activity and health, which directly influence energy demand and pricing. Changes in GDP reflect shifts in industrial and consumer demand for energy, while unemployment rates can impact consumption patterns, both of which affect natural gas prices at Henry Hub.\n\n\nView Code\nprice<-read.csv(\"datasets/eda/Henry_Hub_Natural_Gas_Spot_Price.csv\")\nprice$Date <- as.Date(price$Month, format = \"%m/%d/%y\")\nprice <- subset(price, select = -Month)\nnames(price)[1] <- \"price\"\nprice <- price[c(\"Date\", \"price\")]\n# Convert Date to a quarterly period using lubridate\nprice <- price %>%\n  mutate(Date = floor_date(Date, \"quarter\"))\n# Aggregate data by quarter\nprice <- price %>%\n  group_by(Date) %>%\n  summarize(\n    price = round(mean(price),2),  \n    .groups = 'drop'        # Drops the grouping structure after summarizing\n  )\n\ngdp<-read.csv(\"datasets/eda/gdp.csv\")\ngdp$Date <- as.Date(gdp$Quarter, format = \"%Y-%m-%d\")\ngdp <- subset(gdp, select = -Quarter)\nnames(gdp)[1]<-\"gdp\"\ngdp <- gdp[c(\"Date\", \"gdp\")]\n\nunemp<-read.csv(\"datasets/eda/unemp.csv\")\n##unpivot\nunemp <- unemp %>%\n  pivot_longer(\n    cols = Jan:Dec,  #the range of columns to unpivot\n    names_to = \"Month\",  \n    values_to = \"Unemp\"  \n  )\nunemp$Date <- make_date(year = unemp$Year, month = match(unemp$Month, month.abb), day = 1)\n# reorder columns to place Date first and drop Year and Month as they are no longer needed\nunemp <- unemp[, c(\"Date\", \"Unemp\")]\nunemp$Date <- as.Date(unemp$Date, format = \"%Y-%m-%d\")\n# Convert Date to a quarterly period using lubridate\nunemp <- unemp %>%\n  mutate(Date = floor_date(Date, \"quarter\"))\n# Aggregate data by quarter\nunemp <- unemp %>%\n  group_by(Date) %>%\n  summarize(\n    unemp = round(mean(Unemp),2),  \n    .groups = 'drop'        # Drops the grouping structure after summarizing\n  )\n\neco <- price %>%\n  left_join(gdp, by = \"Date\") %>%\n  left_join(unemp, by = \"Date\")\n\neco_ts = ts(eco, start = 2005, frequency =4 )\n\nhead_eco<-head(eco)\nknitr::kable(head_eco, caption = \"Natural Gas Henry Hub Prices and Macroeconomic Indicators in the US Dataset\")\n\n\n\nNatural Gas Henry Hub Prices and Macroeconomic Indicators in the US Dataset\n\n\nDate\nprice\ngdp\nunemp\n\n\n\n\n2005-01-01\n6.42\n12767.29\n5.30\n\n\n2005-04-01\n6.94\n12922.66\n5.10\n\n\n2005-07-01\n9.64\n13142.64\n4.97\n\n\n2005-10-01\n12.26\n13324.20\n4.97\n\n\n2006-01-01\n7.71\n13599.16\n4.73\n\n\n2006-04-01\n6.54\n13753.42\n4.63\n\n\n\n\n\n\n\n(ARIMAX) Exports ~ Domestic Consumption + Exchange Rate + GDP of of the importing country(Netherlands) \nI chose domestic consumption for this analysis because it directly reflects the internal demand for natural gas within the U.S., influencing how much is available for export. The exchange rate affects the competitiveness of U.S. natural gas in global markets, with favorable rates potentially boosting exports by making them more cost-effective for foreign buyers. Additionally, the GDP of the importing country, the Netherlands in this case, was chosen because it indicates the economic health and energy demand of a key market, impacting the volume of natural gas they are likely to import from the U.S.\n\n\nView Code\nexports<-read.csv(\"datasets/eda/exports.csv\")\nexports$Date <- as.Date(exports$Month, format = \"%Y-%m-%d\")\nexports <- subset(exports, select = -Month)\nexports <- exports[c(\"Date\", \"Exports\")]\n# Filter rows where the year is greater than 2004\nexports <- exports[as.integer(format(exports$Date, \"%Y\")) > 2004, ]\n# Convert Date to a quarterly period using lubridate\nexports <- exports %>%\n  mutate(Date = floor_date(Date, \"quarter\"))\n# Aggregate data by quarter\nexports <- exports %>%\n  group_by(Date) %>%\n  summarize(\n    exports = round(mean(Exports),2),\n    .groups = 'drop'        # Drops the grouping structure after summarizing\n  )\n\n\nconsumption<-read.csv(\"datasets/eda/Consumption.csv\")\nconsumption$Date <- as.Date(consumption$Month, format = \"%m/%d/%y\")\nconsumption <- subset(consumption, select = -Month)\nconsumption <- consumption[c(\"Date\", \"Consumption\")]\n# Convert Date to a quarterly period using lubridate\nconsumption <- consumption %>%\n  mutate(Date = floor_date(Date, \"quarter\"))\n# Aggregate data by quarter\nconsumption <- consumption %>%\n  group_by(Date) %>%\n  summarize(\n    consumption = round(mean(Consumption),2),\n    .groups = 'drop'        # Drops the grouping structure after summarizing\n  )\n\nfx_rate<-read.csv(\"datasets/eda/exchange_rate.csv\")\nfx_rate$Date <- as.Date(fx_rate$Date, format = \"%Y-%m-%d\")\nnames(fx_rate)[2] <- \"fx_rate\"\n\n\ngdp_nl<-read.csv(\"datasets/eda/gdp_int.csv\")\ngdp_nl$Date <- as.Date(gdp_nl$Date, format = \"%m/%d/%y\")\nnames(gdp_nl)[2] <- \"gdp_nl\"\n\nexport <- exports %>%\n  left_join(consumption, by = \"Date\") %>%\n  left_join(fx_rate, by = \"Date\") %>%\n  left_join(gdp_nl, by = \"Date\")\n\nexport_ts = ts(export, start = 2005, frequency =4 )\nhead_export<-head(export)\nknitr::kable(head_export, caption = \"US Exports of Natural Gas and the Influencing Factors Dataset\")\n\n\n\nUS Exports of Natural Gas and the Influencing Factors Dataset\n\n\nDate\nexports\nconsumption\nfx_rate\ngdp_nl\n\n\n\n\n2005-01-01\n92397.00\n2336877\n105.2033\n134841.1\n\n\n2005-04-01\n56429.67\n1593871\n103.9500\n136587.9\n\n\n2005-07-01\n50286.33\n1601402\n102.4767\n138671.4\n\n\n2005-10-01\n43753.67\n1805996\n101.5567\n140509.7\n\n\n2006-01-01\n61266.67\n2145338\n100.7100\n143090.4\n\n\n2006-04-01\n57815.33\n1586060\n102.1833\n145172.7\n\n\n\n\n\n\n\n(ARIMAX) CO2 emissions ~ Natural Gas Consumption + Natural Gas Production\nI chose Natural Gas Consumption and Natural Gas Production as exogenous variables in the ARIMAX model for predicting CO2 emissions, because these variables are directly linked to emissions levels. Natural gas consumption is a primary source of CO2 emissions, particularly in power generation and industrial uses, making it a key variable for accurate emissions modeling. Similarly, the rate of natural gas production influences the availability and usage of natural gas, thereby affecting overall emissions.\n\n\nView Code\nco2_emissions<-read.csv(\"datasets/eda/co2_emissions.csv\")\nco2_emissions$Date <- as.Date(co2_emissions$Date, format = \"%m/%d/%y\")\nco2_emissions <- subset(co2_emissions, select = c(Date,co2_value))\n# Filter rows where the year is greater than 2004\nco2_emissions <- co2_emissions[as.integer(format(co2_emissions$Date, \"%Y\")) > 2004, ]\n# Filter rows where the year is less than 2023\nco2_emissions <- co2_emissions[as.integer(format(co2_emissions$Date, \"%Y\"))<2023, ]\n\nconsumption<-read.csv(\"datasets/eda/Consumption.csv\")\nconsumption$Date <- as.Date(consumption$Month, format = \"%m/%d/%y\")\nconsumption <- subset(consumption, select = -Month)\n\nproduction<-read.csv(\"datasets/eda/Production.csv\")\nproduction$Date <- as.Date(production$Month, format = \"%m/%d/%y\")\nproduction <- subset(production, select = -Month)\n\nenv_impact <- co2_emissions %>%\n  left_join(consumption, by = \"Date\") %>%\n   left_join(production, by = \"Date\") \n\nenv_impact_ts = ts(env_impact, start = 2005, end = c(2022,12), frequency =12 )\n\nhead_env_impact<-head(env_impact)\nknitr::kable(head_env_impact, caption = \"Natural Gas Environmental Impact in the US Dataset\")\n\n\n\nNatural Gas Environmental Impact in the US Dataset\n\n\nDate\nco2_value\nConsumption\nProduction\n\n\n\n\n2005-01-01\n1200.931\n2561858\n2035036\n\n\n2005-01-01\n138.000\n2561858\n2035036\n\n\n2005-02-01\n120.689\n2242986\n1870546\n\n\n2005-03-01\n118.592\n2205787\n2080504\n\n\n2005-04-01\n92.529\n1724877\n1979474\n\n\n2005-05-01\n81.622\n1522613\n2010625\n\n\n\n\n\n\n\n(VAR) Prices(Henry Hub) ~ Imports + Production\nPrices at Henry Hub, imports, and production were chosen as endogenous variables for the VAR model to capture the interdependent dynamics within the natural gas market. Henry Hub prices are a benchmark for natural gas pricing and are influenced by changes in supply and demand, which are directly impacted by production levels and import volumes. By analyzing these variables together, the model can illustrate how fluctuations in production and imports directly affect prices, providing a comprehensive view of the supply-demand balance in the U.S. natural gas market.\n\n\nView Code\nprice<-read.csv(\"datasets/eda/Henry_Hub_Natural_Gas_Spot_Price.csv\")\nprice$Date <- as.Date(price$Month, format = \"%m/%d/%y\")\nprice <- subset(price, select = -Month)\nnames(price)[1] <- \"price\"\n\nimports<-read.csv(\"datasets/eda/imports.csv\")\nimports$Date <- as.Date(imports$Month, format = \"%Y-%m-%d\")\nimports <- subset(imports, select = -Month)\nnames(imports)[1] <- \"imports\"\nimports <- imports[c(\"Date\", \"imports\")]\n# Filter rows where the year is greater than 2004\nimports <- imports[as.integer(format(imports$Date, \"%Y\")) > 2004, ]\n\nproduction<-read.csv(\"datasets/eda/Production.csv\")\nproduction$Date <- as.Date(production$Month, format = \"%m/%d/%y\")\nproduction <- subset(production, select = -Month)\n\nsupply_dynamics <- price %>%\n  left_join(imports, by = \"Date\") %>%\n  left_join(production, by = \"Date\")\n\nsupply_dynamics <- supply_dynamics[, c(2, 1, 3:ncol(supply_dynamics))]\nknitr::kable(head(supply_dynamics), caption = \"Natural Gas Supply Dynamics in the US Dataset\")\n\n\n\nNatural Gas Supply Dynamics in the US Dataset\n\n\nDate\nprice\nimports\nProduction\n\n\n\n\n2005-01-01\n6.15\n405016\n2035036\n\n\n2005-02-01\n6.14\n356495\n1870546\n\n\n2005-03-01\n6.96\n379650\n2080504\n\n\n2005-04-01\n7.16\n326147\n1979474\n\n\n2005-05-01\n6.47\n333997\n2010625\n\n\n2005-06-01\n7.18\n321710\n1972975"
  },
  {
    "objectID": "multivariate.html#arimaxsarimax-modeling",
    "href": "multivariate.html#arimaxsarimax-modeling",
    "title": "Multivariate TS Models (ARIMAX/SARIMAX/VAR)",
    "section": "ARIMAX/SARIMAX Modeling",
    "text": "ARIMAX/SARIMAX Modeling\n\nTime series plot\n\nNatural gas demandEconometric DynamicsExportsCO2 Emissions\n\n\n\n\nView Code\nautoplot(demand_ts[,c(2:5)], facets=TRUE, color=\"#92c54b\") +\n  xlab(\"Time\") + ylab(\"\") + theme_bw() +\n  ggtitle(\"Factors Affecting Consumption Variations\")\n\n\n\n\n\nThe graph seems to illustrate a clear seasonal pattern in natural gas consumption, which is likely correlated with temperature variations. This pattern typically shows higher consumption during the colder months when temperatures are low, indicating heating demand is driving usage. The production of natural gas appears to trend upward over time, suggesting increased capacity or efficiency in production methods. Interestingly, while there’s seasonality in consumption that aligns with temperature changes, production doesn’t show a similar seasonal dip but instead a steady or possibly growing trend over the years. Storage levels also exhibit seasonality, with peaks presumably during times of lower consumption when excess production can be stored, and troughs when storage is drawn down during periods of high consumption.\n\n\n\n\nView Code\nautoplot(eco_ts[,c(2:4)], facets=TRUE, color=\"#92c54b\") +\n  xlab(\"Time\") + ylab(\"\")+ theme_bw() +\n  ggtitle(\"Macroeconomic Factors Affecting Natural Gas Prices\")\n\n\n\n\n\nThe graph shows an unexpected trend between natural gas prices, U.S. GDP, and unemployment. Despite steady economic growth indicated by rising GDP, natural gas prices rise initially but then decline, suggesting influences from factors like technological advances or energy policy changes. The sharp increase in unemployment in 2020 leads to a significant drop in prices, indicating decreased demand. However, as the economy recovers and unemployment falls, prices begin to recover, reflecting a rebound in demand for natural gas.\n\n\n\n\nView Code\nautoplot(export_ts[,c(2:5)], facets=TRUE, color=\"#92c54b\") +\n  xlab(\"Time\") + ylab(\"\")+ theme_bw() +\n  ggtitle(\"Factors Affecting Variations in US Exports\")\n\n\n\n\n\nThe plot shows that the growth of U.S. natural gas exports alongside factors like domestic consumption, foreign exchange rates, and the GDP of the Netherlands. It shows a steady increase in exports, reflecting the U.S.’s growing role in global energy markets. Domestic consumption follows a consistent cyclical pattern, suggesting seasonal variability in demand which doesn’t appear to directly impact the rising trend in exports. The foreign exchange rate exhibits fluctuations which could influence international trade dynamics, potentially making U.S. gas more or less attractive to foreign buyers depending on the strength of the dollar. Lastly, the GDP of the Netherlands, a significant importer, shows a steady rise, indicating economic growth that could correlate with increased demand for U.S. natural gas, aligning with the increase in exports.\n\n\n\n\nView Code\nautoplot(env_impact_ts[,c(2:4)], facets=TRUE, color=\"#92c54b\") +\n  xlab(\"Time\") + ylab(\"\")+ theme_bw() +\n  ggtitle(\"Factors Affecting CO2 Emission Variations\")\n\n\n\n\n\nAccording to the graph CO2 emissions exhibit sharp, periodic spikes that could correlate with specific events or time periods—potentially seasonal changes in energy use. Natural gas consumption shows a fluctuating but consistent pattern, suggesting a regular, perhaps seasonal, variation—likely related to heating needs in winter and cooling in summer. Production, however, trends upward, indicating growing output over the years. Together, this plot shows consumption and production trends directly impact emission levels.\n\n\n\n\n\nAutomatic Model Fitting\n\nNatural gas demandEconometric DynamicsExportsCO2 Emissions\n\n\n\n\nView Code\nxreg_demand <- cbind(temperature = demand_ts[, \"temperature\"],\n              Production = demand_ts[, \"Production\"],\n               storage = demand_ts[, \"storage\"] )\n\nfit_demand <- auto.arima(demand_ts[, \"Consumption\"], xreg = xreg_demand)\nsummary(fit_demand)\n\n\nSeries: demand_ts[, \"Consumption\"] \nRegression with ARIMA(0,0,2)(0,1,2)[12] errors \n\nCoefficients:\n         ma1     ma2    sma1     sma2      drift  temperature  Production\n      0.4825  0.2238  -0.723  -0.1183  2111.3781   -28006.454      0.2838\ns.e.  0.0771  0.0638   0.081   0.0827   669.2816     2840.643      0.0732\n      storage\n      -0.0663\ns.e.   0.0329\n\nsigma^2 = 6.886e+09:  log likelihood = -2602.93\nAIC=5223.86   AICc=5224.78   BIC=5253.72\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE    MAPE     MASE\nTraining set -521.2747 79045.42 61595.38 -0.1426391 2.80638 0.526904\n                    ACF1\nTraining set 0.003687805\n\n\nView Code\ncheckresiduals(fit_demand, theme = theme_bw())\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,0,2)(0,1,2)[12] errors\nQ* = 28.821, df = 20, p-value = 0.09135\n\nModel df: 4.   Total lags used: 24\n\n\nBest model: ARIMA(0,0,2)(0,1,2)[12]\n\n\n\n\nView Code\nxreg_eco <- cbind(gdp = eco_ts[, \"gdp\"],\n              unemp = eco_ts[, \"unemp\"])\n\nfit_eco <- auto.arima(eco_ts[, \"price\"], xreg = xreg_eco)\nsummary(fit_eco)\n\n\nSeries: eco_ts[, \"price\"] \nRegression with ARIMA(0,1,0)(0,0,2)[4] errors \n\nCoefficients:\n         sma1     sma2    drift    gdp   unemp\n      -0.3273  -0.4984  -0.2165  1e-03  0.2079\ns.e.   0.1632   0.2082   0.1129  7e-04  0.1730\n\nsigma^2 = 1.141:  log likelihood = -105.07\nAIC=222.15   AICc=223.46   BIC=235.72\n\nTraining set error measures:\n                       ME     RMSE       MAE       MPE    MAPE      MASE\nTraining set -0.002765651 1.022808 0.7047353 -1.651351 15.3355 0.5172927\n                    ACF1\nTraining set -0.03771922\n\n\nView Code\ncheckresiduals(fit_eco, theme = theme_bw())\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0)(0,0,2)[4] errors\nQ* = 13.842, df = 6, p-value = 0.03145\n\nModel df: 2.   Total lags used: 8\n\n\nBest model: ARIMA(0,1,0)(0,0,2)[4]\n\n\n\n\nView Code\nxreg_export <- cbind(consumption = export_ts[, \"consumption\"],\n                     fx_rate = export_ts[, \"fx_rate\"],\n                    gdp_nl = export_ts[, \"gdp_nl\"] )\n\nfit_export <- auto.arima(export_ts[, \"exports\"], xreg = xreg_export)\nsummary(fit_export)\n\n\nSeries: export_ts[, \"exports\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  consumption    fx_rate  gdp_nl\n      0.9682       0.0256  -2805.966  2.7013\ns.e.  0.0280       0.0061   1423.789  0.7156\n\nsigma^2 = 618599493:  log likelihood = -830.24\nAIC=1670.48   AICc=1671.38   BIC=1681.86\n\nTraining set error measures:\n                   ME     RMSE     MAE       MPE     MAPE      MASE       ACF1\nTraining set 631.4008 24170.91 15116.9 -3.351663 9.746448 0.4376682 -0.1334921\n\n\nView Code\ncheckresiduals(fit_export, theme = theme_bw())\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,0,0) errors\nQ* = 10.79, df = 7, p-value = 0.1481\n\nModel df: 1.   Total lags used: 8\n\n\nBest model: ARIMA(1,0,0)\n\n\n\n\nView Code\nxreg_env_impact <- cbind(Consumption = env_impact_ts[, \"Consumption\"],\n              Production = env_impact_ts[, \"Production\"] )\n\nfit_env_impact <- auto.arima(env_impact_ts[, \"co2_value\"], xreg = xreg_env_impact)\nsummary(fit_env_impact)\n\n\nSeries: env_impact_ts[, \"co2_value\"] \nRegression with ARIMA(0,0,1)(0,0,2)[12] errors \n\nCoefficients:\n          ma1     sma1     sma2  intercept  Consumption  Production\n      -0.6175  -0.3055  -0.5099  -248.4645        4e-04      -2e-04\ns.e.   0.1470   0.1103   0.0763     6.0078        1e-04       0e+00\n\nsigma^2 = 55394:  log likelihood = -1489.81\nAIC=2993.62   AICc=2994.15   BIC=3017.24\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE      MASE      ACF1\nTraining set 5.983612 232.0674 162.3454 -19.40169 99.99742 0.7961153 0.1138366\n\n\nView Code\ncheckresiduals(fit_env_impact, theme = theme_bw())\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,0,1)(0,0,2)[12] errors\nQ* = 246.45, df = 21, p-value < 2.2e-16\n\nModel df: 3.   Total lags used: 24\n\n\nBest model: ARIMA(0,0,1)(0,0,2)[12]\n\n\n\n\n\nManual Model Fitting\n\nNatural gas demandEconometric DynamicsExportsCO2 Emissions\n\n\n\n\nView Code\ndemand$Consumption<-ts(demand$Consumption,star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\ndemand$Production<-ts(demand$Production,star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\ndemand$temperature<-ts(demand$temperature,star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\ndemand$storage<-ts(demand$storage,star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\n############# First fit the linear model##########\nfit_demand.reg <- lm(Consumption ~ Production+temperature+storage, data=demand)\nsummary(fit_demand.reg)\n\n\n\nCall:\nlm(formula = Consumption ~ Production + temperature + storage, \n    data = demand)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-423377 -179561   -8729  202940  528191 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.289e+06  1.523e+05  15.031   <2e-16 ***\nProduction   5.123e-01  2.859e-02  17.918   <2e-16 ***\ntemperature -2.041e+04  1.054e+03 -19.366   <2e-16 ***\nstorage     -4.938e-02  2.122e-02  -2.327   0.0209 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 225800 on 212 degrees of freedom\nMultiple R-squared:  0.7787,    Adjusted R-squared:  0.7756 \nF-statistic: 248.6 on 3 and 212 DF,  p-value: < 2.2e-16\n\n\nView Code\ncheckresiduals(fit_demand.reg, theme = theme_bw())\n\n\n\n\n\n\n    Breusch-Godfrey test for serial correlation of order up to 10\n\ndata:  Residuals\nLM test = 172.11, df = 10, p-value < 2.2e-16\n\n\nThe ACF plot suggests that there’s autocorrelation at certain lags that the model hasn’t accounted for.Residuals Distribution resemble a normal distribution, which is a good sign of model fit.\n\n\n\n\nView Code\neco$price<-ts(eco$price,star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\neco$gdp<-ts(eco$gdp,star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\neco$unemp<-ts(eco$unemp,star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n############# First fit the linear model##########\nfit_eco.reg <- lm(price ~ gdp+unemp, data=eco)\nsummary(fit_eco.reg)\n\n\n\nCall:\nlm(formula = price ~ gdp + unemp, data = eco)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7756 -1.5938 -0.1842  0.7598  5.6088 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.385e+01  1.672e+00   8.285 6.02e-12 ***\ngdp         -3.668e-04  6.976e-05  -5.259 1.54e-06 ***\nunemp       -4.647e-01  1.157e-01  -4.017 0.000148 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.907 on 69 degrees of freedom\nMultiple R-squared:  0.3164,    Adjusted R-squared:  0.2966 \nF-statistic: 15.97 on 2 and 69 DF,  p-value: 2.001e-06\n\n\nView Code\ncheckresiduals(fit_eco.reg, theme = theme_bw())\n\n\n\n\n\n\n    Breusch-Godfrey test for serial correlation of order up to 10\n\ndata:  Residuals\nLM test = 49.407, df = 10, p-value = 3.43e-07\n\n\nThe ACF plot does not display significant autocorrelation in the residuals, this generally indicates that the residuals are random, which is a sign of a good fit. The histogram shows some deviations from normality, especially with a skew towards the right.\n\n\n\n\nView Code\nexport$consumption<-ts(export$consumption,star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nexport$fx_rate<-ts(export$fx_rate,star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nexport$gdp_nl<-ts(export$gdp_nl,star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n############# First fit the linear model##########\nfit_export.reg <- lm(exports ~ consumption+fx_rate+gdp_nl, data=export)\nsummary(fit_export.reg)\n\n\n\nCall:\nlm(formula = exports ~ consumption + fx_rate + gdp_nl, data = export)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-114272  -29193   -5976   19885  115631 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.476e+06  2.234e+05  -6.609 7.14e-09 ***\nconsumption  4.085e-02  1.622e-02   2.519   0.0141 *  \nfx_rate      5.554e+03  2.097e+03   2.648   0.0101 *  \ngdp_nl       5.956e+00  2.619e-01  22.742  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45270 on 68 degrees of freedom\nMultiple R-squared:  0.9298,    Adjusted R-squared:  0.9267 \nF-statistic: 300.1 on 3 and 68 DF,  p-value: < 2.2e-16\n\n\nView Code\ncheckresiduals(fit_export.reg, theme = theme_bw())\n\n\n\n\n\n\n    Breusch-Godfrey test for serial correlation of order up to 10\n\ndata:  Residuals\nLM test = 47.27, df = 10, p-value = 8.432e-07\n\n\nThe ACF plot indicates significant autocorrelations at multiple lags. This is a strong indication that the model has not fully captured the autocorrelative structure of the data, and there may be additional lags or other dependencies that should be incorporated into the model. The shape of residual plot is also not perfectly symmetrical, indicating skewness in the distribution of residuals. Overall, the residuals exhibit some problematic features such as trends, heavy tails, skewness, and significant autocorrelation, all of which suggest that the current model could be improved to better fit the underlying dynamics of the data.\n\n\n\n\nView Code\nenv_impact$co2_value<-ts(env_impact$co2_value,star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\nenv_impact$Consumption<-ts(env_impact$Consumption,star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\nenv_impact$Production<-ts(env_impact$Production,star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\n############# First fit the linear model##########\nfit_env_impact.reg <- lm(co2_value ~ Consumption+Production, data=env_impact)\nsummary(fit_env_impact.reg)\n\n\n\nCall:\nlm(formula = co2_value ~ Consumption + Production, data = env_impact)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-446.15 -164.77   -5.79   50.29 1172.80 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.106e+02  1.046e+02  -2.969 0.003305 ** \nConsumption  4.164e-04  4.521e-05   9.209  < 2e-16 ***\nProduction  -1.565e-04  4.225e-05  -3.703 0.000267 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 295.9 on 231 degrees of freedom\nMultiple R-squared:  0.274, Adjusted R-squared:  0.2678 \nF-statistic:  43.6 on 2 and 231 DF,  p-value: < 2.2e-16\n\n\nView Code\ncheckresiduals(fit_env_impact.reg, theme = theme_bw())\n\n\n\n\n\n\n    Breusch-Godfrey test for serial correlation of order up to 10\n\ndata:  Residuals\nLM test = 90.652, df = 10, p-value = 3.977e-15\n\n\nACF Plot suggests that there is little autocorrelation in the residuals, indicating that the model has accounted for the time series autocorrelation effectively.For the distribution of Residuals plot, while there’s a noticeable peak that aligns with the normal distribution, the long tails suggest the presence of outliers or extreme values that the model did not predict accurately.\n\n\n\n\n\nFitting residuals\n\nNatural gas demandEconometric DynamicsExportsCO2 Emissions\n\n\n\n\nView Code\nres.fit_demand<-ts(residuals(fit_demand.reg),star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n############## Then look at the residuals ############\nacf(res.fit_demand,main=\"ACF Plot\")\n\n\n\n\n\n\n\nView Code\nPacf(res.fit_demand,main=\"PACF Plot\")\n\n\n\n\n\n\n\nView Code\nacf(diff(res.fit_demand, lag = 12),main=\"ACF Plot: Seasonality\")\n\n\n\n\n\n\n\nView Code\nPacf(diff(res.fit_demand, lag = 12),main=\"PACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\n\nView Code\nres.fit_eco<-ts(residuals(fit_eco.reg),star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit_eco,main=\"ACF Plot\")\n\n\n\n\n\n\n\nView Code\nPacf(res.fit_eco,main=\"PACF Plot\")\n\n\n\n\n\n\n\n\n\nView Code\nres.fit_export<-ts(residuals(fit_export.reg),star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit_export,main=\"ACF Plot\")\n\n\n\n\n\n\n\nView Code\nPacf(res.fit_export,main=\"PACF Plot\")\n\n\n\n\n\n\n\n\n\nView Code\nres.fit_env_impact<-ts(residuals(fit_env_impact.reg),star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n############## Then look at the residuals ############\nacf(res.fit_env_impact,main=\"ACF Plot\")\n\n\n\n\n\n\n\nView Code\nPacf(res.fit_env_impact,main=\"PACF Plot\")\n\n\n\n\n\n\n\n\n\n\nFinding Model Parameters\n\nNatural gas demandEconometric DynamicsExportsCO2 Emissions\n\n\np = 1,2,3,4\nq = 1,2,3,4\nP = 1\nQ = 1,2,3\nD = 1\nd = 0\n\n\nView Code\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,d1,d2,data){\n  \n  temp=c()\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*150),nrow=150)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          for(d in d1:d2)\n       \n        {\n          if(p+d+q+P+D+Q<=10)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1),method=\"ML\")\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n\n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  }\n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\n\noutput=SARIMA.c(p1=1,p2=4,q1=1,q2=4,P1=1,P2=1,Q1=1,Q2=3,d1=0,d2=0,data=residuals(fit_demand.reg))\n\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q      AIC    BIC     AICc\n38 3 0 2 0 1 0 5575.973 5599.6 5576.511\n\n\n\n\nView Code\noutput[which.min(output$BIC),]\n\n\n   p d q P D Q      AIC    BIC     AICc\n38 3 0 2 0 1 0 5575.973 5599.6 5576.511\n\n\n\n\nView Code\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q      AIC    BIC     AICc\n38 3 0 2 0 1 0 5575.973 5599.6 5576.511\n\n\nModel 1 (Best Model) : ARIMA(3,0,2)(0,1,0)[12]\nModel 2 (Auto Arima): ARIMA(0,0,2)(0,1,2)[12]\n\n\np = 1\nq = 1,2,3,4\nd = 0,1\n\n\nView Code\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=8)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(0,1,0,4,data=residuals(fit_eco.reg))\n\noutput[which.min(output$AIC),] \n\n\n  p d q     AIC      BIC     AICc\n8 0 1 3 227.575 236.6257 228.1811\n\n\n\n\nView Code\noutput[which.min(output$BIC),]\n\n\n  p d q     AIC      BIC   AICc\n2 0 1 0 232.012 234.2747 232.07\n\n\n\n\nView Code\noutput[which.min(output$AICc),]\n\n\n  p d q     AIC      BIC     AICc\n8 0 1 3 227.575 236.6257 228.1811\n\n\nModel 1 (Best Model) : ARIMA(0,1,3)\nModel 2 (Second Best Model) : ARIMA(0,1,0)\nModel 3 (Auto Arima): ARIMA(0,1,0)(0,0,2)[4]\nAlthough Auto Arima have seasonal components, I opted for an ARIMA model over SARIMA based on manual analysis, which indicated that the seasonal components (P=0, D=0, Q=0) were unnecessary. This simpler ARIMA model aligns with the principle of parsimony, avoiding overfitting while adequately modeling the data without the inclusion of statistically insignificant seasonal terms.\n\n\np = 1\nq = 1,2,3,4\nd = 0,1\n\n\nView Code\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=8)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(0,1,0,4,data=residuals(fit_export.reg))\n\noutput[which.min(output$AIC),] \n\n\n   p d q      AIC      BIC     AICc\n14 1 1 1 1662.293 1669.081 1662.651\n\n\n\n\nView Code\noutput[which.min(output$BIC),]\n\n\n   p d q      AIC      BIC     AICc\n12 1 1 0 1664.272 1668.798 1664.449\n\n\n\n\nView Code\noutput[which.min(output$AICc),]\n\n\n   p d q      AIC      BIC     AICc\n14 1 1 1 1662.293 1669.081 1662.651\n\n\nModel 1 (Best Model) : ARIMA(1,1,1)\nModel 2 (Second Best Model) : ARIMA(1,1,0)\nModel 3 (Auto Arima): ARIMA(1,0,0)\n\n\np = 1,2,3\nq = 1\nd = 0,1\n\n\nView Code\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=8)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(1,3,1,1,data=residuals(fit_env_impact.reg))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC     BIC     AICc\n3 2 0 1 3253.174 3270.45 3253.437\n\n\n\n\nView Code\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n1 1 0 1 3253.988 3267.809 3254.162\n\n\n\n\nView Code\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC     BIC     AICc\n3 2 0 1 3253.174 3270.45 3253.437\n\n\nModel 1 (Best Model) : ARIMA(2,0,1)\nModel 2 (Second Best Model) : ARIMA(1,0,1)\nModel 3 (Auto Arima): ARIMA(0,0,1)(0,0,2)[12]\nAlthough Auto Arima have seasonal components, I opted for an ARIMA model over SARIMA based on manual analysis, which indicated that the seasonal components (P=0, D=0, Q=0) were unnecessary. This simpler ARIMA model aligns with the principle of parsimony, avoiding overfitting while adequately modeling the data without the inclusion of statistically insignificant seasonal terms.\n\n\n\n\n\nModel Diagnostics\n\nNatural gas demandEconometric DynamicsExportsCO2 Emissions\n\n\n\nModel 1Model 2\n\n\n\n\nView Code\nmodel_output_demand1 <- capture.output(sarima(res.fit_demand, 3,0,2,0,1,0,12)) \n\n\n\n\n\n\n\nView Code\ncat(model_output_demand1[98:109], model_output_demand1[length(model_output_demand1)], sep = \"\\n\")\n\n\nCoefficients: \n         Estimate        SE  t.value p.value\nar1        1.3989    0.0668  20.9360  0.0000\nar2       -1.2696    0.0804 -15.7986  0.0000\nar3        0.3892    0.0668   5.8256  0.0000\nma1       -1.0461    0.0267 -39.1784  0.0000\nma2        1.0000    0.0465  21.4826  0.0000\nconstant -48.3397 1169.8796  -0.0413  0.9671\n\nsigma^2 estimated as 10343181207 on 198 degrees of freedom \n \nAIC = 25.99005  AICc = 25.99214  BIC = 26.10391 \n \n\n\n\n\n\n\nView Code\nmodel_output_demand2 <- capture.output(sarima(res.fit_demand, 0,0,2,0,1,2,12)) \n\n\n\n\n\n\n\nView Code\ncat(model_output_demand2[28:38], model_output_demand2[length(model_output_demand2)], sep = \"\\n\")\n\n\nCoefficients: \n         Estimate       SE t.value p.value\nma1        0.4491   0.0717  6.2601  0.0000\nma2        0.2045   0.0598  3.4195  0.0008\nsma1      -0.6976   0.0849 -8.2168  0.0000\nsma2      -0.1778   0.0850 -2.0933  0.0376\nconstant 147.5125 200.8884  0.7343  0.4636\n\nsigma^2 estimated as 7248694815 on 199 degrees of freedom \n \nAIC = 25.67953  AICc = 25.68102  BIC = 25.77712 \n \n\n\n\n\n\nThe ACF plot reveals that most autocorrelation values are indeed within the confidence interval, indicating a good fit for both the models. The Q-Q plot suggest that the residuals are normally distributed for both the models.From the Ljung-Box Test, the p-values for model 2 are typically above 0.05 suggesting that the residuals are random, which is desirable in a good model fit. On comparing AIC, AICc, model 2 has lower values indicating a better fit.\n\n\n\nModel 1Model 2Model 3\n\n\n\n\nView Code\nmodel_output_eco1 <- capture.output(sarima(res.fit_eco, 0,1,3)) \n\n\n\n\n\n\n\nView Code\ncat(model_output_eco1[26:35], model_output_eco1[length(model_output_eco1)], sep = \"\\n\")\n\n\nCoefficients: \n         Estimate     SE t.value p.value\nma1       -0.0127 0.1180 -0.1077  0.9145\nma2       -0.3783 0.1036 -3.6520  0.0005\nma3       -0.1599 0.1054 -1.5164  0.1341\nconstant   0.0239 0.0636  0.3760  0.7081\n\nsigma^2 estimated as 1.279842 on 67 degrees of freedom \n \nAIC = 3.231425  AICc = 3.239961  BIC = 3.390769 \n \n\n\n\n\n\n\nView Code\nmodel_output_eco2 <- capture.output(sarima(res.fit_eco, 0,1,0)) \n\n\n\n\n\n\n\nView Code\ncat(model_output_eco2[10:17], model_output_eco2[length(model_output_eco2)], sep = \"\\n\")\n\n\n \nCoefficients: \n         Estimate    SE t.value p.value\nconstant   0.0469 0.145  0.3235  0.7473\n\nsigma^2 estimated as 1.492206 on 70 degrees of freedom \n \nAIC = 3.294471  AICc = 3.295287  BIC = 3.358208 \n \n\n\n\n\n\n\nView Code\nmodel_output_eco3 <- capture.output(sarima(res.fit_eco, 0,1,0,0,0,2,4)) \n\n\n\n\n\n\n\nView Code\ncat(model_output_eco3[25:34], model_output_eco3[length(model_output_eco3)], sep = \"\\n\")\n\n\nCoefficients: \n         Estimate     SE t.value p.value\nsma1      -0.2987 0.1580 -1.8906  0.0629\nsma2      -0.1484 0.1663 -0.8923  0.3754\nconstant   0.0241 0.0837  0.2876  0.7746\n\nsigma^2 estimated as 1.392162 on 68 degrees of freedom \n \nAIC = 3.291319  AICc = 3.296364  BIC = 3.418794 \n \n \n\n\n\n\n\nThe ACF plot reveals that most autocorrelation values are indeed within the confidence interval, indicating a good fit for all the models. The Q-Q plot suggest that the residuals are normally distributed for all the models.From the Ljung-Box Test, the p-values for model 1 are typically above 0.05 suggesting that the residuals are random, which is desirable in a good model fit. On comparing AIC, AICc, model 1 has lower values indicating a better fit.\n\n\n\nModel 1Model 2Model 3\n\n\n\n\nView Code\nmodel_output_export1 <- capture.output(sarima(res.fit_export, 1,1,1)) \n\n\n\n\n\n\n\nView Code\ncat(model_output_export1[152:161], model_output_export1[length(model_output_export1)], sep = \"\\n\")\n\n\nCoefficients: \n           Estimate        SE  t.value p.value\nar1         -0.9703    0.0593 -16.3552  0.0000\nma1          0.8652    0.1209   7.1575  0.0000\nconstant -2972.9302 3131.5295  -0.9494  0.3458\n\nsigma^2 estimated as 776009007 on 68 degrees of freedom \n \nAIC = 23.42813  AICc = 23.43317  BIC = 23.5556 \n \n \n\n\n\n\n\n\nView Code\nmodel_output_export2 <- capture.output(sarima(res.fit_export, 1,1,0)) \n\n\n\n\n\n\n\nView Code\ncat(model_output_export2[20:28], model_output_export2[length(model_output_export2)], sep = \"\\n\")\n\n\nCoefficients: \n           Estimate        SE t.value p.value\nar1         -0.2856    0.1138 -2.5099  0.0144\nconstant -2695.6108 2659.9601 -1.0134  0.3144\n\nsigma^2 estimated as 824886684 on 69 degrees of freedom \n \nAIC = 23.45434  AICc = 23.45682  BIC = 23.54995 \n \n \n\n\n\n\n\n\nView Code\nmodel_output_export3 <- capture.output(sarima(res.fit_export, 1,0,0)) \n\n\n\n\n\n\n\nView Code\ncat(model_output_export3[27:35], model_output_export3[length(model_output_export3)], sep = \"\\n\")\n\n\nCoefficients: \n        Estimate         SE t.value p.value\nar1       0.7996     0.0804  9.9421  0.0000\nxmean -1456.9356 16169.8092 -0.0901  0.9285\n\nsigma^2 estimated as 838187240 on 70 degrees of freedom \n \nAIC = 23.48213  AICc = 23.48455  BIC = 23.57699 \n \n \n\n\n\n\n\nThe ACF plot reveals that most autocorrelation values are indeed within the confidence interval, indicating a good fit for all the models. The Q-Q plot suggest that the residuals are normally distributed for all the models.From the Ljung-Box Test, the p-values for model 1 are typically above 0.05 suggesting that the residuals are random, which is desirable in a good model fit. On comparing AIC, AICc, model 1 has lower values indicating a better fit.\n\n\n\nModel 1Model 2Model 3\n\n\n\n\nView Code\nmodel_output_env_impact1 <- capture.output(sarima(res.fit_env_impact, 2,0,1)) \n\n\n\n\n\n\n\nView Code\ncat(model_output_env_impact1[26:37], model_output_env_impact1[length(model_output_env_impact1)], sep = \"\\n\")\n\n\n \nCoefficients: \n      Estimate     SE  t.value p.value\nar1     0.1754 0.0847   2.0719  0.0394\nar2    -0.1298 0.0764  -1.6981  0.0908\nma1    -0.7776 0.0592 -13.1299  0.0000\nxmean   0.0348 3.8214   0.0091  0.9927\n\nsigma^2 estimated as 60969.9 on 230 degrees of freedom \n \nAIC = 13.90245  AICc = 13.9032  BIC = 13.97628 \n \n \n\n\n\n\n\n\nView Code\nmodel_output_env_impact2 <- capture.output(sarima(res.fit_env_impact, 1,0,1)) \n\n\n\n\n\n\n\nView Code\ncat(model_output_env_impact2[29:37], model_output_env_impact2[length(model_output_env_impact2)], sep = \"\\n\")\n\n\nCoefficients: \n      Estimate     SE  t.value p.value\nar1     0.2019 0.0801   2.5206  0.0124\nma1    -0.8249 0.0416 -19.8089  0.0000\nxmean  -0.0189 3.6342  -0.0052  0.9958\n\nsigma^2 estimated as 61720.43 on 231 degrees of freedom \n \nAIC = 13.90593  AICc = 13.90638  BIC = 13.965 \n \n\n\n\n\n\n\nView Code\nmodel_output_env_impact3 <- capture.output(sarima(res.fit_env_impact, 0,0,1,0,0,2,12)) \n\n\n\n\n\n\n\nView Code\ncat(model_output_env_impact3[38:47], model_output_env_impact3[length(model_output_env_impact3)], sep = \"\\n\")\n\n\nCoefficients: \n      Estimate     SE t.value p.value\nma1    -0.5051 0.1220 -4.1419  0.0000\nsma1   -0.3161 0.1040 -3.0396  0.0026\nsma2   -0.4309 0.0694 -6.2139  0.0000\nxmean  -1.6697 2.6574 -0.6283  0.5304\n\nsigma^2 estimated as 56381.74 on 230 degrees of freedom \n \nAIC = 13.86174  AICc = 13.86249  BIC = 13.93557 \n \n\n\n\n\n\nThe ACF plot reveals that most autocorrelation values are indeed within the confidence interval, indicating a good fit for all the models. The Q-Q plot suggest that the residuals are normally distributed for model 3.From the Ljung-Box Test, the p-values for model 1 and 2 are typically above 0.05 suggesting that the residuals are random, which is desirable in a good model fit. On comparing AIC, AICc, model 3 has lower values indicating a better fit.\n\n\n\n\n\nCross Validation\n\nNatural gas demandEconometric DynamicsExportsCO2 Emissions\n\n\nModel 1 (Best Model) : ARIMA(3,0,2)(0,1,0)[12]\nModel 2 (Auto Arima): ARIMA(0,0,2)(0,1,2)[12]\n\n\nView Code\nx=res.fit_demand\n  \nset.seed(123)\nk <- 124# first training set length \nn <- length(x) \n\n#n-k=168; 168/12=14; k=144\n\n# matrix(NA,(n-k)/4,4)\nres1 <- matrix(NA,(n-k)/4,12)\nres2 <- matrix(NA,(n-k)/4,12)\n\nmae1 <- matrix(NA, (n-k)/4,12)\nmae2 <- matrix(NA,(n-k)/4,12)\n\nst <- tsp(x)[1]+(k-1)/12\n\nfor(i in 1:(n-k)/12)\n{\n  #xtrain <- window(a10, start=st+(i-k+1)/4, end=st+i/4)\n  xtrain <- window(x, end=st + i-1)\n  xtest <- window(x, start=st + (i-1) + 1/12, end=st + i)\n\n  fit <- Arima(xtrain, order=c(3,0,2), seasonal=list(order=c(0,1,0), period=12),\n                include.drift=TRUE,  method=\"CSS\")\n  fcast <- forecast(fit, h=12)\n  \n  fit2 <-Arima(xtrain, order=c(0,0,2), seasonal=list(order=c(0,1,2), period=12),\n                include.drift=TRUE,  method=\"CSS\")\n  fcast2 <- forecast(fit2, h=12)\n\n  \n  mae1[i,] <- abs(fcast$mean-xtest)\n  mae2[i,] <- abs(fcast2$mean-xtest)\n\n  ####### to calculate RMSE ##########\n  res1[i,] <- (fcast$mean-xtest)^2\n  res2[i,] <- (fcast2$mean-xtest)^2\n\n}\n\nplot(1:12, colMeans(mae1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"MAE\")\nlines(1:12, colMeans(mae2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topright\",legend=c(\"Fit1:SARIMA(3,0,2)x(0,1,0)\",\"Fit2:SARIMA(0,0,2)x(0,1,2)\"),col=2:4,lty=1)\n\n\n\n\n\n\n\nView Code\n########### RMSE ############\n\nrmse1 =sqrt(colMeans(res1,na.rm=TRUE))\nrmse2=sqrt(colMeans(res2,na.rm=TRUE))\n\nplot(1:12, rmse1, type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:12, rmse2, type=\"l\",col=3)\n\nlegend(\"topright\",legend=c(\"Fit1:SARIMA(3,0,2)x(0,1,0)\",\"Fit2:SARIMA(0,0,2)x(0,1,2)\"),col=2:4,lty=1)\n\n\n\n\n\nFit2:SARIMA(0,0,2)x(0,1,2) is better as it has lower MSE and RMSE.\n\n\n\n\nView Code\nx=res.fit_eco\n  \nset.seed(123)\nk <- 51# first training set length \nn <- length(x) \n\n#n-k=168; 168/12=14; k=144\n\n# matrix(NA,(n-k)/4,4)\nres1 <- matrix(NA,(n-k)/4,4)\nres2 <- matrix(NA,(n-k)/4,4)\nres3 <- matrix(NA,(n-k)/4,4)\n\nmae1 <- matrix(NA, (n-k)/4,4)\nmae2 <- matrix(NA,(n-k)/4,4)\nmae3 <- matrix(NA,(n-k)/4,4)\n\n\n\nst <- tsp(x)[1]+(k-1)/4\n\nfor(i in 1:(n-k)/4)\n{\n  #xtrain <- window(a10, start=st+(i-k+1)/4, end=st+i/4)\n  xtrain <- window(x, end=st + i-1)\n  xtest <- window(x, start=st + (i-1) + 1/4, end=st + i)\n\n  fit <- Arima(xtrain, order=c(0,1,3),include.drift=TRUE)\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <-Arima(xtrain, order=c(0,1,0),include.drift=TRUE)\n  fcast2 <- forecast(fit2, h=4)\n \n  fit3 <- Arima(xtrain, order=c(0,1,0), seasonal=list(order=c(0,0,2), period=4),\n                include.drift=TRUE,  method=\"CSS\")\n  fcast3 <- forecast(fit3, h=4)\n  \n  mae1[i,] <- abs(fcast$mean-xtest)\n  mae2[i,] <- abs(fcast2$mean-xtest)\n  mae3[i,] <- abs(fcast3$mean-xtest)\n  \n  ####### to calculate RMSE ##########\n  res1[i,] <- (fcast$mean-xtest)^2\n  res2[i,] <- (fcast2$mean-xtest)^2\n  res3[i,] <- (fcast3$mean-xtest)^2\n  \n  \n}\n\nplot(1:4, colMeans(mae1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"MAE\")\nlines(1:4, colMeans(mae2,na.rm=TRUE), type=\"l\",col=3)\nlines(1:4, colMeans(mae3,na.rm=TRUE), type=\"l\",col=4)\nlegend(\"topright\",legend=c(\"Fit1:ARIMA(0,1,3)\",\"Fit2:ARIMA(0,1,0)\",\"Fit3:ARIMA(0,1,0)(0,0,2)\"),col=2:5,lty=1)\n\n\n\n\n\n\n\nView Code\n########### RMSE ############\n\nrmse1 =sqrt(colMeans(res1,na.rm=TRUE))\nrmse2=sqrt(colMeans(res2,na.rm=TRUE))\nrmse3=sqrt(colMeans(res3,na.rm=TRUE))\n\n\nplot(1:4, rmse1, type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, rmse2, type=\"l\",col=3)\nlines(1:4, rmse3, type=\"l\",col=4)\nlegend(\"topright\",legend=c(\"Fit1:ARIMA(0,1,3)\",\"Fit2:ARIMA(0,1,0)\",\"Fit3:ARIMA(0,1,0)(0,0,2)\"),col=2:5,lty=1)\n\n\n\n\n\nFit2:ARIMA(0,1,0) is better as it has overall lower MSE and RMSE.\n\n\n\n\nView Code\nx=res.fit_export\n  \nset.seed(123)\nk <- 51# first training set length \nn <- length(x) \n\n#n-k=168; 168/12=14; k=144\n\n# matrix(NA,(n-k)/4,4)\nres1 <- matrix(NA,(n-k)/4,4)\nres2 <- matrix(NA,(n-k)/4,4)\nres3 <- matrix(NA,(n-k)/4,4)\n\nmae1 <- matrix(NA, (n-k)/4,4)\nmae2 <- matrix(NA,(n-k)/4,4)\nmae3 <- matrix(NA,(n-k)/4,4)\n\n\n\nst <- tsp(x)[1]+(k-1)/4\n\nfor(i in 1:(n-k)/4)\n{\n  #xtrain <- window(a10, start=st+(i-k+1)/4, end=st+i/4)\n  xtrain <- window(x, end=st + i-1)\n  xtest <- window(x, start=st + (i-1) + 1/4, end=st + i)\n\n  fit <- Arima(xtrain, order=c(1,1,1),include.drift=TRUE)\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <-Arima(xtrain, order=c(1,1,0),include.drift=TRUE)\n  fcast2 <- forecast(fit2, h=4)\n  \n  fit3 <- Arima(xtrain, order=c(1,0,0),include.drift=TRUE)\n  fcast3 <- forecast(fit3, h=4)\n  \n  mae1[i,] <- abs(fcast$mean-xtest)\n  mae2[i,] <- abs(fcast2$mean-xtest)\n  mae3[i,] <- abs(fcast3$mean-xtest)\n  \n  ####### to calculate RMSE ##########\n  res1[i,] <- (fcast$mean-xtest)^2\n  res2[i,] <- (fcast2$mean-xtest)^2\n  res3[i,] <- (fcast3$mean-xtest)^2\n  \n  \n}\n\nplot(1:4, colMeans(mae1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"MAE\")\nlines(1:4, colMeans(mae2,na.rm=TRUE), type=\"l\",col=3)\nlines(1:4, colMeans(mae3,na.rm=TRUE), type=\"l\",col=4)\nlegend(\"topright\",legend=c(\"Fit1:ARIMA(1,1,1)\",\"Fit2:ARIMA(1,1,0)\",\"Fit3:ARIMA(1,0,0)\"),col=2:5,lty=1)\n\n\n\n\n\n\n\nView Code\n########### RMSE ############\n\nrmse1 =sqrt(colMeans(res1,na.rm=TRUE))\nrmse2=sqrt(colMeans(res2,na.rm=TRUE))\nrmse3=sqrt(colMeans(res3,na.rm=TRUE))\n\n\nplot(1:4, rmse1, type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, rmse2, type=\"l\",col=3)\nlines(1:4, rmse3, type=\"l\",col=4)\nlegend(\"topright\",legend=c(\"Fit1:ARIMA(1,1,1)\",\"Fit2:ARIMA(1,1,0)\",\"Fit3:ARIMA(1,0,0)\"),col=2:5,lty=1)\n\n\n\n\n\nFit3:ARIMA(1,0,0) is better as it has overall lower MSE and RMSE.\n\n\n\n\nView Code\nx=res.fit_env_impact\n  \nset.seed(123)\nk <- 144# first training set length \nn <- length(x) \n\n#n-k=168; 168/12=14; k=144\n\n# matrix(NA,(n-k)/4,4)\nres1 <- matrix(NA,(n-k)/4,12)\nres2 <- matrix(NA,(n-k)/4,12)\nres3 <- matrix(NA,(n-k)/4,12)\n\nmae1 <- matrix(NA, (n-k)/4,12)\nmae2 <- matrix(NA,(n-k)/4,12)\nmae3 <- matrix(NA,(n-k)/4,12)\n\n\n\nst <- tsp(x)[1]+(k-1)/12\n\nfor(i in 1:(n-k)/12)\n{\n  #xtrain <- window(a10, start=st+(i-k+1)/4, end=st+i/4)\n  xtrain <- window(x, end=st + i-1)\n  xtest <- window(x, start=st + (i-1) + 1/12, end=st + i)\n\n  fit <- Arima(xtrain, order=c(2,0,1),include.drift=TRUE)\n  fcast <- forecast(fit, h=12)\n  \n  fit2 <-Arima(xtrain, order=c(1,0,1),include.drift=TRUE)\n  fcast2 <- forecast(fit2, h=12)\n  \n  fit3 <- Arima(xtrain, order=c(0,0,1), seasonal=list(order=c(0,0,2), period=12),\n                include.drift=TRUE,  method=\"CSS\")\n  fcast3 <- forecast(fit3, h=12)\n  \n  mae1[i,] <- abs(fcast$mean-xtest)\n  mae2[i,] <- abs(fcast2$mean-xtest)\n  mae3[i,] <- abs(fcast3$mean-xtest)\n  \n  ####### to calculate RMSE ##########\n  res1[i,] <- (fcast$mean-xtest)^2\n  res2[i,] <- (fcast2$mean-xtest)^2\n  res3[i,] <- (fcast3$mean-xtest)^2\n  \n  \n}\n\nplot(1:12, colMeans(mae1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"MAE\")\nlines(1:12, colMeans(mae2,na.rm=TRUE), type=\"l\",col=3)\nlines(1:12, colMeans(mae3,na.rm=TRUE), type=\"l\",col=4)\nlegend(\"topright\",legend=c(\"Fit1:ARIMA(2,0,1)\",\"Fit2:ARIMA(1,0,1)\",\"Fit3:ARIMA(0,0,1)(0,0,2)[12]\"),col=2:5,lty=1)\n\n\n\n\n\n\n\nView Code\n########### RMSE ############\n\nrmse1 =sqrt(colMeans(res1,na.rm=TRUE))\nrmse2=sqrt(colMeans(res2,na.rm=TRUE))\nrmse3=sqrt(colMeans(res3,na.rm=TRUE))\n\n\nplot(1:12, rmse1, type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:12, rmse2, type=\"l\",col=3)\nlines(1:12, rmse3, type=\"l\",col=4)\nlegend(\"topright\",legend=c(\"Fit1:ARIMA(2,0,1)\",\"Fit2:ARIMA(1,0,1)\",\"Fit3:ARIMA(0,0,1)(0,0,2)[12]\"),col=2:5,lty=1)\n\n\n\n\n\nFit1:ARIMA(2,0,1) is better as it has overall lower MSE and RMSE.\n\n\n\n\n\nFinal Model Fitting\n\nNatural gas demandEconometric DynamicsExportsCO2 Emissions\n\n\n\n\nView Code\n# best model fit for forcasting\nxreg_demand <- cbind(temperature = demand_ts[, \"temperature\"], Production = demand_ts[, \"Production\"],\n              storage =  demand_ts[, \"storage\"])\n\nfit_demand_ts <- Arima(demand_ts[, \"Consumption\"],order=c(0,0,2),seasonal = c(0,1,2),xreg=xreg_demand)\nsummary(fit_demand_ts)\n\n\nSeries: demand_ts[, \"Consumption\"] \nRegression with ARIMA(0,0,2)(0,1,2)[12] errors \n\nCoefficients:\n         ma1     ma2     sma1     sma2  temperature  Production  storage\n      0.4815  0.2300  -0.6999  -0.1436   -30050.877      0.5029  -0.0279\ns.e.  0.0773  0.0636   0.0812   0.0815     2826.539      0.0263   0.0310\n\nsigma^2 = 7.188e+09:  log likelihood = -2607.83\nAIC=5231.66   AICc=5232.4   BIC=5258.21\n\nTraining set error measures:\n                   ME     RMSE     MAE       MPE     MAPE      MASE\nTraining set 6183.833 80968.66 63415.8 0.1870671 2.888607 0.5424765\n                     ACF1\nTraining set -0.000946875\n\n\nModel_Equation\n\n    \n        \\(consumption_{t} = \\mu+0.4815\\varepsilon_{t-1} +0.2300\\varepsilon_{t-2}−0.6999\\varepsilon_{t-12}−0.1436\\varepsilon_{t-24} \\)\n        \\(−30050.877temperature_t+0.5029Production_t−0.0279Storage_t+\\varepsilon_t\\)\n    \n\n\n\n\n\n\nView Code\n# best model fit for forcasting\nxreg_eco <- cbind( gdp =  eco_ts[, \"gdp\"],unemp = eco_ts[, \"unemp\"])\n\nfit_eco_ts <- Arima(eco_ts[, \"price\"],order=c(0,1,0),xreg=xreg_eco)\nsummary(fit_eco_ts)\n\n\nSeries: eco_ts[, \"price\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        gdp    unemp\n      2e-04  -0.0586\ns.e.  6e-04   0.1777\n\nsigma^2 = 1.395:  log likelihood = -111.55\nAIC=229.1   AICc=229.46   BIC=235.89\n\nTraining set error measures:\n                      ME     RMSE       MAE      MPE     MAPE      MASE\nTraining set -0.04566038 1.156305 0.7787039 -2.88408 16.12788 0.5715875\n                  ACF1\nTraining set 0.1185369\n\n\nModel_Equation\n\\(price_{t} = \\mu+ (2e-04)gdp_t-0.0586unemp_t+\\varepsilon_t\\)\n\n\n\n\nView Code\n# best model fit for forcasting\nxreg_export <- cbind(\n              consumption =  export_ts[, \"consumption\"],fx_rate = export_ts[, \"fx_rate\"],gdp_nl = export_ts[, \"gdp_nl\"])\n\nfit_export_ts <- Arima(export_ts[, \"exports\"],order=c(1,0,0),xreg=xreg_export)\nsummary(fit_export_ts)\n\n\nSeries: export_ts[, \"exports\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  intercept  consumption    fx_rate  gdp_nl\n      0.9658  -66757.46       0.0257  -2370.611  2.7993\ns.e.  0.0323  327974.10       0.0061   2505.615  0.8929\n\nsigma^2 = 628058050:  log likelihood = -830.22\nAIC=1672.43   AICc=1673.72   BIC=1686.09\n\nTraining set error measures:\n                   ME     RMSE     MAE       MPE     MAPE     MASE       ACF1\nTraining set 631.1364 24175.25 14983.5 -3.316443 9.583512 0.433806 -0.1389813\n\n\nModel_Equation\n\n    \n        \\(exports_{t} = -66757.46+0.9658\\varepsilon_{t-1} +0.0257consumption_t \\)\n        \\(-2370.611fxrate_t+2.7993gdpnl_t+\\varepsilon_t\\)\n    \n\n\n\n\n\nView Code\n# best model fit for forcasting\nxreg_env_impact <- cbind(Consumption = env_impact_ts[, \"Consumption\"],\n              Production =  env_impact_ts[, \"Production\"])\n\nfit_env_impact_ts <- Arima(env_impact_ts[, \"co2_value\"],order=c(2,0,1),xreg=xreg_env_impact)\nsummary(fit_env_impact_ts)\n\n\nSeries: env_impact_ts[, \"co2_value\"] \nRegression with ARIMA(2,0,1) errors \n\nCoefficients:\n         ar1      ar2      ma1  intercept  Consumption  Production\n      0.2072  -0.1185  -0.8299  -247.0346        4e-04      -1e-04\ns.e.  0.0782   0.0775   0.0410    16.4180        0e+00       1e-04\n\nsigma^2 = 61520:  log likelihood = -1494.9\nAIC=3003.79   AICc=3004.33   BIC=3027.42\n\nTraining set error measures:\n                   ME    RMSE      MAE       MPE     MAPE      MASE        ACF1\nTraining set 1.732973 244.563 150.1234 -41.40206 80.50691 0.7361805 0.001927893\n\n\nModel_Equation\n\n    \n        \\(co2emissions_{t} = -247.0346 +0.2072\\varepsilon_{t-1} -0.1185 \\varepsilon_{t-2}-0.8299\\varepsilon_{t-12} \\)\n        \\(+(4e-04) consumption_t-(1e-04)production_t+\\varepsilon_t\\)\n    \n\n\n\n\n\n\nForecasting\n\nNatural gas demandEconometric DynamicsExportsCO2 Emissions\n\n\n\n\nView Code\n#fiting an ARIMA model to the temperature variable\ntemperature_fit_demand<-auto.arima(demand$temperature) \nftemperature_demand<-forecast(temperature_fit_demand)\nsummary(temperature_fit_demand) \n\n\nSeries: demand$temperature \nARIMA(1,0,1)(2,1,2)[12] \n\nCoefficients:\n         ar1      ma1    sar1     sar2     sma1    sma2\n      0.8966  -0.7705  0.1582  -0.3150  -1.0719  0.1859\ns.e.  0.0785   0.1071  0.2699   0.0796   0.2711  0.2780\n\nsigma^2 = 3.645:  log likelihood = -430.92\nAIC=875.84   AICc=876.41   BIC=899.06\n\nTraining set error measures:\n                     ME     RMSE      MAE        MPE    MAPE      MASE\nTraining set 0.05894715 1.827797 1.374266 -0.1026046 3.04484 0.6512202\n                    ACF1\nTraining set -0.02248767\n\n\n\n\nView Code\n#fitting an ARIMA model to the Production variable\nProduction_fit_demand<-auto.arima(demand$Production) \nfProduction_demand<-forecast(Production_fit_demand)\nsummary(Production_fit_demand) \n\n\nSeries: demand$Production \nARIMA(0,1,1)(1,1,1)[12] \n\nCoefficients:\n          ma1     sar1     sma1\n      -0.2318  -0.1148  -0.6329\ns.e.   0.0806   0.0976   0.0785\n\nsigma^2 = 3.033e+09:  log likelihood = -2506.58\nAIC=5021.16   AICc=5021.36   BIC=5034.41\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE      MASE      ACF1\nTraining set 2289.614 52995.22 34932.59 0.07971598 1.319548 0.2680922 0.0231852\n\n\n\n\nView Code\n#fitting an ARIMA model to the storage variable\nstorage_fit_demand<-auto.arima(demand$storage) \nfstorage_demand<-forecast(storage_fit_demand)\nsummary(storage_fit_demand) \n\n\nSeries: demand$storage \nARIMA(2,0,1)(2,1,1)[12] \n\nCoefficients:\n         ar1      ar2      ma1    sar1     sar2     sma1\n      1.6807  -0.7070  -0.6125  0.0847  -0.3563  -0.8757\ns.e.  0.1948   0.1852   0.2243  0.0812   0.0780   0.0837\n\nsigma^2 = 9.631e+09:  log likelihood = -2644.73\nAIC=5303.46   AICc=5304.03   BIC=5326.68\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE    MAPE      MASE        ACF1\nTraining set 12403.42 93960.96 70783.51 0.1578639 1.02821 0.1939838 -0.01086143\n\n\n\n\nView Code\n# Forcast Natural gas demand using feature variables\nfxreg_demand_ts <- cbind(temperature = ftemperature_demand$mean, Production = fProduction_demand$mean,\n              storage = fstorage_demand$mean)\n\nfcast_demand_ts <- forecast(fit_demand_ts, xreg=fxreg_demand_ts) \n\nautoplot(fcast_demand_ts,fcol = '#92c54b') + xlab(\"Time\") + theme_bw()+ \n  ylab(\"Consumption\")\n\n\n\n\n\nFindings: The data shows pronounced seasonal patterns, with sharp peaks and troughs occurring regularly each year. This pattern suggests strong seasonal influences on consumption.The amplitude of the seasonal fluctuations appears to be relatively stable over time, indicating consistent seasonal effects across the years. The forecast continues the pattern of seasonal peaks and troughs observed in the historical data, indicating that the model expects future behavior to be similar to past behavior.\n\n\n\n\nView Code\n#fiting an ARIMA model to the unemp variable\nunemp_fit_eco<-auto.arima(eco$unemp) \nfunemp_eco<-forecast(unemp_fit_eco)\nsummary(unemp_fit_eco) \n\n\nSeries: eco$unemp \nARIMA(0,1,0) \n\nsigma^2 = 1.615:  log likelihood = -117.76\nAIC=237.52   AICc=237.58   BIC=239.78\n\nTraining set error measures:\n                      ME     RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.02395417 1.261955 0.4446569 -1.600193 5.992066 0.3438721\n                   ACF1\nTraining set -0.1753812\n\n\n\n\nView Code\n#fitting an ARIMA model to the GDP variable\ngdp_fit_eco<-auto.arima(eco$gdp) \nfgdp_eco<-forecast(gdp_fit_eco)\nsummary(gdp_fit_eco) \n\n\nSeries: eco$gdp \nARIMA(0,2,1) \n\nCoefficients:\n          ma1\n      -0.9320\ns.e.   0.0502\n\nsigma^2 = 123648:  log likelihood = -510.22\nAIC=1024.44   AICc=1024.61   BIC=1028.93\n\nTraining set error measures:\n                   ME     RMSE     MAE       MPE      MAPE     MASE       ACF1\nTraining set 33.78236 344.2332 154.266 0.1181144 0.8012411 0.185697 -0.1468472\n\n\n\n\nView Code\n# Forcast price using feature variables\nfxreg_eco_ts <- cbind(gdp = fgdp_eco$mean,\n              unemp=funemp_eco$mean)\n\nfcast_eco_ts <- forecast(fit_eco_ts, xreg=fxreg_eco_ts) \n\nautoplot(fcast_eco_ts,fcol = '#92c54b') + xlab(\"Time\")+ theme_bw() +\n  ylab(\"Prices influenced by macroeconomic factors\")\n\n\n\n\n\nFindings: The time series shows significant fluctuations and a couple of sharp peaks, particularly notable around the years 2005, 2010, and 2020. These peaks indicates market disruptions impacting the prices modeled. The general trend between the peaks appears relatively stable or slowly declining, especially from around 2010 to 2015 and stabilizing with slight fluctuations thereafter until 2020. The forecast suggests a potential continuation of the recent trend seen just before 2022, which appears to be upward, suggesting an expectation of rising prices influenced by macroeconomic factors.\n\n\n\n\nView Code\n#fiting an ARIMA model to the consumption variable\nconsumption_fit_export<-auto.arima(export$consumption) \nfconsumption_export<-forecast(consumption_fit_export)\nsummary(consumption_fit_export) \n\n\nSeries: export$consumption \nARIMA(0,0,1)(2,1,1)[4] with drift \n\nCoefficients:\n         ma1    sar1     sar2     sma1       drift\n      0.6381  0.1587  -0.4543  -0.7347  12841.9715\ns.e.  0.1188  0.1279   0.1171   0.1216    886.8046\n\nsigma^2 = 5.066e+09:  log likelihood = -857.01\nAIC=1726.02   AICc=1727.4   BIC=1739.34\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE    MAPE      MASE\nTraining set -4520.991 66581.18 52819.52 -0.3773109 2.46093 0.5640389\n                    ACF1\nTraining set -0.04919122\n\n\n\n\nView Code\n#fitting an ARIMA model to the exchange rate variable\nfx_rate_fit_export<-auto.arima(export$fx_rate) \nffx_rate_export<-forecast(fx_rate_fit_export)\nsummary(fx_rate_fit_export) \n\n\nSeries: export$fx_rate \nARIMA(0,1,0) \n\nsigma^2 = 1.4:  log likelihood = -112.69\nAIC=227.38   AICc=227.44   BIC=229.64\n\nTraining set error measures:\n                      ME     RMSE       MAE         MPE      MAPE      MASE\nTraining set -0.04918699 1.175063 0.9309056 -0.05457483 0.9354786 0.5015178\n                  ACF1\nTraining set 0.1887399\n\n\n\n\nView Code\n#fitting an ARIMA model to the GDP variable\ngdp_nl_fit_export<-auto.arima(export$gdp_nl) \nfgdp_nl_export<-forecast(gdp_nl_fit_export)\nsummary(gdp_nl_fit_export) \n\n\nSeries: export$gdp_nl \nARIMA(0,2,1) \n\nCoefficients:\n          ma1\n      -0.8849\ns.e.   0.0659\n\nsigma^2 = 1.1e+07:  log likelihood = -667.05\nAIC=1338.09   AICc=1338.27   BIC=1342.59\n\nTraining set error measures:\n                   ME   RMSE     MAE        MPE      MAPE      MASE       ACF1\nTraining set 245.7257 3246.4 1565.21 0.08137514 0.8394132 0.2156717 -0.1191757\n\n\n\n\nView Code\n# Forcast export using feature variables\nfxreg_export_ts <- cbind(consumption = fconsumption_export$mean,\n              fx_rate=ffx_rate_export$mean,\n             gdp_nl=fgdp_nl_export$mean )\n\nfcast_export_ts <- forecast(fit_export_ts, xreg=fxreg_export_ts) \n\nautoplot(fcast_export_ts,fcol = '#92c54b') + xlab(\"Time\")+ theme_bw() +\n  ylab(\"Exports\")\n\n\n\n\n\nFindings: There’s a clear upward trend in the exports from around 2005 to approximately 2022. This suggests growth over these years.There is a particularly sharp increase in exports around the years 2015 through 2022, indicating a period of significant growth. Overall, the forecast suggests optimism about the future direction of exports, albeit with growing uncertainty as predictions extend further into the future.\n\n\n\n\nView Code\n#fiting an ARIMA model to the Consumption variable\nConsumption_fit_env_impact<-auto.arima(env_impact$Consumption) \nfConsumption_env_impact<-forecast(Consumption_fit_env_impact)\nsummary(Consumption_fit_env_impact) \n\n\nSeries: env_impact$Consumption \nARIMA(1,1,1)(1,0,2)[12] \n\nCoefficients:\n         ar1      ma1    sar1    sma1    sma2\n      0.6145  -0.9779  0.1611  0.6208  0.4307\ns.e.  0.0639   0.0110  0.1453  0.1250  0.0806\n\nsigma^2 = 4.605e+10:  log likelihood = -3193.85\nAIC=6399.71   AICc=6400.08   BIC=6420.41\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 14612.77 211822.9 174026.2 -0.5983582 7.724336 0.7300607\n                   ACF1\nTraining set 0.07045743\n\n\n\n\nView Code\n#fitting an ARIMA model to the Production variable\nProduction_fit_env_impact<-auto.arima(env_impact$Production) \nfProduction_env_impact<-forecast(Production_fit_env_impact)\nsummary(Production_fit_env_impact) \n\n\nSeries: env_impact$Production \nARIMA(0,1,1)(0,0,1)[12] with drift \n\nCoefficients:\n          ma1     sma1     drift\n      -0.5927  -0.3839  7708.609\ns.e.   0.0704   0.0857  1646.643\n\nsigma^2 = 9.379e+09:  log likelihood = -3005.32\nAIC=6018.64   AICc=6018.81   BIC=6032.44\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -1087.363 96013.37 70250.08 -0.2407207 2.667694 0.4658773\n                   ACF1\nTraining set 0.01137183\n\n\n\n\nView Code\n# Forcast co2 emmisions using feature variables\nfxreg_env_impact_ts <- cbind(Consumption = fConsumption_env_impact$mean,\n              Production = fProduction_env_impact$mean)\n\nfcast_env_impact_ts <- forecast(fit_env_impact_ts, xreg=fxreg_env_impact_ts) \n\nautoplot(fcast_env_impact_ts,fcol = '#92c54b') + xlab(\"Time\")+ theme_bw() +\n  ylab(\"CO2 Emissions\")\n\n\n\n\n\nFindings: The best model forecast maintains the seasonal pattern, but with a diminishing intensity compared to the observed trend. However, the observed trend line depicts a growing intensity, suggesting that this model may not fully capture the trend. This discrepancy indicates that the chosen exogenous variables alone might not exert a significant influence on CO2 emissions, suggesting additional regressors for a more comprehensive analysis"
  },
  {
    "objectID": "multivariate.html#var-modeling",
    "href": "multivariate.html#var-modeling",
    "title": "Multivariate TS Models (ARIMAX/SARIMAX/VAR)",
    "section": "VAR Modeling",
    "text": "VAR Modeling\n\n\nView Code\npairs(cbind(price=price[, -1] , production=production[, -1] , imports=imports[, -1] ))\n\n\n\n\n\nView Code\ndata_no_date <- supply_dynamics[, -1] \ncorrplot(cor(data_no_date), method='color')\n\n\n\n\n\nThe plots above display the relationships between price, production, and imports using scatter plots and correlation matrices. The scatter plots show potential nonlinear relationships among the variables, particularly between imports and the other two variables, while the correlation matrix reveals strong negative correlations between production and imports, and moderate correlations between prices and the other variables.\n\nTime series plot\n\n\nView Code\nprice_ts = ts(price$price, start = 2005,end = c(2022,12),frequency = 12)\n# # storage_ts = ts(storage$storage, start = 2005,end = c(2022,12),frequency = 12)\n# # exports_ts = ts(exports$exports, start = 2005,end = c(2022,12),frequency = 12)\nimports_ts = ts(imports$imports, start = 2005,end = c(2022,12),frequency = 12)\n# # temp_ts = ts(temperature$temperature, start = 2005,end = c(2022,12),frequency = 12)\nproduction_ts = ts(production$Production, start = 2005,end = c(2022,12),frequency = 12)\n# \n# Combine precipitation and temp\nsupply = cbind(price_ts,imports_ts,production_ts)\ncolnames(supply) <- c(\"price\", \"imports\",\"production\")\n\n# supply.ts<-ts(supply_dynamics,star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\nautoplot(supply, facets=TRUE, color=\"#92c54b\") + theme_bw()+\n     xlab(\"Time\") + ylab(\"\") + \n  ggtitle(\"Supply Dynamics in the US\")\n\n\n\n\n\nThe plot depicts the dynamics of U.S. natural gas prices, imports, and production over time. Prices experienced significant fluctuations, particularly with notable peaks around 2005 and 2008, stabilizing and then increasing gradually post-2015, reflecting market conditions and economic factors. Imports show a general decline from 2005 until 2012, stabilizing afterward, suggesting increased self-sufficiency in domestic production or shifts in energy policy. Production consistently rose from 2005, indicating advances in extraction technologies like hydraulic fracturing.\n\n\nFinding Best p\n\n\nView Code\nVARselect(supply, lag.max=10, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     6      3      2      6 \n\n$criteria\n                  1            2            3            4            5\nAIC(n) 4.280098e+01 4.249361e+01 4.238072e+01 4.238209e+01 4.229855e+01\nHQ(n)  4.289898e+01 4.265041e+01 4.259632e+01 4.265650e+01 4.263176e+01\nSC(n)  4.304330e+01 4.288132e+01 4.291382e+01 4.306059e+01 4.312244e+01\nFPE(n) 3.874727e+18 2.849641e+18 2.545913e+18 2.550249e+18 2.347020e+18\n                  6            7            8            9           10\nAIC(n) 4.223640e+01 4.229797e+01 4.233180e+01 4.231287e+01 4.228915e+01\nHQ(n)  4.262841e+01 4.274878e+01 4.284141e+01 4.288129e+01 4.291637e+01\nSC(n)  4.320569e+01 4.341265e+01 4.359187e+01 4.371834e+01 4.384001e+01\nFPE(n) 2.207172e+18 2.349597e+18 2.433487e+18 2.391663e+18 2.340185e+18\n\n\nAccording to the selection criterion, p = 2, 3 and 6.Therefore, I’m fitting several models with p=1(for simplicity), 2, 3 and 6.=> VAR(1), VAR(2), VAR(3), VAR(6)\n\n\nVAR model fitting\n\nVAR(1)VAR(2)VAR(3)VAR(6)\n\n\n\n\nView Code\nsummary(vars::VAR(supply, p=1, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: price, imports, production \nDeterministic variables: both \nSample size: 215 \nLog Likelihood: -5516.817 \nRoots of the characteristic polynomial:\n0.9266 0.725 0.4645\nCall:\nvars::VAR(y = supply, p = 1, type = \"both\")\n\n\nEstimation results for equation price: \n====================================== \nprice = price.l1 + imports.l1 + production.l1 + const + trend \n\n                Estimate Std. Error t value Pr(>|t|)    \nprice.l1       8.913e-01  3.164e-02  28.175   <2e-16 ***\nimports.l1     7.834e-07  1.761e-06   0.445   0.6568    \nproduction.l1  7.985e-07  3.974e-07   2.009   0.0458 *  \nconst         -9.641e-01  7.503e-01  -1.285   0.2002    \ntrend         -8.060e-03  4.058e-03  -1.986   0.0483 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.7977 on 210 degrees of freedom\nMultiple R-Squared: 0.884,  Adjusted R-squared: 0.8818 \nF-statistic: 400.2 on 4 and 210 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation imports: \n======================================== \nimports = price.l1 + imports.l1 + production.l1 + const + trend \n\n                Estimate Std. Error t value Pr(>|t|)    \nprice.l1       2.603e+03  9.709e+02   2.681  0.00792 ** \nimports.l1     6.790e-01  5.404e-02  12.565  < 2e-16 ***\nproduction.l1 -9.263e-03  1.220e-02  -0.759  0.44843    \nconst          1.099e+05  2.303e+04   4.773  3.4e-06 ***\ntrend         -8.361e+01  1.246e+02  -0.671  0.50279    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 24480 on 210 degrees of freedom\nMultiple R-Squared: 0.8168, Adjusted R-squared: 0.8133 \nF-statistic:   234 on 4 and 210 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation production: \n=========================================== \nproduction = price.l1 + imports.l1 + production.l1 + const + trend \n\n                Estimate Std. Error t value Pr(>|t|)    \nprice.l1       1.924e+04  4.654e+03   4.135 5.13e-05 ***\nimports.l1    -6.366e-01  2.590e-01  -2.458   0.0148 *  \nproduction.l1  5.458e-01  5.846e-02   9.337  < 2e-16 ***\nconst          8.823e+05  1.104e+05   7.994 8.58e-14 ***\ntrend          3.814e+03  5.970e+02   6.388 1.06e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 117300 on 210 degrees of freedom\nMultiple R-Squared: 0.9548, Adjusted R-squared: 0.9539 \nF-statistic:  1108 on 4 and 210 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n                price   imports production\nprice          0.6363 2.630e+03 -9.141e+03\nimports     2630.1653 5.994e+08  1.170e+09\nproduction -9141.4090 1.170e+09  1.377e+10\n\nCorrelation matrix of residuals:\n              price imports production\nprice       1.00000  0.1347   -0.09767\nimports     0.13469  1.0000    0.40742\nproduction -0.09767  0.4074    1.00000\n\n\n\n\n\n\nView Code\nsummary(vars::VAR(supply, p=2, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: price, imports, production \nDeterministic variables: both \nSample size: 214 \nLog Likelihood: -5453.452 \nRoots of the characteristic polynomial:\n0.9452 0.7807 0.5786 0.5043 0.5043 0.035\nCall:\nvars::VAR(y = supply, p = 2, type = \"both\")\n\n\nEstimation results for equation price: \n====================================== \nprice = price.l1 + imports.l1 + production.l1 + price.l2 + imports.l2 + production.l2 + const + trend \n\n                Estimate Std. Error t value Pr(>|t|)    \nprice.l1       9.038e-01  7.077e-02  12.771   <2e-16 ***\nimports.l1    -2.306e-06  2.541e-06  -0.908   0.3652    \nproduction.l1  1.288e-06  5.221e-07   2.467   0.0144 *  \nprice.l2      -1.973e-02  7.162e-02  -0.275   0.7832    \nimports.l2     4.412e-06  2.571e-06   1.716   0.0876 .  \nproduction.l2 -5.638e-07  5.054e-07  -1.115   0.2659    \nconst         -1.260e+00  8.685e-01  -1.451   0.1484    \ntrend         -6.651e-03  4.562e-03  -1.458   0.1464    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.799 on 206 degrees of freedom\nMultiple R-Squared: 0.8856, Adjusted R-squared: 0.8817 \nF-statistic: 227.8 on 7 and 206 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation imports: \n======================================== \nimports = price.l1 + imports.l1 + production.l1 + price.l2 + imports.l2 + production.l2 + const + trend \n\n                Estimate Std. Error t value Pr(>|t|)    \nprice.l1      -4.062e+02  2.101e+03  -0.193  0.84689    \nimports.l1     7.862e-01  7.543e-02  10.422  < 2e-16 ***\nproduction.l1 -4.679e-02  1.550e-02  -3.019  0.00286 ** \nprice.l2       2.775e+03  2.126e+03   1.305  0.19333    \nimports.l2    -1.385e-01  7.631e-02  -1.814  0.07106 .  \nproduction.l2  6.029e-02  1.500e-02   4.018 8.23e-05 ***\nconst          8.410e+04  2.578e+04   3.262  0.00129 ** \ntrend         -3.037e+02  1.354e+02  -2.242  0.02602 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 23720 on 206 degrees of freedom\nMultiple R-Squared: 0.8296, Adjusted R-squared: 0.8238 \nF-statistic: 143.3 on 7 and 206 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation production: \n=========================================== \nproduction = price.l1 + imports.l1 + production.l1 + price.l2 + imports.l2 + production.l2 + const + trend \n\n                Estimate Std. Error t value Pr(>|t|)    \nprice.l1       9.868e+03  9.102e+03   1.084 0.279554    \nimports.l1    -1.109e+00  3.268e-01  -3.393 0.000828 ***\nproduction.l1  3.528e-01  6.715e-02   5.254 3.70e-07 ***\nprice.l2       2.021e+03  9.211e+03   0.219 0.826584    \nimports.l2     7.242e-01  3.306e-01   2.191 0.029598 *  \nproduction.l2  4.110e-01  6.501e-02   6.323 1.56e-09 ***\nconst          4.658e+05  1.117e+05   4.170 4.49e-05 ***\ntrend          2.040e+03  5.868e+02   3.476 0.000620 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 102800 on 206 degrees of freedom\nMultiple R-Squared: 0.9656, Adjusted R-squared: 0.9645 \nF-statistic: 827.3 on 7 and 206 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n                price   imports production\nprice          0.6384      3069 -9.385e+03\nimports     3069.0047 562578817  9.051e+08\nproduction -9384.9999 905147602  1.056e+10\n\nCorrelation matrix of residuals:\n             price imports production\nprice       1.0000  0.1619    -0.1143\nimports     0.1619  1.0000     0.3714\nproduction -0.1143  0.3714     1.0000\n\n\n\n\n\n\nView Code\nsummary(vars::VAR(supply, p=3, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: price, imports, production \nDeterministic variables: both \nSample size: 213 \nLog Likelihood: -5406.267 \nRoots of the characteristic polynomial:\n0.9536 0.8361 0.6128 0.6128 0.5083 0.5083 0.3599 0.3599 0.3447\nCall:\nvars::VAR(y = supply, p = 3, type = \"both\")\n\n\nEstimation results for equation price: \n====================================== \nprice = price.l1 + imports.l1 + production.l1 + price.l2 + imports.l2 + production.l2 + price.l3 + imports.l3 + production.l3 + const + trend \n\n                Estimate Std. Error t value Pr(>|t|)    \nprice.l1       8.917e-01  7.168e-02  12.440   <2e-16 ***\nimports.l1    -1.274e-06  2.609e-06  -0.488   0.6258    \nproduction.l1  1.388e-06  5.930e-07   2.340   0.0202 *  \nprice.l2       1.107e-01  9.688e-02   1.143   0.2544    \nimports.l2     1.664e-06  3.504e-06   0.475   0.6353    \nproduction.l2 -4.726e-09  6.083e-07  -0.008   0.9938    \nprice.l3      -1.352e-01  7.181e-02  -1.882   0.0612 .  \nimports.l3     2.785e-06  2.683e-06   1.038   0.3006    \nproduction.l3 -7.842e-07  5.563e-07  -1.410   0.1601    \nconst         -1.321e+00  9.176e-01  -1.439   0.1516    \ntrend         -5.180e-03  4.833e-03  -1.072   0.2851    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.7963 on 202 degrees of freedom\nMultiple R-Squared: 0.888,  Adjusted R-squared: 0.8824 \nF-statistic: 160.1 on 10 and 202 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation imports: \n======================================== \nimports = price.l1 + imports.l1 + production.l1 + price.l2 + imports.l2 + production.l2 + price.l3 + imports.l3 + production.l3 + const + trend \n\n                Estimate Std. Error t value Pr(>|t|)    \nprice.l1      -1.095e+03  2.132e+03  -0.514  0.60815    \nimports.l1     8.251e-01  7.760e-02  10.632  < 2e-16 ***\nproduction.l1 -5.559e-02  1.764e-02  -3.151  0.00187 ** \nprice.l2       4.689e+03  2.882e+03   1.627  0.10526    \nimports.l2    -2.799e-01  1.042e-01  -2.685  0.00785 ** \nproduction.l2  7.624e-02  1.809e-02   4.214 3.78e-05 ***\nprice.l3      -1.600e+03  2.136e+03  -0.749  0.45477    \nimports.l3     1.493e-01  7.981e-02   1.871  0.06282 .  \nproduction.l3 -9.481e-03  1.655e-02  -0.573  0.56728    \nconst          7.338e+04  2.729e+04   2.688  0.00778 ** \ntrend         -2.541e+02  1.438e+02  -1.768  0.07862 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 23680 on 202 degrees of freedom\nMultiple R-Squared: 0.8307, Adjusted R-squared: 0.8223 \nF-statistic: 99.09 on 10 and 202 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation production: \n=========================================== \nproduction = price.l1 + imports.l1 + production.l1 + price.l2 + imports.l2 + production.l2 + price.l3 + imports.l3 + production.l3 + const + trend \n\n                Estimate Std. Error t value Pr(>|t|)    \nprice.l1       3.602e+03  8.557e+03   0.421  0.67420    \nimports.l1    -8.520e-01  3.115e-01  -2.735  0.00678 ** \nproduction.l1  1.495e-01  7.080e-02   2.112  0.03589 *  \nprice.l2       8.793e+03  1.157e+04   0.760  0.44799    \nimports.l2    -4.815e-01  4.183e-01  -1.151  0.25111    \nproduction.l2  4.667e-01  7.262e-02   6.427 9.19e-10 ***\nprice.l3      -3.848e+03  8.572e+03  -0.449  0.65399    \nimports.l3     1.282e+00  3.203e-01   4.002 8.81e-05 ***\nproduction.l3  1.953e-01  6.641e-02   2.940  0.00366 ** \nconst          2.840e+05  1.095e+05   2.592  0.01023 *  \ntrend          1.867e+03  5.770e+02   3.236  0.00142 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 95060 on 202 degrees of freedom\nMultiple R-Squared: 0.971,  Adjusted R-squared: 0.9696 \nF-statistic: 676.9 on 10 and 202 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n               price   imports production\nprice          0.634      2831      -9707\nimports     2831.428 560974632  820222971\nproduction -9706.961 820222971 9035981970\n\nCorrelation matrix of residuals:\n             price imports production\nprice       1.0000  0.1501    -0.1282\nimports     0.1501  1.0000     0.3643\nproduction -0.1282  0.3643     1.0000\n\n\n\n\n\n\nView Code\nsummary(vars::VAR(supply, p=6, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: price, imports, production \nDeterministic variables: both \nSample size: 210 \nLog Likelihood: -5288.235 \nRoots of the characteristic polynomial:\n0.9711 0.8957 0.8762 0.8762 0.7918 0.7918 0.7909 0.7909 0.782 0.782 0.7731 0.6454 0.6454 0.5766 0.5766 0.5092 0.5092 0.2999\nCall:\nvars::VAR(y = supply, p = 6, type = \"both\")\n\n\nEstimation results for equation price: \n====================================== \nprice = price.l1 + imports.l1 + production.l1 + price.l2 + imports.l2 + production.l2 + price.l3 + imports.l3 + production.l3 + price.l4 + imports.l4 + production.l4 + price.l5 + imports.l5 + production.l5 + price.l6 + imports.l6 + production.l6 + const + trend \n\n                Estimate Std. Error t value Pr(>|t|)    \nprice.l1       9.189e-01  7.420e-02  12.385  < 2e-16 ***\nimports.l1    -3.981e-06  2.711e-06  -1.468  0.14366    \nproduction.l1  1.376e-06  6.700e-07   2.054  0.04138 *  \nprice.l2       6.564e-02  9.728e-02   0.675  0.50061    \nimports.l2     4.750e-06  3.598e-06   1.320  0.18840    \nproduction.l2 -9.910e-07  6.806e-07  -1.456  0.14705    \nprice.l3      -5.121e-02  9.721e-02  -0.527  0.59894    \nimports.l3     2.527e-06  3.581e-06   0.706  0.48127    \nproduction.l3 -8.821e-07  7.096e-07  -1.243  0.21536    \nprice.l4       6.058e-02  9.655e-02   0.627  0.53114    \nimports.l4    -7.472e-06  3.626e-06  -2.060  0.04073 *  \nproduction.l4  1.756e-06  6.839e-07   2.568  0.01101 *  \nprice.l5      -3.257e-01  9.891e-02  -3.293  0.00118 ** \nimports.l5     1.155e-05  3.654e-06   3.160  0.00184 ** \nproduction.l5 -7.328e-07  6.880e-07  -1.065  0.28819    \nprice.l6       1.865e-01  7.554e-02   2.468  0.01445 *  \nimports.l6    -2.989e-06  2.869e-06  -1.042  0.29870    \nproduction.l6  1.917e-07  5.950e-07   0.322  0.74767    \nconst         -1.858e+00  9.771e-01  -1.902  0.05875 .  \ntrend         -5.544e-03  5.293e-03  -1.048  0.29618    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.7734 on 190 degrees of freedom\nMultiple R-Squared: 0.8989, Adjusted R-squared: 0.8888 \nF-statistic: 88.92 on 19 and 190 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation imports: \n======================================== \nimports = price.l1 + imports.l1 + production.l1 + price.l2 + imports.l2 + production.l2 + price.l3 + imports.l3 + production.l3 + price.l4 + imports.l4 + production.l4 + price.l5 + imports.l5 + production.l5 + price.l6 + imports.l6 + production.l6 + const + trend \n\n                Estimate Std. Error t value Pr(>|t|)    \nprice.l1      -6.855e+01  2.213e+03  -0.031   0.9753    \nimports.l1     7.673e-01  8.085e-02   9.490   <2e-16 ***\nproduction.l1 -4.888e-02  1.998e-02  -2.447   0.0153 *  \nprice.l2       3.451e+03  2.901e+03   1.190   0.2357    \nimports.l2    -1.985e-01  1.073e-01  -1.850   0.0659 .  \nproduction.l2  6.676e-02  2.030e-02   3.289   0.0012 ** \nprice.l3       6.500e+02  2.899e+03   0.224   0.8228    \nimports.l3     2.058e-03  1.068e-01   0.019   0.9846    \nproduction.l3  2.975e-02  2.116e-02   1.406   0.1614    \nprice.l4      -4.785e+03  2.879e+03  -1.662   0.0982 .  \nimports.l4     8.236e-02  1.081e-01   0.762   0.4473    \nproduction.l4  1.877e-03  2.040e-02   0.092   0.9268    \nprice.l5      -6.749e+02  2.950e+03  -0.229   0.8193    \nimports.l5     2.113e-01  1.090e-01   1.939   0.0540 .  \nproduction.l5 -2.352e-02  2.052e-02  -1.146   0.2531    \nprice.l6       3.615e+03  2.253e+03   1.605   0.1102    \nimports.l6    -1.043e-01  8.555e-02  -1.219   0.2243    \nproduction.l6 -2.000e-02  1.774e-02  -1.127   0.2611    \nconst          5.807e+04  2.914e+04   1.993   0.0477 *  \ntrend         -1.694e+02  1.578e+02  -1.073   0.2845    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 23060 on 190 degrees of freedom\nMultiple R-Squared: 0.847,  Adjusted R-squared: 0.8317 \nF-statistic: 55.35 on 19 and 190 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation production: \n=========================================== \nproduction = price.l1 + imports.l1 + production.l1 + price.l2 + imports.l2 + production.l2 + price.l3 + imports.l3 + production.l3 + price.l4 + imports.l4 + production.l4 + price.l5 + imports.l5 + production.l5 + price.l6 + imports.l6 + production.l6 + const + trend \n\n                Estimate Std. Error t value Pr(>|t|)    \nprice.l1       6.644e+03  8.586e+03   0.774 0.440056    \nimports.l1    -8.224e-01  3.138e-01  -2.621 0.009477 ** \nproduction.l1  1.385e-01  7.753e-02   1.787 0.075568 .  \nprice.l2       1.166e+04  1.126e+04   1.036 0.301675    \nimports.l2    -5.676e-01  4.164e-01  -1.363 0.174424    \nproduction.l2  4.036e-01  7.877e-02   5.123 7.34e-07 ***\nprice.l3      -7.681e+03  1.125e+04  -0.683 0.495558    \nimports.l3     1.134e+00  4.145e-01   2.735 0.006829 ** \nproduction.l3  1.900e-01  8.212e-02   2.314 0.021761 *  \nprice.l4      -1.017e+04  1.117e+04  -0.910 0.363873    \nimports.l4     2.008e-01  4.197e-01   0.478 0.632909    \nproduction.l4  1.789e-01  7.915e-02   2.260 0.024938 *  \nprice.l5       6.868e+03  1.145e+04   0.600 0.549245    \nimports.l5     1.246e-01  4.229e-01   0.295 0.768644    \nproduction.l5  1.944e-01  7.962e-02   2.442 0.015529 *  \nprice.l6       3.486e+03  8.742e+03   0.399 0.690524    \nimports.l6    -2.884e-01  3.320e-01  -0.869 0.386099    \nproduction.l6 -2.698e-01  6.885e-02  -3.918 0.000124 ***\nconst          2.874e+05  1.131e+05   2.541 0.011840 *  \ntrend          1.581e+03  6.125e+02   2.581 0.010610 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 89510 on 190 degrees of freedom\nMultiple R-Squared: 0.9753, Adjusted R-squared: 0.9728 \nF-statistic: 394.8 on 19 and 190 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n                price   imports production\nprice       5.982e-01      2332     -12088\nimports     2.332e+03 531981382  777816406\nproduction -1.209e+04 777816406 8011276298\n\nCorrelation matrix of residuals:\n             price imports production\nprice       1.0000  0.1307    -0.1746\nimports     0.1307  1.0000     0.3768\nproduction -0.1746  0.3768     1.0000\n\n\n\n\n\nWe can see that some models have some variables significant but not all. Let’s proceed with cross validation.\n\n\nCross validation\nI’m using Cross validation to find the best model.\n\n\nView Code\nn=length(price_ts)\nk=15\n\nrmse1 <- matrix(NA, n-k,12)\nrmse2 <- matrix(NA, n-k,12)\nrmse3 <- matrix(NA, n-k,12)\nrmse4 <- matrix(NA, n-k,12)\nyear<-c()\n\n# Convert data frame to time series object\nts_obj <- ts(supply, star=decimal_date(as.Date(\"2005-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\nst <- tsp(ts_obj )[1]+(k-1)/12\n\nfor(i in 1:4)\n{\n  \n  xtrain <- window(ts_obj, end=st + i-1)\n  xtest <- window(ts_obj, start=st + (i-1) + 1/12, end=st + i)\n  \n  ######## first Model ############\n  fit <- VAR(ts_obj, p=1, type='both')\n  fcast <- predict(fit, n.ahead = 3)\n  \n  fprice<-fcast$fcst$price\n  fimports<-fcast$fcst$imports\n  fprod<-fcast$fcst$production\n \n  ff<-data.frame(fprice[,1],fimports[,1],fprod[,1]) #collecting the forecasts for 3 variables\n\n  year<-st + (i-1) + 1/12 #starting year\n  \n  ff<-ts(ff,start=c(year,1),frequency = 12)\n  \n  a = 12*i-11 #going from 1,13,25,37\n  b= 12*i #12, 24, 36\n  \n  ##### collecting errors ######\n  rmse1[c(a:b),]  <-sqrt((ff-xtest)^2)\n  \n  ######## Second Model ############\n  fit2 <- vars::VAR(ts_obj, p=2, type='both')\n  fcast2 <- predict(fit2, n.ahead = 3)\n  \n  fprice<-fcast2$fcst$price\n  fimports<-fcast2$fcst$imports\n  fprod<-fcast2$fcst$production\n\n  ff2<-data.frame(fprice[,1],fimports[,1],fprod[,1])\n  \n  year<-st + (i-1) + 1/12\n  \n  ff2<-ts(ff2,start=c(year,1),frequency = 12)\n  \n  a = 12*i-11 #going from 1,13,25,37\n  b= 12*i #12, 24, 36\n  rmse2[c(a:b),]  <-sqrt((ff2-xtest)^2)\n  \n    ######## Third Model ############\n  fit3 <- vars::VAR(ts_obj, p=3, type='both')\n  fcast3 <- predict(fit3, n.ahead = 3)\n  \n  fprice<-fcast3$fcst$price\n  fimports<-fcast3$fcst$imports\n  fprod<-fcast3$fcst$production\n\n  ff3<-data.frame(fprice[,1],fimports[,1],fprod[,1])\n  \n  year<-st + (i-1) + 1/12\n  \n  ff3<-ts(ff3,start=c(year,1),frequency = 12)\n  \n  a = 12*i-11 #going from 1,13,25,37\n  b= 12*i #12, 24, 36\n  rmse3[c(a:b),]  <-sqrt((ff3-xtest)^2)\n  \n      ######## Fourth Model ############\n  fit4 <- vars::VAR(ts_obj, p=6, type='both')\n  fcast4 <- predict(fit4, n.ahead = 3)\n  \n  fprice<-fcast4$fcst$price\n  fimports<-fcast4$fcst$imports\n  fprod<-fcast4$fcst$production\n\n  ff4<-data.frame(fprice[,1],fimports[,1],fprod[,1])\n  \n  year<-st + (i-1) + 1/12\n  \n  ff4<-ts(ff4,start=c(year,1),frequency = 12)\n  \n  a = 12*i-11 #going from 1,13,25,37\n  b= 12*i #12, 24, 36\n  rmse4[c(a:b),]  <-sqrt((ff4-xtest)^2)\n  \n}\n\nplot(1:12, colMeans(rmse1,na.rm=TRUE), type=\"l\", col=2, xlab=\"Year\", ylab=\"RMSE\", main=\"RMSE plot for supply\")\nlines(1:12, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlines(1:12, colMeans(rmse3,na.rm=TRUE), type=\"l\",col=4)\nlines(1:12, colMeans(rmse4,na.rm=TRUE), type=\"l\",col=5)\nlegend(\"topleft\",legend=c(\"VAR 1\",\"VAR 2\",\"VAR 3\",\"VAR 6\"),col=2:6,lty=1)\n\n\n\n\n\nVAR(1) has lower RMSE than other VAR models , therefore VAR(1) is a better fit.\n\n\nModel Equation\nModel Equation for best model VAR(1)\nUsing notation I= imports, t= time, Pr= prices, and P= production\n\\(\\hat{Pr}=-(9.641e-01) -(8.060e-03)t+(8.913e-01)Pr_{t-1}+(7.834e-07)I_{t-1}+(7.985e-07)P_{t-1}\\)\n\\(\\hat{I}=(1.099e+05) -(8.361e+01)t+(2.603e+03)Pr_{t-1}+(6.790e-01)I_{t-1}-(9.263e-03)P_{t-1}\\)\n\\(\\hat{P}=( 8.823e+05) +( 3.814e+03 )t+(1.924e+04 )Pr_{t-1}-(6.366e-01)I_{t-1}-(5.458e-01)P_{t-1}\\)\n\n\nForecasting\n\n\nView Code\nvar1 <- VAR(supply, p=1, type=\"const\")\nforecast(var1) %>%\n  autoplot(color = '#68893c') + xlab(\"Year\")+ theme_bw()\n\n\n\n\n\nFindings: Imports shows a generally stable trend in imports, with minor fluctuations but no significant changes indicated towards the future. The price forecast indicates a slight uptick following a period of stability, suggesting a possible increase in market prices in the near future. This could be due to external market pressures. The production forecast shows a slight decline. This suggests potential challenges or constraints that might affect production capabilities in the future. These observations suggest that while imports remain stable and prices might see a small decrease, production could face some downward pressure moving forward."
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "This section delves into the data retrieved from the specified data sources tab. To gain a thorough understanding of the dataset, various visualization techniques will be employed, providing diverse perspectives and insights into the underlying patterns and relationships within the data."
  },
  {
    "objectID": "dataviz.html#natural-gas-company-stocks",
    "href": "dataviz.html#natural-gas-company-stocks",
    "title": "Data Visualization",
    "section": "Natural Gas Company Stocks",
    "text": "Natural Gas Company Stocks\n\n\nView Code\nlibrary(quantmod)\nlibrary(ggplot2)\nlibrary(plotly)\n\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"CVX\",\"BP\",\"SHEL\",\"XOM\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2000-01-02\",\n             to = \"2022-12-29\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\nstock <- data.frame(CVX$CVX.Adjusted,\n                    BP$BP.Adjusted,\n                    SHEL$SHEL.Adjusted,\n                    XOM$XOM.Adjusted)\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\n\n################################################\n\nggplot(stock, aes(x=date)) +\n  geom_line(aes(y=CVX, colour=\"CVX\"))+\n  geom_line(aes(y=BP, colour=\"BP\"))+\n  geom_line(aes(y=SHEL, colour=\"SHEL\"))+\n  geom_line(aes(y=XOM, colour=\"XOM\"))+\n   labs(\n    title = \"Stock Prices for the Natural Gas Companies\",\n    subtitle = \"From 2000-2022\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    theme(panel.background = element_rect(fill = \"white\", colour = \"grey50\"))+\n    guides(colour=guide_legend(title=\"Company Stocks\")) +\n    scale_color_manual(values = c(\"CVX\" = \"#db0000\", \"BP\" = \"pink\",\"SHEL\" = \"#7eb900\", \"XOM\" = \"#0068b5\"),name = \"Natural Gas Companies\")\n\n\n\n\n\nThe graph shows the stock prices of various natural gas companies over a period from 2000 to 2022 indicating the adjusted closing prices of the company stocks. Overall stocks seem to follow a somewhat correlated pattern, indicating possible market or sector-wide influences. There’s a noticeable peak and sharp decline around the year 2020 for one stock in particular, which could correspond with market volatility during the COVID-19 pandemic. Some stocks display more volatility than others, with more frequent and pronounced peaks and troughs."
  },
  {
    "objectID": "dataviz.html#trade-dynamics",
    "href": "dataviz.html#trade-dynamics",
    "title": "Data Visualization",
    "section": "Trade Dynamics",
    "text": "Trade Dynamics\n\nExports\n\n                   \nThis visualization presents a dual-axis chart tracking U.S. natural gas exports alongside the export prices from 1997 to 2022. One axis represents the volume of exports, while the second axis displays the price. The chart illustrate the relationship between the quantity of exports and their market prices over time. The visualization shows an upward trend in the price of natural gas, while the export volumes fluctuate over the same period. This could suggest that even though the quantity of exports is not steadily increasing, the demand or other market factors are driving prices higher.\n\n\nImports\n\n                   \nThis visualization presents a dual-axis chart tracking U.S. natural gas imports alongside the import prices from 1997 to 2022. One axis represents the volume of imports, while the second axis displays the price. The chart illustrate the relationship between the quantity of imports and their market prices over time. The visualization shows fluctuations in the price of natural gas till 2020 where it dipped to minimum due to COVID-19 pandamic. The import volumes shows downward trend. This could suggest that even though the quantity of imports is not steadily increasing, the demand or other market factors are driving prices higher.\n\n\nExports vs Imports\n\n                   \nNatural gas exports were less than imports until 2017, and then exports exceeded imports, it likely reflects a significant shift in the U.S. energy market. Around that time, the U.S. natural gas industry experienced a boom due to advances in extraction technology like fracking, which led to increased production and enabled the U.S. to become a net exporter of natural gas. This visualization illustrates the transition of the U.S. from a net importer to a net exporter of natural gas."
  },
  {
    "objectID": "dataviz.html#enironmental-impact",
    "href": "dataviz.html#enironmental-impact",
    "title": "Data Visualization",
    "section": "Enironmental Impact",
    "text": "Enironmental Impact\n\nGHG Emissions\n\n\n\n\nView Code\nghg_df<-read.csv(\"datasets/Book3.csv\")\n\nq<-ggplot(ghg_df) + \n  geom_area(aes(x = Year, y = natural.gas),\n            fill = 1, alpha = 0.85) +\n  geom_area(aes(x = Year, y = Total.GHG..emissions.from.fuel.combustion),\n            fill = 1, alpha = 0.1)+\nannotate(\"text\", x = 2000, y = 800, label = \"GHG Emissions from Natural Gas\", color=\"white\")  + \n  annotate(\"text\", x = 2000, y = 3000, label = \"Total GHG Emissions from fuel combustion\", color=\"black\")  + \n   labs(\n    title = \"GHG Emissions of Natural Gas in the US \",\n    subtitle = \"from 1971-2022\",\n    x = \"Year\",\n    y = \"GHG Emissions (million tonnes of CO2 eq)\")+\n   theme(panel.background = element_rect(fill = \"white\", colour = \"grey50\"),panel.border = element_rect(colour = \"grey50\", fill=NA, size=0.5))\n\nq\n\n\n\n\n\nThe graph illustrates greenhouse gas (GHG) emissions related to natural gas in the U.S. from 1971 to 2022. The chart shows two layers, with one representing the total GHG emissions from fuel combustion and the other specifically for natural gas, we can observe the significant natural gas is on the overall emissions. Although the trends is constant over time, the emissions from natural gas have been slightly increasing since 2017 which makes sense as the production of natural gas in the U.S. is increasing since 2017 , and moreover the proportion of natural gas-related emissions remained constant over the years even when the total from all fuel combustion was increasing around 2000s."
  },
  {
    "objectID": "dataviz.html#natural-gas-prices",
    "href": "dataviz.html#natural-gas-prices",
    "title": "Data Visualization",
    "section": "Natural Gas Prices",
    "text": "Natural Gas Prices\n\n\nView Code\nprices_df<-read.csv(\"datasets/Book4.csv\")\n\ns<-ggplot(prices_df, aes(x = Year)) + \n  geom_line(aes(y = Residential, color = \"Residential\"), size = 0.5) + \n  geom_line(aes(y = Industrial, color = \"Industrial\"), size = 0.5) +\n  geom_line(aes(y = Commercial, color = \"Commercial\"), size = 0.5) +\n  geom_line(aes(y = Electric.Power.Price, color = \"Electric Power\"), size = 0.5) +\n  labs(\n    title = paste(\"U.S. Natural Gas Prices by Sector from 1997 to 2023\"),\n    x = \"Year\",\n    y = \"Price (Dollars per Thousand Cubic Feet)\",\n    color = \"Sector\"  # Label for the legend\n  ) +\n  scale_color_manual(values = c(\"Residential\" = \"darkred\", \n                                \"Industrial\" = \"steelblue\",\n                                \"Commercial\" = \"green\",\n                                \"Electric Power\" = \"purple\")) +\n  theme(panel.background = element_rect(fill = \"white\", colour = \"grey50\"),\n        panel.border = element_rect(colour = \"grey50\", fill = NA, size = 0.5))\nggplotly(s)\n\n\n\n\n\n\nThis plot appears to show the U.S. natural gas prices by sector from 1997 to 2023. It tracks the price trends in four different sectors: Residential, Industrial, Commercial, and Electric Power. Residential sector consistently pays the most for natural gas while electric power sector pays least for natural gas, any periods of sharp price increases or decreases, and the relative volatility of prices in each sector. The prices of natural gas across shows a sudden rise in 2008 while a fall in 2020 due to Covid-19 across all sectors. Currently, eventhough the production and consumption of natural gas is increasing the prices have decreased."
  },
  {
    "objectID": "dataviz.html#natural-gas-production-and-consumption",
    "href": "dataviz.html#natural-gas-production-and-consumption",
    "title": "Data Visualization",
    "section": "Natural Gas Production and Consumption",
    "text": "Natural Gas Production and Consumption\n\nNatural Gas Production and ConsumptionNatural Gas Consumption by Sector\n\n\n\n\nView Code\nprod_consump_df<-read.csv(\"datasets/Book5.csv\")\n\nfig <- plot_ly(prod_consump_df, x = ~Year)\n\nfig <- fig %>% add_lines(y = ~Consumption, name = \"Consumption\", \n                         text = ~paste(\"Year:\", Year, \"<br>Consumption:\", Consumption),\n                         hoverinfo = \"text\")\n\nfig <- fig %>% add_lines(y = ~Production, name = \"Production\", visible = F,\n                         text = ~paste(\"Year:\", Year, \"<br>Production:\", Production),\n                         hoverinfo = \"text\")\n\nfig <- fig %>% layout(\n   title = paste(\"U.S. Natural Gas Consumption/Production\", \"<br><sub>From 2005 to 2022</sub>\"),\n    yaxis = list(title = \"Consumption (in trillion cubic feet)\"),\n    updatemenus = list(\n      list(\n        y = 1.1,\n       \n        buttons = list(\n          list(method = \"update\",\n           args = list(list(visible = list(TRUE, FALSE)),\n                           list(yaxis = list(title = \"Consumption (in trillion cubic feet)\"))),\n               label = \"Consumption\"),\n\n          list(method = \"update\",\n              args = list(list(visible = list(FALSE,TRUE)),\n                           list(yaxis = list(title = \"Production (in billion cubic meters)\"))),\n               label = \"Production\")\n          \n          )\n      )\n    )\n  )\nfig\n\n\n\n\n\n\nConsumption of natural gas in the United States amounted to 32.26 trillion cubic feet in 2022. This was a record high, up from 30.65 trillion cubic feet the previous year. Natural gas consumption increased consistently from 2009 to 2019, after over a decade of fluctuation from 1995 to 2008, before decreasing during the COVID-19 pandemic.Similarly, production of natural gas in the United States has increased consistently from 2009 to 2019, after over a decade of fluctuation from 1995 to 2008, before decreasing during the COVID-19 pandemic.\n\n\n\n\nView Code\nconsump_sec_df<-read.csv(\"datasets/Book6.csv\")\n\nconsump_sec_df$sector1 <- consump_sec_df$sector\n\ndf2 <- dplyr::select(consump_sec_df, -sector)\n\nggplot(consump_sec_df, aes(x = Year, y = consumption, group=sector)) +\n geom_line(data=df2, aes(x = Year, y = consumption, group=sector1), colour=\"grey\")+\n  geom_line(aes(colour = sector),show.legend = FALSE) +\n   xlab(\"Year\")+\n  ylab(\"consumption (in billion cubic feet)\")+\n   theme(panel.background = element_rect(fill = \"white\", colour = \"grey50\"),panel.border = element_rect(colour = \"grey50\", fill=NA, size=0.5))+\n  labs(\n    title = \"U.S. Natural Gas Consumption by Sector\",\n    subtitle = \"From 2005-2022\")+\n  facet_wrap(~sector)\n\n\n\n\n\nFrom the graph we can see that natural gas is typically used in the United States to generate electricity. As a result, the electric power sector is responsible for the largest share of natural gas consumption in the U.S., followed closely by the industrial sector. Consumption of natural gas by the electric power sector has doubled since 2005, but remained lower than industrial sector consumption until 2012."
  },
  {
    "objectID": "dataviz.html#industry-employment",
    "href": "dataviz.html#industry-employment",
    "title": "Data Visualization",
    "section": "Industry Employment",
    "text": "Industry Employment\n\nBy GenderBy StateBy Wage\n\n\n\n                   \nThe visualization shows the distribution of the workforce by gender and age in Natural gas distribution industry. The workforce of Natural gas distribution industry in 2021 was 124,009 people, with 21.4% woman, and 78.6% men highlighting a significant gender disparity in the industry. This disparity points to a broader conversation about gender representation in the energy sector, potentially indicating underlying challenges in workforce diversity and inclusion within this field.\n\n\n\n\nView Code\nimport pandas as pd\nimport plotly.graph_objects as go\n\nemp_df = pd.read_csv('datasets/employment_by_location.csv')\n\n#GET UNIQUE VALUES\nyears = emp_df[\"Year\"].unique()\n\n#INITIALIZE GRAPH OBJECT\nfig = go.Figure()\n\n#DEFINE TRACES FOR ALL POSSIBLE CASES\nfor year in years:\n    df_year = emp_df.query(f\"Year == {year}\")\n    fig.add_trace(\n                go.Choropleth(\n                    locations=df_year['Code'],  # Spatial coordinates\n                    z=df_year['Total Population'].astype(float), # Data to be color-coded\n                    zmin=1000,  # Set the minimum value for the color bar\n                    zmax=16000,  # Set the maximum value for the color bar\n                    locationmode = 'USA-states', # set of locations match entries in `locations`\n                    text=df_year.apply(lambda df_year: f\"Year: {df_year['Year']}<br>State: {df_year['State']}<br>Workforce: {df_year['Total Population']}\", axis=1), #hovertext\n                    hoverinfo=\"text\",\n                    colorscale = 'Greens',\n                    colorbar_title=\"Workforce\",\n                    visible=False\n                )    \n            )  \n        \n#MAKE FIRST TRACE VISABLE\nfig.data[0].visible = True\n\n# Create and add slider\nsteps = []\n\nfor i in range(len(fig.data)):\n    step = dict(\n        method=\"update\",\n        args=[{\"visible\": [False] * len(fig.data)},\n              {\"title\": \"U.S. Natural Gas Distribution Employment by Location in \" + str(2021-i)}],  # layout attribute\n        label=str(2021-i)\n    )\n    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n    steps.append(step)\n\n#DEFINE SLIDERS\nsliders = [dict(\n    active=10,\n    currentvalue={\"prefix\": \"Year: \"},\n    pad={\"t\": 50},\n    steps=steps\n)]\n\nfig.update_layout(\n    sliders=sliders\n)\n\nfig.update_layout(\n    title_text = 'U.S. Natural Gas Distribution Workforce by Location in 2021',\n    geo_scope='usa', # limite map scope to USA\n)\n\nfig.update_layout(template=\"plotly_white\")\nfig['layout']['sliders'][0]['pad']=dict(r= 1, t= 1,)\n\nfig.write_html(\"emp_viz.html\")\n\nfig.show()\n\n\n \n\n  \n\n\nThis map shows the states in the United States shaded by workforce for Natural gas distribution Industry. The states that concentrated the largest workforce in 2021 were Texas (14.9k), California (11.8k), and Pennsylvania (10.5k), indicating these states as major hubs for this sector in the U.S. This concentration could be tied to its large population, hot climate and extensive industrial sector, and the state depends on reliable and affordable energy.\n\n\n\n\nView Code\nwage_df<-read.csv(\"datasets/Yearly Wage Ranking.csv\")\n\nt <- ggplot(wage_df, aes(x = Year, y = Average.Wage))+\n \n   geom_bar(stat = \"identity\", fill = \"#92c54b\") +\n\n   labs(\n    title = paste(\"U.S. Natural Gas Distribution Workforce Yearly Wage From 2014-2021\"), \n    x = \"Year\",\n    y = \"Average Annual Wage\")+\n  theme(panel.background = element_rect(fill = \"white\", colour = \"grey50\"),\n        panel.border = element_rect(colour = \"grey50\", fill = NA, size = 0.5),)\nggplotly(t)\n\n\n\n\n\n\nThe graph shows the average annual wages in the natural gas distribution industry from 2014-2021. The plot is showing an increasing trend. In 2021, Natural gas distribution Industry had an average annual wage of $86,281, $26,686 more than the average national salary of $59,596.Looking at the graph we can say that the Natural Gas Distribution industry offers competitive wages that have consistently increased over the analyzed period (2014-2021). This upward trend suggests a positive economic outlook or growing value in the industry. Additionally, the fact that the industry’s average wage substantially exceeds the national average indicates the industry’s potential attractiveness to job seekers due to its higher compensation levels."
  },
  {
    "objectID": "dl.html#getting-started",
    "href": "dl.html#getting-started",
    "title": "Deep Learning for TS",
    "section": "Getting started",
    "text": "Getting started"
  },
  {
    "objectID": "dl.html#data-science",
    "href": "dl.html#data-science",
    "title": "Deep Learning for TS",
    "section": "Data science",
    "text": "Data science"
  },
  {
    "objectID": "dl_ts.html",
    "href": "dl_ts.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "View Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport yfinance as yf\nimport plotly.express as px\nimport statsmodels.api as sm\nfrom IPython.display import IFrame\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU"
  },
  {
    "objectID": "dl_ts.html#load-data",
    "href": "dl_ts.html#load-data",
    "title": "Deep Learning for TS",
    "section": "Load data",
    "text": "Load data\n\n\nView Code\nconsumption = pd.read_csv(\"datasets/eda/Consumption.csv\")\nconsumption = consumption.rename(columns={'Month':'t', 'Consumption':'y'})\nconsumption = consumption[['t', 'y']]\nt = np.array([*range(0, consumption.shape[0])])\nx = np.array(consumption[\"y\"]).reshape(t.shape[0], 1)\nfeature_columns = [0]  # columns to use as features\ntarget_columns = [0]  # columns to use as targets\n\n\n\n\nView Code\nprint(type(t), type(x))\nprint(t.shape, x.shape)\n\n\n<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n(216,) (216, 1)"
  },
  {
    "objectID": "dl_ts.html#visualize-raw-data",
    "href": "dl_ts.html#visualize-raw-data",
    "title": "Deep Learning for TS",
    "section": "Visualize “raw” data",
    "text": "Visualize “raw” data\n\n\nView Code\nfig, ax = plt.subplots()\nfor i in range(0, x.shape[1]):\n    ax.plot(t, x[:, i], \"o\", alpha=0.5)\n    ax.plot(t, x[:, i], \"-\")\nax.plot(t, 0 * x[:, 0], \"-\")  # add baseline for reference\nplt.show()"
  },
  {
    "objectID": "dl_ts.html#normalize",
    "href": "dl_ts.html#normalize",
    "title": "Deep Learning for TS",
    "section": "Normalize",
    "text": "Normalize\n\n\nView Code\nprint(np.mean(x, axis=0).shape, np.std(x, axis=0).shape)\nx = (x - np.mean(x, axis=0)) / np.std(x, axis=0)\nprint(x.shape)\n\n\n(1,) (1,)\n(216, 1)\n\n\n\n\nView Code\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x.shape[1]):\n    ax.plot(t, x[:, i], \"o\")\n    ax.plot(t, x[:, i], \"-\")\nax.plot(t, 0 * x[:, 0], \"-\")  # add baseline for reference\nplt.show()"
  },
  {
    "objectID": "dl_ts.html#split",
    "href": "dl_ts.html#split",
    "title": "Deep Learning for TS",
    "section": "Split",
    "text": "Split\nPartition into training and validation\n\n\nView Code\nsplit_fraction = 0.75\ncut = int(split_fraction * x.shape[0])\ntt = t[0:cut]\nxt = x[0:cut]\ntv = t[cut:]\nxv = x[cut:]\n\n\n\n\nView Code\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x.shape[1]):\n    ax.plot(tt, xt[:, i], \"ro\", alpha=0.25)\n    ax.plot(tt, xt[:, i], \"g-\")\nfor i in range(0, x.shape[1]):\n    ax.plot(tv, xv[:, i], \"bo\", alpha=0.25)\n    ax.plot(tv, xv[:, i], \"g-\")\nplt.show()"
  },
  {
    "objectID": "dl_ts.html#mini-batch-creation",
    "href": "dl_ts.html#mini-batch-creation",
    "title": "Deep Learning for TS",
    "section": "Mini-batch creation",
    "text": "Mini-batch creation\n\n\nView Code\n# function to form time-series mini-batches\n# inputs are the mini-batching hyper-params\n\n# x-> 304 x 3\n\n\ndef form_arrays(\n    x,\n    lookback=3,\n    delay=1,\n    step=1,\n    feature_columns=[0],\n    target_columns=[0],\n    unique=False,\n    verbose=False,\n):\n    # verbose=True --> report and plot for debugging\n    # unique=True --> don't re-sample:\n    # x1,x2,x3 --> x4 then x4,x5,x6 --> x7 instead of x2,x3,x4 --> x5\n\n    # initialize\n    i_start = 0\n    count = 0\n\n    # initialize output arrays with samples\n    x_out = []\n    y_out = []\n\n    # sequentially build mini-batch samples\n    while i_start + lookback + delay < x.shape[0]:\n\n        # define index bounds\n        i_stop = i_start + lookback\n        i_pred = i_stop + delay\n\n        # report if desired\n        if verbose and count < 2:\n            print(\"indice range:\", i_start, i_stop, \"-->\", i_pred)\n\n        # define arrays:\n        # method-1: buggy due to indexing from left\n        # numpy's slicing --> start:stop:step\n        # xtmp=x[i_start:i_stop+1:steps]\n\n        # method-2: non-vectorized but cleaner\n        indices_to_keep = []\n        j = i_stop\n        while j >= i_start:\n            indices_to_keep.append(j)\n            j = j - step\n\n        # create mini-batch sample\n        xtmp = x[indices_to_keep, :]  # isolate relevant indices\n        xtmp = xtmp[:, feature_columns]  # isolate desire features\n        ytmp = x[i_pred, target_columns]\n        x_out.append(xtmp)\n        y_out.append(ytmp)\n\n        # report if desired\n        if verbose and count < 2:\n            print(xtmp, \"-->\", ytmp)\n        if verbose and count < 2:\n            print(\"shape:\", xtmp.shape, \"-->\", ytmp.shape)\n\n        # PLOT FIRST SAMPLE IF DESIRED FOR DEBUGGING\n        if verbose and count < 2:\n            fig, ax = plt.subplots()\n            ax.plot(x, \"b-\")\n            ax.plot(x, \"bx\")\n            ax.plot(indices_to_keep, xtmp, \"go\")\n            ax.plot(i_pred * np.ones(len(target_columns)), ytmp, \"ro\")\n            plt.show()\n\n        # UPDATE START POINT\n        if unique:\n            i_start += lookback\n        i_start += 1\n        count += 1\n\n    return np.array(x_out), np.array(y_out)\n\n\n\n\nView Code\nL=4; S=1; D=1\n\nXt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=True)\n\n# validation\nXv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=True)\n\nprint(\"training:\",Xt.shape,Yt.shape)\nprint(\"validation:\",Xv.shape,Yv.shape)\n\n\nindice range: 0 4 --> 5\n[[-1.43677685e+00]\n [-1.01144131e+00]\n [-1.48583565e-04]\n [ 7.80761951e-02]\n [ 7.48623565e-01]] --> [-1.41257488]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-1.41257488e+00]\n [-1.43677685e+00]\n [-1.01144131e+00]\n [-1.48583565e-04]\n [ 7.80761951e-02]] --> [-1.09191406]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 0 4 --> 5\n[[ 1.07264267]\n [ 0.16822216]\n [-0.10471185]\n [ 0.22706476]\n [ 0.30462503]] --> [1.66895021]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[ 1.66895021]\n [ 1.07264267]\n [ 0.16822216]\n [-0.10471185]\n [ 0.22706476]] --> [2.5622339]\nshape: (5, 1) --> (1,)\n\n\n\n\n\ntraining: (157, 5, 1) (157, 1)\nvalidation: (49, 5, 1) (49, 1)"
  },
  {
    "objectID": "dl_ts.html#reshape",
    "href": "dl_ts.html#reshape",
    "title": "Deep Learning for TS",
    "section": "Reshape",
    "text": "Reshape\n\n\nView Code\nprint(\"---------- Data setup ----------\")\n# RECALL\nprint(\"training:\", Xt.shape, Yt.shape)\nprint(\"validation:\", Xv.shape, Yv.shape)\n\n# RESHAPE INTO A DATA FRAME\nXt1 = Xt.reshape(Xt.shape[0], Xt.shape[1] * Xt.shape[2])\nXv1 = Xv.reshape(Xv.shape[0], Xv.shape[1] * Xv.shape[2])\n\n# NEW SIZES\nprint(Xt1.shape, \"-->\", Yt.shape)\nprint(Xv1.shape, \"-->\", Yv.shape)\n\n\n---------- Data setup ----------\ntraining: (157, 5, 1) (157, 1)\nvalidation: (49, 5, 1) (49, 1)\n(157, 5) --> (157, 1)\n(49, 5) --> (49, 1)"
  },
  {
    "objectID": "dl_ts.html#rnn",
    "href": "dl_ts.html#rnn",
    "title": "Deep Learning for TS",
    "section": "RNN",
    "text": "RNN\n\n\nView Code\n# NEW SIZES\nprint(Xt.shape, \"-->\", Yt.shape)\nprint(Xv.shape, \"-->\", Yv.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\n                     )\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt,\n    Yt,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv, Yv),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\n\nerror_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_MSE': [mean_squared_error(Yt, Ytp)],\n    'training_MAE': [mean_absolute_error(Yt, Ytp)],\n    'training_RMSE': [training_RMSE],\n    'validation_MSE': [mean_squared_error(Yv, Yvp)],\n    'validation_MAE': [mean_absolute_error(Yv, Yvp)],\n    'Validation_RMSE': [validation_RMSE]\n}\n\n\n(137, 25, 1) --> (137, 1)\n(29, 25, 1) --> (29, 1)\nModel: \"sequential_47\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_19 (SimpleRNN)   (None, 32)                1088      \n                                                                 \n dense_45 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 8ms/step\n1/1 [==============================] - 0s 37ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.05053047134509638\n MAE: 0.17307047780654716\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.22622829182028734\n MAE: 0.3700426347004958\n\n\n\n\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt.shape, \"-->\", Yt.shape)\nprint(Xv.shape, \"-->\", Yv.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt,\n    Yt,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv, Yv),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\nerror_table['model'].append('Recurrent Neural Network - L2 reg')\nerror_table['training_MSE'].append(mean_squared_error(Yt, Ytp))\nerror_table['training_MAE'].append(mean_absolute_error(Yt, Ytp))\nerror_table['training_RMSE'].append(training_RMSE)\nerror_table['validation_MSE'].append(mean_squared_error(Yv, Yvp))\nerror_table['validation_MAE'].append(mean_absolute_error(Yv, Yvp))\nerror_table['Validation_RMSE'].append(validation_RMSE)\n\n\n(137, 25, 1) --> (137, 1)\n(29, 25, 1) --> (29, 1)\nModel: \"sequential_48\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_20 (SimpleRNN)   (None, 32)                1088      \n                                                                 \n dense_46 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 7ms/step\n1/1 [==============================] - 0s 114ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.05183517016500182\n MAE: 0.17257064009999173\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.12334667036404573\n MAE: 0.2816170702077876"
  },
  {
    "objectID": "dl_ts.html#gru",
    "href": "dl_ts.html#gru",
    "title": "Deep Learning for TS",
    "section": "GRU",
    "text": "GRU\n\n\nView Code\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n#batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt,\n    Yt,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv, Yv),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\nerror_table['model'].append('GRU Neural Network')\nerror_table['training_MSE'].append(mean_squared_error(Yt, Ytp))\nerror_table['training_MAE'].append(mean_absolute_error(Yt, Ytp))\nerror_table['training_RMSE'].append(training_RMSE)\nerror_table['validation_MSE'].append(mean_squared_error(Yv, Yvp))\nerror_table['validation_MAE'].append(mean_absolute_error(Yv, Yvp))\nerror_table['Validation_RMSE'].append(validation_RMSE)\n\n\nModel: \"sequential_49\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_13 (GRU)                (None, 32)                3360      \n                                                                 \n dense_47 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 6ms/step\n1/1 [==============================] - 0s 29ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.09210219796686013\n MAE: 0.23578547503021127\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.12443122517613742\n MAE: 0.2515008846085269\n\n\n\n\n\n\n\n\n\n\nView Code\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n#batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt,\n    Yt,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv, Yv),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\nerror_table['model'].append('GRU Neural Network - L2 reg')\nerror_table['training_MSE'].append(mean_squared_error(Yt, Ytp))\nerror_table['training_MAE'].append(mean_absolute_error(Yt, Ytp))\nerror_table['training_RMSE'].append(training_RMSE)\nerror_table['validation_MSE'].append(mean_squared_error(Yv, Yvp))\nerror_table['validation_MAE'].append(mean_absolute_error(Yv, Yvp))\nerror_table['Validation_RMSE'].append(validation_RMSE)\n\n\nModel: \"sequential_50\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_14 (GRU)                (None, 32)                3360      \n                                                                 \n dense_48 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 6ms/step\n1/1 [==============================] - 0s 27ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.09844711004902787\n MAE: 0.2373325362284778\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.1316382215317313\n MAE: 0.26988027540311876"
  },
  {
    "objectID": "dl_ts.html#lstm",
    "href": "dl_ts.html#lstm",
    "title": "Deep Learning for TS",
    "section": "LSTM",
    "text": "LSTM\n\n\nView Code\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\nerror_table['model'].append('LSTM Neural Network')\nerror_table['training_MSE'].append(mean_squared_error(Yt, Ytp))\nerror_table['training_MAE'].append(mean_absolute_error(Yt, Ytp))\nerror_table['training_RMSE'].append(training_RMSE)\nerror_table['validation_MSE'].append(mean_squared_error(Yv, Yvp))\nerror_table['validation_MAE'].append(mean_absolute_error(Yv, Yvp))\nerror_table['Validation_RMSE'].append(validation_RMSE)\n\n\nModel: \"sequential_51\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_13 (LSTM)              (None, 32)                4352      \n                                                                 \n dense_49 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 1s 6ms/step\n1/1 [==============================] - 0s 30ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.0955949331562601\n MAE: 0.23919241974621855\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.0676928820252565\n MAE: 0.7333692664010899\n\n\n\n\n\n\n\n\n\n\nView Code\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\nerror_table['model'].append('LSTM Neural Network - L2 reg')\nerror_table['training_MSE'].append(mean_squared_error(Yt, Ytp))\nerror_table['training_MAE'].append(mean_absolute_error(Yt, Ytp))\nerror_table['training_RMSE'].append(training_RMSE)\nerror_table['validation_MSE'].append(mean_squared_error(Yv, Yvp))\nerror_table['validation_MAE'].append(mean_absolute_error(Yv, Yvp))\nerror_table['Validation_RMSE'].append(validation_RMSE)\n\n\nModel: \"sequential_52\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_14 (LSTM)              (None, 32)                4352      \n                                                                 \n dense_50 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 5ms/step\n1/1 [==============================] - 0s 32ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.14689253753394854\n MAE: 0.30902141055650023\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.5509573826378982\n MAE: 0.586544130029394"
  },
  {
    "objectID": "dl_ts.html#question-1--model-comparison",
    "href": "dl_ts.html#question-1--model-comparison",
    "title": "Deep Learning for TS",
    "section": "Question 1- model comparison",
    "text": "Question 1- model comparison\n\n\nView Code\nfrom IPython.display import display, Markdown\nerror_df = pd.DataFrame(error_table)\n# Display the DataFrame with a caption\ndisplay(Markdown(\"**Table 1: Errors**\"))\ndisplay(error_df)\n\n\nTable 1: Errors\n\n\n\n\n  \n    \n\n\n  \n    \n      \n      model\n      training_MSE\n      training_MAE\n      training_RMSE\n      validation_MSE\n      validation_MAE\n      Validation_RMSE\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.050530\n      0.173070\n      0.224790\n      0.226228\n      0.370043\n      0.475635\n    \n    \n      1\n      Recurrent Neural Network - L2 reg\n      0.051835\n      0.172571\n      0.227673\n      0.123347\n      0.281617\n      0.351207\n    \n    \n      2\n      GRU Neural Network\n      0.092102\n      0.235785\n      0.303483\n      0.124431\n      0.251501\n      0.352748\n    \n    \n      3\n      GRU Neural Network - L2 reg\n      0.098447\n      0.237333\n      0.313763\n      0.131638\n      0.269880\n      0.362820\n    \n    \n      4\n      LSTM Neural Network\n      0.095595\n      0.239192\n      0.309184\n      1.067693\n      0.733369\n      1.033292\n    \n    \n      5\n      LSTM Neural Network - L2 reg\n      0.146893\n      0.309021\n      0.383266\n      0.550957\n      0.586544\n      0.742265"
  },
  {
    "objectID": "dl_ts.html#utility-function",
    "href": "dl_ts.html#utility-function",
    "title": "Deep Learning for TS",
    "section": "Utility function",
    "text": "Utility function\nFunction for reporting results\n\n\nView Code\nfrom sklearn.metrics import (\n    mean_squared_error,\n    mean_absolute_percentage_error,\n    mean_absolute_error,\n)\n\n\n# UTILITY FUNCTION\ndef regression_report(yt, ytp, yv, yvp):\n    print(\"---------- Regression report ----------\")\n\n    print(\"TRAINING:\")\n    print(\" MSE:\", mean_squared_error(yt, ytp))\n    print(\" MAE:\", mean_absolute_error(yt, ytp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n\n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yt, ytp, \"ro\")\n    ax.plot(yt, yt, \"b-\")\n    ax.set(\n        xlabel=\"y_data\",\n        ylabel=\"y_predicted\",\n        title=\"Training data parity plot (line y=x represents a perfect fit)\",\n    )\n    plt.show()\n\n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    frac_plot = 1.0\n    upper = int(frac_plot * yt.shape[0])\n    # print(int(0.5*yt.shape[0]))\n    fig, ax = plt.subplots()\n    ax.plot(yt[0:upper], \"b-\")\n    ax.plot(ytp[0:upper], \"r-\", alpha=0.5)\n    ax.plot(ytp[0:upper], \"ro\", alpha=0.25)\n    ax.set(\n        xlabel=\"index\",\n        ylabel=\"y(t (blue=actual & red=prediction)\",\n        title=\"Training: Time-series prediction\",\n    )\n    plt.show()\n\n    print(\"VALIDATION:\")\n    print(\" MSE:\", mean_squared_error(yv, yvp))\n    print(\" MAE:\", mean_absolute_error(yv, yvp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n\n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yv, yvp, \"ro\")\n    ax.plot(yv, yv, \"b-\")\n    ax.set(\n        xlabel=\"y_data\",\n        ylabel=\"y_predicted\",\n        title=\"Validation data parity plot (line y=x represents a perfect fit)\",\n    )\n    plt.show()\n\n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    upper = int(frac_plot * yv.shape[0])\n    fig, ax = plt.subplots()\n    ax.plot(yv[0:upper], \"b-\")\n    ax.plot(yvp[0:upper], \"r-\", alpha=0.5)\n    ax.plot(yvp[0:upper], \"ro\", alpha=0.25)\n    ax.set(\n        xlabel=\"index\",\n        ylabel=\"y(t) (blue=actual & red=prediction)\",\n        title=\"Validation: Time-series prediction\",\n    )\n    plt.show()\n\ndef history_plot(history):\n    FS=18   #FONT SIZE\n    # PLOTTING THE TRAINING AND VALIDATION LOSS\n    history_dict = history.history\n    loss_values = history_dict[\"loss\"]\n    val_loss_values = history_dict[\"val_loss\"]\n    epochs = range(1, len(loss_values) + 1)\n    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n    plt.title(\"Training and validation loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()"
  },
  {
    "objectID": "ts_dl.html",
    "href": "ts_dl.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "On this page, I will be exploring deep learning methodologies for time series analysis. I’m going to implement various deep learning models like RNN, GRUs and LSTMs, with and without L2 regularization using structured approach outlined on this page. Finally, I’ll compare the results of deep learning models with traditional TS models used across this project."
  },
  {
    "objectID": "ts_dl.html#visualize-raw-data",
    "href": "ts_dl.html#visualize-raw-data",
    "title": "Deep Learning for TS",
    "section": "Visualize “raw” data",
    "text": "Visualize “raw” data\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\nfig, ax = plt.subplots()\nfor i in range(0, x.shape[1]):\n    ax.plot(t, x[:, i], \"o\", alpha=0.5)\n    ax.plot(t, x[:, i], \"-\")\nax.plot(t, 0 * x[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nfig, ax = plt.subplots()\nfor i in range(0, x_consump.shape[1]):\n    ax.plot(t_consump, x_consump[:, i], \"o\", alpha=0.5)\n    ax.plot(t_consump, x_consump[:, i], \"-\")\nax.plot(t_consump, 0 * x_consump[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nfig, ax = plt.subplots()\nfor i in range(0, x_prod.shape[1]):\n    ax.plot(t_prod, x_prod[:, i], \"o\", alpha=0.5)\n    ax.plot(t_prod, x_prod[:, i], \"-\")\nax.plot(t_prod, 0 * x_prod[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nfig, ax = plt.subplots()\nfor i in range(0, x_imp.shape[1]):\n    ax.plot(t_imp, x_imp[:, i], \"o\", alpha=0.5)\n    ax.plot(t_imp, x_imp[:, i], \"-\")\nax.plot(t_imp, 0 * x_imp[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nfig, ax = plt.subplots()\nfor i in range(0, x_exp.shape[1]):\n    ax.plot(t_exp, x_exp[:, i], \"o\", alpha=0.5)\n    ax.plot(t_exp, x_exp[:, i], \"-\")\nax.plot(t_exp, 0 * x_exp[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nfig, ax = plt.subplots()\nfor i in range(0, x_stk.shape[1]):\n    ax.plot(t_stk, x_stk[:, i], \"o\", alpha=0.5)\n    ax.plot(t_stk, x_stk[:, i], \"-\")\nax.plot(t_stk, 0 * x_stk[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nfig, ax = plt.subplots()\nfor i in range(0, x_emissions.shape[1]):\n    ax.plot(t_emissions, x_emissions[:, i], \"o\", alpha=0.5)\n    ax.plot(t_emissions, x_emissions[:, i], \"-\")\nax.plot(t_emissions, 0 * x_emissions[:, 0], \"-\")  # add baseline for reference\nplt.show()"
  },
  {
    "objectID": "ts_dl.html#normalize",
    "href": "ts_dl.html#normalize",
    "title": "Deep Learning for TS",
    "section": "Normalize",
    "text": "Normalize\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\nprint(\"----Shape of statistical measures and the normalized Data----\")\nprint(np.mean(x, axis=0).shape, np.std(x, axis=0).shape)\nx = (x - np.mean(x, axis=0)) / np.std(x, axis=0)\nprint(x.shape)\n\n\n----Shape of statistical measures and the normalized Data----\n(1,) (1,)\n(216, 1)\n\n\n\n\nView Code\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x.shape[1]):\n    ax.plot(t, x[:, i], \"o\")\n    ax.plot(t, x[:, i], \"-\")\nax.plot(t, 0 * x[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nprint(\"----Shape of statistical measures and the normalized Data----\")\nprint(np.mean(x_consump, axis=0).shape, np.std(x_consump, axis=0).shape)\nx_consump = (x_consump - np.mean(x_consump, axis=0)) / np.std(x_consump, axis=0)\nprint(x_consump.shape)\n\n\n----Shape of statistical measures and the normalized Data----\n(1,) (1,)\n(216, 1)\n\n\n\n\nView Code\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_consump.shape[1]):\n    ax.plot(t_consump, x_consump[:, i], \"o\")\n    ax.plot(t_consump, x_consump[:, i], \"-\")\nax.plot(t_consump, 0 * x_consump[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nprint(\"----Shape of statistical measures and the normalized Data----\")\nprint(np.mean(x_prod, axis=0).shape, np.std(x_prod, axis=0).shape)\nx_prod = (x_prod - np.mean(x_prod, axis=0)) / np.std(x_prod, axis=0)\nprint(x_prod.shape)\n\n\n----Shape of statistical measures and the normalized Data----\n(1,) (1,)\n(216, 1)\n\n\n\n\nView Code\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_prod.shape[1]):\n    ax.plot(t_prod, x_prod[:, i], \"o\")\n    ax.plot(t_prod, x_prod[:, i], \"-\")\nax.plot(t_prod, 0 * x_prod[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nprint(\"----Shape of statistical measures and the normalized Data----\")\nprint(np.mean(x_imp, axis=0).shape, np.std(x_imp, axis=0).shape)\nx_imp = (x_imp - np.mean(x_imp, axis=0)) / np.std(x_imp, axis=0)\nprint(x_imp.shape)\n\n\n----Shape of statistical measures and the normalized Data----\n(1,) (1,)\n(312, 1)\n\n\n\n\nView Code\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_imp.shape[1]):\n    ax.plot(t_imp, x_imp[:, i], \"o\")\n    ax.plot(t_imp, x_imp[:, i], \"-\")\nax.plot(t_imp, 0 * x_imp[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nprint(\"----Shape of statistical measures and the normalized Data----\")\nprint(np.mean(x_exp, axis=0).shape, np.std(x_exp, axis=0).shape)\nx_exp = (x_exp - np.mean(x_exp, axis=0)) / np.std(x_exp, axis=0)\nprint(x_exp.shape)\n\n\n----Shape of statistical measures and the normalized Data----\n(1,) (1,)\n(312, 1)\n\n\n\n\nView Code\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_exp.shape[1]):\n    ax.plot(t_exp, x_exp[:, i], \"o\")\n    ax.plot(t_exp, x_exp[:, i], \"-\")\nax.plot(t_exp, 0 * x_exp[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nprint(\"----Shape of statistical measures and the normalized Data----\")\nprint(np.mean(x_stk, axis=0).shape, np.std(x_stk, axis=0).shape)\nx_stk = (x_stk - np.mean(x_stk, axis=0)) / np.std(x_stk, axis=0)\nprint(x_stk.shape)\n\n\n----Shape of statistical measures and the normalized Data----\n(1,) (1,)\n(4510, 1)\n\n\n\n\nView Code\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_stk.shape[1]):\n    ax.plot(t_stk, x_stk[:, i], \"o\")\n    ax.plot(t_stk, x_stk[:, i], \"-\")\nax.plot(t_stk, 0 * x_stk[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nprint(\"----Shape of statistical measures and the normalized Data----\")\nprint(np.mean(x_emissions, axis=0).shape, np.std(x_emissions, axis=0).shape)\nx_emissions = (x_emissions - np.mean(x_emissions, axis=0)) / np.std(x_emissions, axis=0)\nprint(x_emissions.shape)\n\n\n----Shape of statistical measures and the normalized Data----\n(1,) (1,)\n(310, 1)\n\n\n\n\nView Code\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_emissions.shape[1]):\n    ax.plot(t_emissions, x_emissions[:, i], \"o\")\n    ax.plot(t_emissions, x_emissions[:, i], \"-\")\nax.plot(t_emissions, 0 * x_emissions[:, 0], \"-\")  # add baseline for reference\nplt.show()"
  },
  {
    "objectID": "ts_dl.html#split",
    "href": "ts_dl.html#split",
    "title": "Deep Learning for TS",
    "section": "Split",
    "text": "Split\nPartition into training and validation\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\nsplit_fraction = 0.75\ncut = int(split_fraction * x.shape[0])\ntt = t[0:cut]\nxt = x[0:cut]\ntv = t[cut:]\nxv = x[cut:]\n\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x.shape[1]):\n    ax.plot(tt, xt[:, i], \"ro\", alpha=0.25)\n    ax.plot(tt, xt[:, i], \"g-\")\nfor i in range(0, x.shape[1]):\n    ax.plot(tv, xv[:, i], \"bo\", alpha=0.25)\n    ax.plot(tv, xv[:, i], \"g-\")\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nsplit_fraction = 0.75\ncut_consump = int(split_fraction * x_consump.shape[0])\ntt_consump = t_consump[0:cut_consump]\nxt_consump = x_consump[0:cut_consump]\ntv_consump = t_consump[cut_consump:]\nxv_consump = x_consump[cut_consump:]\n\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_consump.shape[1]):\n    ax.plot(tt_consump, xt_consump[:, i], \"ro\", alpha=0.25)\n    ax.plot(tt_consump, xt_consump[:, i], \"g-\")\nfor i in range(0, x_consump.shape[1]):\n    ax.plot(tv_consump, xv_consump[:, i], \"bo\", alpha=0.25)\n    ax.plot(tv_consump, xv_consump[:, i], \"g-\")\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nsplit_fraction = 0.75\ncut_prod = int(split_fraction * x_prod.shape[0])\ntt_prod = t_prod[0:cut_prod]\nxt_prod = x_prod[0:cut_prod]\ntv_prod = t_prod[cut_prod:]\nxv_prod = x_prod[cut_prod:]\n\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_prod.shape[1]):\n    ax.plot(tt_prod, xt_prod[:, i], \"ro\", alpha=0.25)\n    ax.plot(tt_prod, xt_prod[:, i], \"g-\")\nfor i in range(0, x_prod.shape[1]):\n    ax.plot(tv_prod, xv_prod[:, i], \"bo\", alpha=0.25)\n    ax.plot(tv_prod, xv_prod[:, i], \"g-\")\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nsplit_fraction = 0.75\ncut_imp = int(split_fraction * x_imp.shape[0])\ntt_imp = t_imp[0:cut_imp]\nxt_imp = x_imp[0:cut_imp]\ntv_imp = t_imp[cut_imp:]\nxv_imp = x_imp[cut_imp:]\n\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_imp.shape[1]):\n    ax.plot(tt_imp, xt_imp[:, i], \"ro\", alpha=0.25)\n    ax.plot(tt_imp, xt_imp[:, i], \"g-\")\nfor i in range(0, x_imp.shape[1]):\n    ax.plot(tv_imp, xv_imp[:, i], \"bo\", alpha=0.25)\n    ax.plot(tv_imp, xv_imp[:, i], \"g-\")\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nsplit_fraction = 0.75\ncut_exp = int(split_fraction * x_exp.shape[0])\ntt_exp = t_exp[0:cut_exp]\nxt_exp = x_exp[0:cut_exp]\ntv_exp = t_exp[cut_exp:]\nxv_exp = x_exp[cut_exp:]\n\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_exp.shape[1]):\n    ax.plot(tt_exp, xt_exp[:, i], \"ro\", alpha=0.25)\n    ax.plot(tt_exp, xt_exp[:, i], \"g-\")\nfor i in range(0, x_exp.shape[1]):\n    ax.plot(tv_exp, xv_exp[:, i], \"bo\", alpha=0.25)\n    ax.plot(tv_exp, xv_exp[:, i], \"g-\")\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nsplit_fraction = 0.75\ncut_stk = int(split_fraction * x_stk.shape[0])\ntt_stk = t_stk[0:cut_stk]\nxt_stk = x_stk[0:cut_stk]\ntv_stk = t_stk[cut_stk:]\nxv_stk = x_stk[cut_stk:]\n\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_stk.shape[1]):\n    ax.plot(tt_stk, xt_stk[:, i], \"ro\", alpha=0.25)\n    ax.plot(tt_stk, xt_stk[:, i], \"g-\")\nfor i in range(0, x_stk.shape[1]):\n    ax.plot(tv_stk, xv_stk[:, i], \"bo\", alpha=0.25)\n    ax.plot(tv_stk, xv_stk[:, i], \"g-\")\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nsplit_fraction = 0.75\ncut_emissions = int(split_fraction * x_emissions.shape[0])\ntt_emissions = t_emissions[0:cut_emissions]\nxt_emissions = x_emissions[0:cut_emissions]\ntv_emissions = t_emissions[cut_emissions:]\nxv_emissions = x_emissions[cut_emissions:]\n\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_emissions.shape[1]):\n    ax.plot(tt_emissions, xt_emissions[:, i], \"ro\", alpha=0.25)\n    ax.plot(tt_emissions, xt_emissions[:, i], \"g-\")\nfor i in range(0, x_emissions.shape[1]):\n    ax.plot(tv_emissions, xv_emissions[:, i], \"bo\", alpha=0.25)\n    ax.plot(tv_emissions, xv_emissions[:, i], \"g-\")\nplt.show()"
  },
  {
    "objectID": "ts_dl.html#mini-batch-creation",
    "href": "ts_dl.html#mini-batch-creation",
    "title": "Deep Learning for TS",
    "section": "Mini-batch creation",
    "text": "Mini-batch creation\nFunction to form time-series mini-batches\n\n\nView Code\n# function to form time-series mini-batches\n# inputs are the mini-batching hyper-params\n\n# x-> 304 x 3\n\n\ndef form_arrays(\n    x,\n    lookback=3,\n    delay=1,\n    step=1,\n    feature_columns=[0],\n    target_columns=[0],\n    unique=False,\n    verbose=False,\n):\n    # verbose=True --> report and plot for debugging\n    # unique=True --> don't re-sample:\n    # x1,x2,x3 --> x4 then x4,x5,x6 --> x7 instead of x2,x3,x4 --> x5\n\n    # initialize\n    i_start = 0\n    count = 0\n\n    # initialize output arrays with samples\n    x_out = []\n    y_out = []\n\n    # sequentially build mini-batch samples\n    while i_start + lookback + delay < x.shape[0]:\n\n        # define index bounds\n        i_stop = i_start + lookback\n        i_pred = i_stop + delay\n\n        # report if desired\n        if verbose and count < 2:\n            print(\"indice range:\", i_start, i_stop, \"-->\", i_pred)\n\n        # define arrays:\n        # method-1: buggy due to indexing from left\n        # numpy's slicing --> start:stop:step\n        # xtmp=x[i_start:i_stop+1:steps]\n\n        # method-2: non-vectorized but cleaner\n        indices_to_keep = []\n        j = i_stop\n        while j >= i_start:\n            indices_to_keep.append(j)\n            j = j - step\n\n        # create mini-batch sample\n        xtmp = x[indices_to_keep, :]  # isolate relevant indices\n        xtmp = xtmp[:, feature_columns]  # isolate desire features\n        ytmp = x[i_pred, target_columns]\n        x_out.append(xtmp)\n        y_out.append(ytmp)\n\n        # report if desired\n        if verbose and count < 2:\n            print(xtmp, \"-->\", ytmp)\n        if verbose and count < 2:\n            print(\"shape:\", xtmp.shape, \"-->\", ytmp.shape)\n\n        # PLOT FIRST SAMPLE IF DESIRED FOR DEBUGGING\n        if verbose and count < 2:\n            fig, ax = plt.subplots()\n            ax.plot(x, \"b-\")\n            ax.plot(x, \"bx\")\n            ax.plot(indices_to_keep, xtmp, \"go\")\n            ax.plot(i_pred * np.ones(len(target_columns)), ytmp, \"ro\")\n            plt.show()\n\n        # UPDATE START POINT\n        if unique:\n            i_start += lookback\n        i_start += 1\n        count += 1\n\n    return np.array(x_out), np.array(y_out)\n\n\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\nL=4; S=1; D=1\n\nXt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=True)\n\n# validation\nXv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=True)\n\nprint(\"training:\",Xt.shape,Yt.shape)\nprint(\"validation:\",Xv.shape,Yv.shape)\n\n\nindice range: 0 4 --> 5\n[[0.85610218]\n [1.15450362]\n [1.06801045]\n [0.71338845]\n [0.71771311]] --> [1.16315293]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[1.16315293]\n [0.85610218]\n [1.15450362]\n [1.06801045]\n [0.71338845]] --> [1.35776256]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 0 4 --> 5\n[[-0.17316653]\n [-0.52346387]\n [-0.64455431]\n [-0.66185294]\n [-0.7180735 ]] --> [-0.19478983]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-0.19478983]\n [-0.17316653]\n [-0.52346387]\n [-0.64455431]\n [-0.66185294]] --> [-0.59698306]\nshape: (5, 1) --> (1,)\n\n\n\n\n\ntraining: (157, 5, 1) (157, 1)\nvalidation: (49, 5, 1) (49, 1)\n\n\n\n\n\n\nView Code\nL=4; S=1; D=1\n\nXt_consump,Yt_consump=form_arrays(xt_consump,lookback=L,delay=D,step=S,feature_columns=feature_columns_consump,target_columns=target_columns_consump,unique=False,verbose=True)\n\n# validation\nXv_consump,Yv_consump=form_arrays(xv_consump,lookback=L,delay=D,step=S,feature_columns=feature_columns_consump,target_columns=target_columns_consump,unique=False,verbose=True)\n\nprint(\"training:\",Xt_consump.shape,Yt_consump.shape)\nprint(\"validation:\",Xv_consump.shape,Yv_consump.shape)\n\n\nindice range: 0 4 --> 5\n[[-1.43677685e+00]\n [-1.01144131e+00]\n [-1.48583565e-04]\n [ 7.80761951e-02]\n [ 7.48623565e-01]] --> [-1.41257488]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-1.41257488e+00]\n [-1.43677685e+00]\n [-1.01144131e+00]\n [-1.48583565e-04]\n [ 7.80761951e-02]] --> [-1.09191406]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 0 4 --> 5\n[[ 1.07264267]\n [ 0.16822216]\n [-0.10471185]\n [ 0.22706476]\n [ 0.30462503]] --> [1.66895021]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[ 1.66895021]\n [ 1.07264267]\n [ 0.16822216]\n [-0.10471185]\n [ 0.22706476]] --> [2.5622339]\nshape: (5, 1) --> (1,)\n\n\n\n\n\ntraining: (157, 5, 1) (157, 1)\nvalidation: (49, 5, 1) (49, 1)\n\n\n\n\n\n\nView Code\nL=4; S=1; D=1\n\nXt_prod,Yt_prod=form_arrays(xt_prod,lookback=L,delay=D,step=S,feature_columns=feature_columns_prod,target_columns=target_columns_prod,unique=False,verbose=True)\n\n# validation\nXv_prod,Yv_prod=form_arrays(xv_prod,lookback=L,delay=D,step=S,feature_columns=feature_columns_prod,target_columns=target_columns_prod,unique=False,verbose=True)\n\nprint(\"training:\",Xt_prod.shape,Yt_prod.shape)\nprint(\"validation:\",Xv_prod.shape,Yv_prod.shape)\n\n\nindice range: 0 4 --> 5\n[[-1.1702813 ]\n [-1.2273701 ]\n [-1.04221774]\n [-1.4269967 ]\n [-1.12554455]] --> [-1.23928047]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-1.23928047]\n [-1.1702813 ]\n [-1.2273701 ]\n [-1.04221774]\n [-1.4269967 ]] --> [-1.21881347]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 0 4 --> 5\n[[1.11922115]\n [1.21958425]\n [0.92508149]\n [1.02074385]\n [0.89624295]] --> [1.36032606]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[1.36032606]\n [1.11922115]\n [1.21958425]\n [0.92508149]\n [1.02074385]] --> [1.33331651]\nshape: (5, 1) --> (1,)\n\n\n\n\n\ntraining: (157, 5, 1) (157, 1)\nvalidation: (49, 5, 1) (49, 1)\n\n\n\n\n\n\nView Code\nL=4; S=1; D=1\n\nXt_imp,Yt_imp=form_arrays(xt_imp,lookback=L,delay=D,step=S,feature_columns=feature_columns_imp,target_columns=target_columns_imp,unique=False,verbose=True)\n\n# validation\nXv_imp,Yv_imp=form_arrays(xv_imp,lookback=L,delay=D,step=S,feature_columns=feature_columns_imp,target_columns=target_columns_imp,unique=False,verbose=True)\n\nprint(\"training:\",Xt_imp.shape,Yt_imp.shape)\nprint(\"validation:\",Xv_imp.shape,Yv_imp.shape)\n\n\nindice range: 0 4 --> 5\n[[-0.81519714]\n [-0.88043638]\n [-0.53520443]\n [-0.83698639]\n [-0.14415448]] --> [-0.99167718]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-0.99167718]\n [-0.81519714]\n [-0.88043638]\n [-0.53520443]\n [-0.83698639]] --> [-0.92788811]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 0 4 --> 5\n[[-1.01139213]\n [-1.00395772]\n [-0.87469078]\n [-0.4434583 ]\n [-0.3876727 ]] --> [-0.09820799]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-0.09820799]\n [-1.01139213]\n [-1.00395772]\n [-0.87469078]\n [-0.4434583 ]] --> [0.10123611]\nshape: (5, 1) --> (1,)\n\n\n\n\n\ntraining: (229, 5, 1) (229, 1)\nvalidation: (73, 5, 1) (73, 1)\n\n\n\n\n\n\nView Code\nL=4; S=1; D=1\n\nXt_exp,Yt_exp=form_arrays(xt_exp,lookback=L,delay=D,step=S,feature_columns=feature_columns_exp,target_columns=target_columns_exp,unique=False,verbose=True)\n\n# validation\nXv_exp,Yv_exp=form_arrays(xv_exp,lookback=L,delay=D,step=S,feature_columns=feature_columns_exp,target_columns=target_columns_exp,unique=False,verbose=True)\n\nprint(\"training:\",Xt_exp.shape,Yt_exp.shape)\nprint(\"validation:\",Xv_exp.shape,Yv_exp.shape)\n\n\nindice range: 0 4 --> 5\n[[-0.90925709]\n [-0.88599358]\n [-0.87117889]\n [-0.89479982]\n [-0.89735727]] --> [-0.91276973]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-0.91276973]\n [-0.90925709]\n [-0.88599358]\n [-0.87117889]\n [-0.89479982]] --> [-0.90901675]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 0 4 --> 5\n[[0.43307505]\n [0.11599498]\n [0.27326245]\n [0.34869168]\n [0.1912763 ]] --> [0.57430149]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[0.57430149]\n [0.43307505]\n [0.11599498]\n [0.27326245]\n [0.34869168]] --> [0.69215349]\nshape: (5, 1) --> (1,)\n\n\n\n\n\ntraining: (229, 5, 1) (229, 1)\nvalidation: (73, 5, 1) (73, 1)\n\n\n\n\n\n\nView Code\nL=4; S=1; D=1\n\nXt_stk,Yt_stk=form_arrays(xt_stk,lookback=L,delay=D,step=S,feature_columns=feature_columns_stk,target_columns=target_columns_stk,unique=False,verbose=True)\n\n# validation\nXv_stk,Yv_stk=form_arrays(xv_stk,lookback=L,delay=D,step=S,feature_columns=feature_columns_stk,target_columns=target_columns_stk,unique=False,verbose=True)\n\nprint(\"training:\",Xt_stk.shape,Yt_stk.shape)\nprint(\"validation:\",Xv_stk.shape,Yv_stk.shape)\n\n\nindice range: 0 4 --> 5\n[[-1.54942093]\n [-1.53950359]\n [-1.55403739]\n [-1.55967994]\n [-1.55369545]] --> [-1.54548849]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-1.54548849]\n [-1.54942093]\n [-1.53950359]\n [-1.55403739]\n [-1.55967994]] --> [-1.54514621]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 0 4 --> 5\n[[0.97752522]\n [1.04526005]\n [1.06061717]\n [1.06089093]\n [1.0740536 ]] --> [1.030451]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[1.030451  ]\n [0.97752522]\n [1.04526005]\n [1.06061717]\n [1.06089093]] --> [1.01866013]\nshape: (5, 1) --> (1,)\n\n\n\n\n\ntraining: (3377, 5, 1) (3377, 1)\nvalidation: (1123, 5, 1) (1123, 1)\n\n\n\n\n\n\nView Code\nL=4; S=1; D=1\n\nXt_emissions,Yt_emissions=form_arrays(xt_emissions,lookback=L,delay=D,step=S,feature_columns=feature_columns_emissions,target_columns=target_columns_emissions,unique=False,verbose=True)\n\n# validation\nXv_emissions,Yv_emissions=form_arrays(xv_emissions,lookback=L,delay=D,step=S,feature_columns=feature_columns_emissions,target_columns=target_columns_emissions,unique=False,verbose=True)\n\nprint(\"training:\",Xt_emissions.shape,Yt_emissions.shape)\nprint(\"validation:\",Xv_emissions.shape,Yv_emissions.shape)\n\n\nindice range: 0 4 --> 5\n[[-0.34947117]\n [-0.33473844]\n [-0.29320189]\n [-0.25213225]\n [-0.22136124]] --> [-0.36881048]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-0.36881048]\n [-0.34947117]\n [-0.33473844]\n [-0.29320189]\n [-0.25213225]] --> [-0.3599988]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 0 4 --> 5\n[[-0.17532518]\n [-0.18899325]\n [-0.0868485 ]\n [ 3.74965096]\n [-0.12675237]] --> [-0.24779927]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-0.24779927]\n [-0.17532518]\n [-0.18899325]\n [-0.0868485 ]\n [ 3.74965096]] --> [-0.29592599]\nshape: (5, 1) --> (1,)\n\n\n\n\n\ntraining: (227, 5, 1) (227, 1)\nvalidation: (73, 5, 1) (73, 1)"
  },
  {
    "objectID": "ts_dl.html#utility-function",
    "href": "ts_dl.html#utility-function",
    "title": "Deep Learning for TS",
    "section": "Utility function",
    "text": "Utility function\nFunction for reporting results\n\n\nView Code\nfrom sklearn.metrics import (\n    mean_squared_error,\n    mean_absolute_percentage_error,\n    mean_absolute_error,\n)\n\n\n# UTILITY FUNCTION\ndef regression_report(yt, ytp, yv, yvp):\n    print(\"---------- Regression report ----------\")\n\n    print(\"TRAINING:\")\n    print(\" MSE:\", mean_squared_error(yt, ytp))\n    print(\" MAE:\", mean_absolute_error(yt, ytp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n\n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yt, ytp, \"ro\")\n    ax.plot(yt, yt, \"b-\")\n    ax.set(\n        xlabel=\"y_data\",\n        ylabel=\"y_predicted\",\n        title=\"Training data parity plot (line y=x represents a perfect fit)\",\n    )\n    plt.show()\n\n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    frac_plot = 1.0\n    upper = int(frac_plot * yt.shape[0])\n    # print(int(0.5*yt.shape[0]))\n    fig, ax = plt.subplots()\n    ax.plot(yt[0:upper], \"b-\")\n    ax.plot(ytp[0:upper], \"r-\", alpha=0.5)\n    ax.plot(ytp[0:upper], \"ro\", alpha=0.25)\n    ax.set(\n        xlabel=\"index\",\n        ylabel=\"y(t (blue=actual & red=prediction)\",\n        title=\"Training: Time-series prediction\",\n    )\n    plt.show()\n\n    print(\"VALIDATION:\")\n    print(\" MSE:\", mean_squared_error(yv, yvp))\n    print(\" MAE:\", mean_absolute_error(yv, yvp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n\n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yv, yvp, \"ro\")\n    ax.plot(yv, yv, \"b-\")\n    ax.set(\n        xlabel=\"y_data\",\n        ylabel=\"y_predicted\",\n        title=\"Validation data parity plot (line y=x represents a perfect fit)\",\n    )\n    plt.show()\n\n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    upper = int(frac_plot * yv.shape[0])\n    fig, ax = plt.subplots()\n    ax.plot(yv[0:upper], \"b-\")\n    ax.plot(yvp[0:upper], \"r-\", alpha=0.5)\n    ax.plot(yvp[0:upper], \"ro\", alpha=0.25)\n    ax.set(\n        xlabel=\"index\",\n        ylabel=\"y(t) (blue=actual & red=prediction)\",\n        title=\"Validation: Time-series prediction\",\n    )\n    plt.show()\n\ndef history_plot(history):\n    FS=18   #FONT SIZE\n    # PLOTTING THE TRAINING AND VALIDATION LOSS\n    history_dict = history.history\n    loss_values = history_dict[\"loss\"]\n    val_loss_values = history_dict[\"val_loss\"]\n    epochs = range(1, len(loss_values) + 1)\n    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n    plt.title(\"Training and validation loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()"
  },
  {
    "objectID": "ts_dl.html#reshape",
    "href": "ts_dl.html#reshape",
    "title": "Deep Learning for TS",
    "section": "Reshape",
    "text": "Reshape\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\nprint(\"---------- Data setup ----------\")\n# RECALL\nprint(\"training:\", Xt.shape, Yt.shape)\nprint(\"validation:\", Xv.shape, Yv.shape)\n\n# RESHAPE INTO A DATA FRAME\nXt1 = Xt.reshape(Xt.shape[0], Xt.shape[1] * Xt.shape[2])\nXv1 = Xv.reshape(Xv.shape[0], Xv.shape[1] * Xv.shape[2])\n\n# NEW SIZES\nprint(Xt1.shape, \"-->\", Yt.shape)\nprint(Xv1.shape, \"-->\", Yv.shape)\n\n\n---------- Data setup ----------\ntraining: (157, 5, 1) (157, 1)\nvalidation: (49, 5, 1) (49, 1)\n(157, 5) --> (157, 1)\n(49, 5) --> (49, 1)\n\n\n\n\n\n\nView Code\nprint(\"---------- Data setup ----------\")\n# RECALL\nprint(\"training:\", Xt_consump.shape, Yt_consump.shape)\nprint(\"validation:\", Xv_consump.shape, Yv_consump.shape)\n\n# RESHAPE INTO A DATA FRAME\nXt1_consump = Xt_consump.reshape(Xt_consump.shape[0], Xt_consump.shape[1] * Xt_consump.shape[2])\nXv1_consump = Xv_consump.reshape(Xv_consump.shape[0], Xv_consump.shape[1] * Xv_consump.shape[2])\n\n# NEW SIZES\nprint(Xt1_consump.shape, \"-->\", Yt_consump.shape)\nprint(Xv1_consump.shape, \"-->\", Yv_consump.shape)\n\n\n---------- Data setup ----------\ntraining: (157, 5, 1) (157, 1)\nvalidation: (49, 5, 1) (49, 1)\n(157, 5) --> (157, 1)\n(49, 5) --> (49, 1)\n\n\n\n\n\n\nView Code\nprint(\"---------- Data setup ----------\")\n# RECALL\nprint(\"training:\", Xt_prod.shape, Yt_prod.shape)\nprint(\"validation:\", Xv_prod.shape, Yv_prod.shape)\n\n# RESHAPE INTO A DATA FRAME\nXt1_prod = Xt_prod.reshape(Xt_prod.shape[0], Xt_prod.shape[1] * Xt_prod.shape[2])\nXv1_prod = Xv_prod.reshape(Xv_prod.shape[0], Xv_prod.shape[1] * Xv_prod.shape[2])\n\n# NEW SIZES\nprint(Xt1_prod.shape, \"-->\", Yt_prod.shape)\nprint(Xv1_prod.shape, \"-->\", Yv_prod.shape)\n\n\n---------- Data setup ----------\ntraining: (157, 5, 1) (157, 1)\nvalidation: (49, 5, 1) (49, 1)\n(157, 5) --> (157, 1)\n(49, 5) --> (49, 1)\n\n\n\n\n\n\nView Code\nprint(\"---------- Data setup ----------\")\n# RECALL\nprint(\"training:\", Xt_imp.shape, Yt_imp.shape)\nprint(\"validation:\", Xv_imp.shape, Yv_imp.shape)\n\n# RESHAPE INTO A DATA FRAME\nXt1_imp = Xt_imp.reshape(Xt_imp.shape[0], Xt_imp.shape[1] * Xt_imp.shape[2])\nXv1_imp = Xv_imp.reshape(Xv_imp.shape[0], Xv_imp.shape[1] * Xv_imp.shape[2])\n\n# NEW SIZES\nprint(Xt1_imp.shape, \"-->\", Yt_imp.shape)\nprint(Xv1_imp.shape, \"-->\", Yv_imp.shape)\n\n\n---------- Data setup ----------\ntraining: (229, 5, 1) (229, 1)\nvalidation: (73, 5, 1) (73, 1)\n(229, 5) --> (229, 1)\n(73, 5) --> (73, 1)\n\n\n\n\n\n\nView Code\nprint(\"---------- Data setup ----------\")\n# RECALL\nprint(\"training:\", Xt_exp.shape, Yt_exp.shape)\nprint(\"validation:\", Xv_exp.shape, Yv_exp.shape)\n\n# RESHAPE INTO A DATA FRAME\nXt1_exp = Xt_exp.reshape(Xt_exp.shape[0], Xt_exp.shape[1] * Xt_exp.shape[2])\nXv1_exp = Xv_exp.reshape(Xv_exp.shape[0], Xv_exp.shape[1] * Xv_exp.shape[2])\n\n# NEW SIZES\nprint(Xt1_exp.shape, \"-->\", Yt_exp.shape)\nprint(Xv1_exp.shape, \"-->\", Yv_exp.shape)\n\n\n---------- Data setup ----------\ntraining: (229, 5, 1) (229, 1)\nvalidation: (73, 5, 1) (73, 1)\n(229, 5) --> (229, 1)\n(73, 5) --> (73, 1)\n\n\n\n\n\n\nView Code\nprint(\"---------- Data setup ----------\")\n# RECALL\nprint(\"training:\", Xt_stk.shape, Yt_stk.shape)\nprint(\"validation:\", Xv_stk.shape, Yv_stk.shape)\n\n# RESHAPE INTO A DATA FRAME\nXt1_stk = Xt_stk.reshape(Xt_stk.shape[0], Xt_stk.shape[1] * Xt_stk.shape[2])\nXv1_stk = Xv_stk.reshape(Xv_stk.shape[0], Xv_stk.shape[1] * Xv_stk.shape[2])\n\n# NEW SIZES\nprint(Xt1_stk.shape, \"-->\", Yt_stk.shape)\nprint(Xv1_stk.shape, \"-->\", Yv_stk.shape)\n\n\n---------- Data setup ----------\ntraining: (3377, 5, 1) (3377, 1)\nvalidation: (1123, 5, 1) (1123, 1)\n(3377, 5) --> (3377, 1)\n(1123, 5) --> (1123, 1)\n\n\n\n\n\n\nView Code\nprint(\"---------- Data setup ----------\")\n# RECALL\nprint(\"training:\", Xt_emissions.shape, Yt_emissions.shape)\nprint(\"validation:\", Xv_emissions.shape, Yv_emissions.shape)\n\n# RESHAPE INTO A DATA FRAME\nXt1_emissions = Xt_emissions.reshape(Xt_emissions.shape[0], Xt_emissions.shape[1] * Xt_emissions.shape[2])\nXv1_emissions = Xv_emissions.reshape(Xv_emissions.shape[0], Xv_emissions.shape[1] * Xv_emissions.shape[2])\n\n# NEW SIZES\nprint(Xt1_emissions.shape, \"-->\", Yt_emissions.shape)\nprint(Xv1_emissions.shape, \"-->\", Yv_emissions.shape)\n\n\n---------- Data setup ----------\ntraining: (227, 5, 1) (227, 1)\nvalidation: (73, 5, 1) (73, 1)\n(227, 5) --> (227, 1)\n(73, 5) --> (73, 1)"
  },
  {
    "objectID": "ts_dl.html#rnn",
    "href": "ts_dl.html#rnn",
    "title": "Deep Learning for TS",
    "section": "RNN",
    "text": "RNN\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt.shape, \"-->\", Yt.shape)\nprint(Xv.shape, \"-->\", Yv.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\n                     )\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt,\n    Yt,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv, Yv),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn (SimpleRNN)      (None, 32)                1088      \n                                                                 \n dense (Dense)               (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 10ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.08564239257871006\n MAE: 0.2153671237872524\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.15087434726218102\n MAE: 0.28913158219267315\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN',\n               mean_squared_error(Yt, Ytp),\n               mean_absolute_error(Yt, Ytp),\n              training_RMSE,\n               mean_squared_error(Yv, Yvp),\n               mean_absolute_error(Yv, Yvp),\n               validation_RMSE]]\nprint(tabulate(error_table, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN     │   0.0856424 │    0.215367 │     0.292647 │  0.150874 │  0.289132 │   0.388425 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_consump.shape, \"-->\", Yt_consump.shape)\nprint(Xv_consump.shape, \"-->\", Yv_consump.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_consump.shape[1],Xt_consump.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_consump)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\n                     )\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_consump,\n    Yt_consump,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_consump, Yv_consump),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_consump = model.predict(Xt_consump)\nYvp_consump = model.predict(Xv_consump)\n\n# REPORT\nregression_report(Yt_consump, Ytp_consump, Yv_consump, Yvp_consump)\n\n# compute RMSE using numpy\ntraining_RMSE_consump = np.sqrt(mean_squared_error(Yt_consump, Ytp_consump))\nvalidation_RMSE_consump = np.sqrt(mean_squared_error(Yv_consump, Yvp_consump))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_1 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_1 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 6ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.12364105531700466\n MAE: 0.27592941675879085\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.4220541753851497\n MAE: 0.5226954378234901\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_consump = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN',\n               mean_squared_error(Yt_consump, Ytp_consump),\n               mean_absolute_error(Yt_consump, Ytp_consump),\n              training_RMSE_consump,\n               mean_squared_error(Yv_consump, Yvp_consump),\n               mean_absolute_error(Yv_consump, Yvp_consump),\n               validation_RMSE_consump]]\nprint(tabulate(error_table_consump, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN     │    0.123641 │    0.275929 │     0.351626 │  0.422054 │  0.522695 │   0.649657 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_prod.shape, \"-->\", Yt_prod.shape)\nprint(Xv_prod.shape, \"-->\", Yv_prod.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_prod.shape[1],Xt_prod.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_prod)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\n                     )\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_prod,\n    Yt_prod,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_prod, Yv_prod),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_prod = model.predict(Xt_prod)\nYvp_prod = model.predict(Xv_prod)\n\n# REPORT\nregression_report(Yt_prod, Ytp_prod, Yv_prod, Yvp_prod)\n\n# compute RMSE using numpy\ntraining_RMSE_prod = np.sqrt(mean_squared_error(Yt_prod, Ytp_prod))\nvalidation_RMSE_prod = np.sqrt(mean_squared_error(Yv_prod, Yvp_prod))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_2 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_2 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.022704780666281316\n MAE: 0.11801223130892244\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.09145595200163269\n MAE: 0.2620996457905679\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_prod = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN',\n               mean_squared_error(Yt_prod, Ytp_prod),\n               mean_absolute_error(Yt_prod, Ytp_prod),\n              training_RMSE_prod,\n               mean_squared_error(Yv_prod, Yvp_prod),\n               mean_absolute_error(Yv_prod, Yvp_prod),\n               validation_RMSE_prod]]\nprint(tabulate(error_table_prod, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN     │   0.0227048 │    0.118012 │     0.150681 │  0.091456 │    0.2621 │   0.302417 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_imp.shape, \"-->\", Yt_imp.shape)\nprint(Xv_imp.shape, \"-->\", Yv_imp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_imp.shape[1],Xt_imp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_imp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\n                     )\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_imp,\n    Yt_imp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_imp, Yv_imp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_imp = model.predict(Xt_imp)\nYvp_imp = model.predict(Xv_imp)\n\n# REPORT\nregression_report(Yt_imp, Ytp_imp, Yv_imp, Yvp_imp)\n\n# compute RMSE using numpy\ntraining_RMSE_imp = np.sqrt(mean_squared_error(Yt_imp, Ytp_imp))\nvalidation_RMSE_imp = np.sqrt(mean_squared_error(Yv_imp, Yvp_imp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_3 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_3 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 1s 6ms/step\n3/3 [==============================] - 0s 8ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.1560476381351537\n MAE: 0.3165801404532817\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.18567062606530266\n MAE: 0.35992918817697195\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_imp = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN',\n               mean_squared_error(Yt_imp, Ytp_imp),\n               mean_absolute_error(Yt_imp, Ytp_imp),\n               training_RMSE_imp,\n               mean_squared_error(Yv_imp, Yvp_imp),\n               mean_absolute_error(Yv_imp, Yvp_imp),\n               validation_RMSE_imp]]\nprint(tabulate(error_table_imp, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN     │    0.156048 │     0.31658 │     0.395029 │  0.185671 │  0.359929 │   0.430895 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_exp.shape, \"-->\", Yt_exp.shape)\nprint(Xv_exp.shape, \"-->\", Yv_exp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_exp.shape[1],Xt_exp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_exp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\n                     )\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_exp,\n    Yt_exp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_exp, Yv_exp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_exp = model.predict(Xt_exp)\nYvp_exp = model.predict(Xv_exp)\n\n# REPORT\nregression_report(Yt_exp, Ytp_exp, Yv_exp, Yvp_exp)\n\n# compute RMSE using numpy\ntraining_RMSE_exp = np.sqrt(mean_squared_error(Yt_exp, Ytp_exp))\nvalidation_RMSE_exp = np.sqrt(mean_squared_error(Yv_exp, Yvp_exp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_4 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_4 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 3ms/step\n3/3 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.005350366835801499\n MAE: 0.055133177214101235\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.7585070258742818\n MAE: 0.6409075051411177\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_exp = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN',\n               mean_squared_error(Yt_exp, Ytp_exp),\n               mean_absolute_error(Yt_exp, Ytp_exp),\n               training_RMSE_exp,\n               mean_squared_error(Yv_exp, Yvp_exp),\n               mean_absolute_error(Yv_exp, Yvp_exp),\n               validation_RMSE_exp]]\nprint(tabulate(error_table_exp, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN     │  0.00535037 │   0.0551332 │    0.0731462 │  0.758507 │  0.640908 │   0.870923 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_stk.shape, \"-->\", Yt_stk.shape)\nprint(Xv_stk.shape, \"-->\", Yv_stk.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_stk.shape[1],Xt_stk.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_stk)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\n                     )\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_stk,\n    Yt_stk,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_stk, Yv_stk),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_stk = model.predict(Xt_stk)\nYvp_stk = model.predict(Xv_stk)\n\n# REPORT\nregression_report(Yt_stk, Ytp_stk, Yv_stk, Yvp_stk)\n\n# compute RMSE using numpy\ntraining_RMSE_stk = np.sqrt(mean_squared_error(Yt_stk, Ytp_stk))\nvalidation_RMSE_stk = np.sqrt(mean_squared_error(Yv_stk, Yvp_stk))\n\n\n(3377, 5, 1) --> (3377, 1)\n(1123, 5, 1) --> (1123, 1)\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_5 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_5 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n106/106 [==============================] - 0s 2ms/step\n36/36 [==============================] - 0s 2ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.003859248605304056\n MAE: 0.04937039958418123\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.2414823291143868\n MAE: 0.2529345975930672\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_stk = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN',\n               mean_squared_error(Yt_stk, Ytp_stk),\n               mean_absolute_error(Yt_stk, Ytp_stk),\n               training_RMSE_stk,\n               mean_squared_error(Yv_stk, Yvp_stk),\n               mean_absolute_error(Yv_stk, Yvp_stk),\n               validation_RMSE_stk]]\nprint(tabulate(error_table_stk, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN     │  0.00385925 │   0.0493704 │    0.0621229 │  0.241482 │  0.252935 │   0.491409 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_emissions.shape, \"-->\", Yt_emissions.shape)\nprint(Xv_emissions.shape, \"-->\", Yv_emissions.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_emissions.shape[1],Xt_emissions.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_emissions)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\n                     )\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_emissions,\n    Yt_emissions,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_emissions, Yv_emissions),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_emissions = model.predict(Xt_emissions)\nYvp_emissions = model.predict(Xv_emissions)\n\n# REPORT\nregression_report(Yt_emissions, Ytp_emissions, Yv_emissions, Yvp_emissions)\n\n# compute RMSE using numpy\ntraining_RMSE_emissions = np.sqrt(mean_squared_error(Yt_emissions, Ytp_emissions))\nvalidation_RMSE_emissions = np.sqrt(mean_squared_error(Yv_emissions, Yvp_emissions))\n\n\n(227, 5, 1) --> (227, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_6 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_6 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 4ms/step\n3/3 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.3033295428161593\n MAE: 0.3487574632804835\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.5143298833546888\n MAE: 0.9687692360570798\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_emissions = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN',\n               mean_squared_error(Yt_emissions, Ytp_emissions),\n               mean_absolute_error(Yt_emissions, Ytp_emissions),\n               training_RMSE_emissions,\n               mean_squared_error(Yv_emissions, Yvp_emissions),\n               mean_absolute_error(Yv_emissions, Yvp_emissions),\n               validation_RMSE_emissions]]\nprint(tabulate(error_table_emissions, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN     │     0.30333 │    0.348757 │     0.550754 │   1.51433 │  0.968769 │    1.23058 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛"
  },
  {
    "objectID": "ts_dl.html#gru",
    "href": "ts_dl.html#gru",
    "title": "Deep Learning for TS",
    "section": "GRU",
    "text": "GRU\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt.shape, \"-->\", Yt.shape)\nprint(Xv.shape, \"-->\", Yv.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt,\n    Yt,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv, Yv),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_14\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru (GRU)                   (None, 32)                3360      \n                                                                 \n dense_14 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 1s 4ms/step\n2/2 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.09307459561413757\n MAE: 0.21239367581368906\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.1832772696117805\n MAE: 0.3187538903683385\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_gru = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU',\n               mean_squared_error(Yt, Ytp),\n               mean_absolute_error(Yt, Ytp),\n              training_RMSE,\n               mean_squared_error(Yv, Yvp),\n               mean_absolute_error(Yv, Yvp),\n               validation_RMSE]]\nprint(tabulate(error_table_gru, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU     │   0.0930746 │    0.212394 │     0.305081 │  0.183277 │  0.318754 │   0.428109 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_consump.shape, \"-->\", Yt_consump.shape)\nprint(Xv_consump.shape, \"-->\", Yv_consump.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_consump.shape[1],Xt_consump.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_consump)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_consump,\n    Yt_consump,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_consump, Yv_consump),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_consump = model.predict(Xt_consump)\nYvp_consump = model.predict(Xv_consump)\n\n# REPORT\nregression_report(Yt_consump, Ytp_consump, Yv_consump, Yvp_consump)\n\n# compute RMSE using numpy\ntraining_RMSE_consump = np.sqrt(mean_squared_error(Yt_consump, Ytp_consump))\nvalidation_RMSE_consump = np.sqrt(mean_squared_error(Yv_consump, Yvp_consump))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_15\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_1 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_15 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.23355664103124701\n MAE: 0.3685060510260475\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.5929653026508562\n MAE: 0.6548865234409081\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_consump_gru = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU',\n               mean_squared_error(Yt_consump, Ytp_consump),\n               mean_absolute_error(Yt_consump, Ytp_consump),\n              training_RMSE_consump,\n               mean_squared_error(Yv_consump, Yvp_consump),\n               mean_absolute_error(Yv_consump, Yvp_consump),\n               validation_RMSE_consump]]\nprint(tabulate(error_table_consump_gru, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU     │    0.233557 │    0.368506 │     0.483277 │  0.592965 │  0.654887 │   0.770042 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_prod.shape, \"-->\", Yt_prod.shape)\nprint(Xv_prod.shape, \"-->\", Yv_prod.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_prod.shape[1],Xt_prod.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_prod)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_prod,\n    Yt_prod,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_prod, Yv_prod),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_prod = model.predict(Xt_prod)\nYvp_prod = model.predict(Xv_prod)\n\n# REPORT\nregression_report(Yt_prod, Ytp_prod, Yv_prod, Yvp_prod)\n\n# compute RMSE using numpy\ntraining_RMSE_prod = np.sqrt(mean_squared_error(Yt_prod, Ytp_prod))\nvalidation_RMSE_prod = np.sqrt(mean_squared_error(Yv_prod, Yvp_prod))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_16\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_2 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_16 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.02292736260911077\n MAE: 0.11412029267993612\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.1303638047994717\n MAE: 0.32803943068324254\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_prod_gru = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU',\n               mean_squared_error(Yt_prod, Ytp_prod),\n               mean_absolute_error(Yt_prod, Ytp_prod),\n              training_RMSE_prod,\n               mean_squared_error(Yv_prod, Yvp_prod),\n               mean_absolute_error(Yv_prod, Yvp_prod),\n               validation_RMSE_prod]]\nprint(tabulate(error_table_prod_gru, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU     │   0.0229274 │     0.11412 │     0.151418 │  0.130364 │  0.328039 │   0.361059 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_imp.shape, \"-->\", Yt_imp.shape)\nprint(Xv_imp.shape, \"-->\", Yv_imp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_imp.shape[1],Xt_imp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_imp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_imp,\n    Yt_imp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_imp, Yv_imp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_imp = model.predict(Xt_imp)\nYvp_imp = model.predict(Xv_imp)\n\n# REPORT\nregression_report(Yt_imp, Ytp_imp, Yv_imp, Yvp_imp)\n\n# compute RMSE using numpy\ntraining_RMSE_imp = np.sqrt(mean_squared_error(Yt_imp, Ytp_imp))\nvalidation_RMSE_imp = np.sqrt(mean_squared_error(Yv_imp, Yvp_imp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_17\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_3 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_17 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 3ms/step\n3/3 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.19197940418155066\n MAE: 0.3450106572232673\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.2222956142443242\n MAE: 0.37296668297977176\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_imp_gru = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU',\n               mean_squared_error(Yt_imp, Ytp_imp),\n               mean_absolute_error(Yt_imp, Ytp_imp),\n               training_RMSE_imp,\n               mean_squared_error(Yv_imp, Yvp_imp),\n               mean_absolute_error(Yv_imp, Yvp_imp),\n               validation_RMSE_imp]]\nprint(tabulate(error_table_imp_gru, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU     │    0.191979 │    0.345011 │     0.438155 │  0.222296 │  0.372967 │   0.471482 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_exp.shape, \"-->\", Yt_exp.shape)\nprint(Xv_exp.shape, \"-->\", Yv_exp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_exp.shape[1],Xt_exp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_exp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_exp,\n    Yt_exp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_exp, Yv_exp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_exp = model.predict(Xt_exp)\nYvp_exp = model.predict(Xv_exp)\n\n# REPORT\nregression_report(Yt_exp, Ytp_exp, Yv_exp, Yvp_exp)\n\n# compute RMSE using numpy\ntraining_RMSE_exp = np.sqrt(mean_squared_error(Yt_exp, Ytp_exp))\nvalidation_RMSE_exp = np.sqrt(mean_squared_error(Yv_exp, Yvp_exp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_18\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_4 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_18 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 3ms/step\n3/3 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.005926264873573486\n MAE: 0.05846742314216646\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.8894577127439768\n MAE: 0.7667608707053157\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_exp_gru = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU',\n               mean_squared_error(Yt_exp, Ytp_exp),\n               mean_absolute_error(Yt_exp, Ytp_exp),\n               training_RMSE_exp,\n               mean_squared_error(Yv_exp, Yvp_exp),\n               mean_absolute_error(Yv_exp, Yvp_exp),\n               validation_RMSE_exp]]\nprint(tabulate(error_table_exp_gru, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU     │  0.00592626 │   0.0584674 │    0.0769822 │  0.889458 │  0.766761 │   0.943111 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_stk.shape, \"-->\", Yt_stk.shape)\nprint(Xv_stk.shape, \"-->\", Yv_stk.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_stk.shape[1],Xt_stk.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_stk)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_stk,\n    Yt_stk,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_stk, Yv_stk),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_stk = model.predict(Xt_stk)\nYvp_stk = model.predict(Xv_stk)\n\n# REPORT\nregression_report(Yt_stk, Ytp_stk, Yv_stk, Yvp_stk)\n\n# compute RMSE using numpy\ntraining_RMSE_stk = np.sqrt(mean_squared_error(Yt_stk, Ytp_stk))\nvalidation_RMSE_stk = np.sqrt(mean_squared_error(Yv_stk, Yvp_stk))\n\n\n(3377, 5, 1) --> (3377, 1)\n(1123, 5, 1) --> (1123, 1)\nModel: \"sequential_19\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_5 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_19 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n106/106 [==============================] - 1s 2ms/step\n36/36 [==============================] - 0s 3ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.0024811101772368917\n MAE: 0.03790346794311486\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.22407084458733548\n MAE: 0.2609358026053276\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_stk_gru = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU',\n               mean_squared_error(Yt_stk, Ytp_stk),\n               mean_absolute_error(Yt_stk, Ytp_stk),\n               training_RMSE_stk,\n               mean_squared_error(Yv_stk, Yvp_stk),\n               mean_absolute_error(Yv_stk, Yvp_stk),\n               validation_RMSE_stk]]\nprint(tabulate(error_table_stk_gru, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU     │  0.00248111 │   0.0379035 │    0.0498107 │  0.224071 │  0.260936 │   0.473361 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_emissions.shape, \"-->\", Yt_emissions.shape)\nprint(Xv_emissions.shape, \"-->\", Yv_emissions.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_emissions.shape[1],Xt_emissions.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_emissions)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_emissions,\n    Yt_emissions,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_emissions, Yv_emissions),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_emissions = model.predict(Xt_emissions)\nYvp_emissions = model.predict(Xv_emissions)\n\n# REPORT\nregression_report(Yt_emissions, Ytp_emissions, Yv_emissions, Yvp_emissions)\n\n# compute RMSE using numpy\ntraining_RMSE_emissions = np.sqrt(mean_squared_error(Yt_emissions, Ytp_emissions))\nvalidation_RMSE_emissions = np.sqrt(mean_squared_error(Yv_emissions, Yvp_emissions))\n\n\n(227, 5, 1) --> (227, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_20\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_6 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_20 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 1s 4ms/step\n3/3 [==============================] - 0s 8ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.826495245899882\n MAE: 0.4748150256824064\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.3218618767871289\n MAE: 0.4843349026939558\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_emissions_gru = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU',\n               mean_squared_error(Yt_emissions, Ytp_emissions),\n               mean_absolute_error(Yt_emissions, Ytp_emissions),\n               training_RMSE_emissions,\n               mean_squared_error(Yv_emissions, Yvp_emissions),\n               mean_absolute_error(Yv_emissions, Yvp_emissions),\n               validation_RMSE_emissions]]\nprint(tabulate(error_table_emissions_gru, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU     │    0.826495 │    0.474815 │     0.909118 │   1.32186 │  0.484335 │    1.14972 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛"
  },
  {
    "objectID": "ts_dl.html#gru-with-l2-regularization",
    "href": "ts_dl.html#gru-with-l2-regularization",
    "title": "Deep Learning for TS",
    "section": "GRU with L2 Regularization",
    "text": "GRU with L2 Regularization\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt.shape, \"-->\", Yt.shape)\nprint(Xv.shape, \"-->\", Yv.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt,\n    Yt,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv, Yv),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_21\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_7 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_21 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 6ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.09432739660797214\n MAE: 0.2136415814493819\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.1839333523380131\n MAE: 0.3036520450568859\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_gru_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU(L2 Reg)',\n               mean_squared_error(Yt, Ytp),\n               mean_absolute_error(Yt, Ytp),\n              training_RMSE,\n               mean_squared_error(Yv, Yvp),\n               mean_absolute_error(Yv, Yvp),\n               validation_RMSE]]\nprint(tabulate(error_table_gru_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU(L2 Reg) │   0.0943274 │    0.213642 │     0.307128 │  0.183933 │  0.303652 │   0.428875 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_consump.shape, \"-->\", Yt_consump.shape)\nprint(Xv_consump.shape, \"-->\", Yv_consump.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_consump.shape[1],Xt_consump.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_consump)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_consump,\n    Yt_consump,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_consump, Yv_consump),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_consump = model.predict(Xt_consump)\nYvp_consump = model.predict(Xv_consump)\n\n# REPORT\nregression_report(Yt_consump, Ytp_consump, Yv_consump, Yvp_consump)\n\n# compute RMSE using numpy\ntraining_RMSE_consump = np.sqrt(mean_squared_error(Yt_consump, Ytp_consump))\nvalidation_RMSE_consump = np.sqrt(mean_squared_error(Yv_consump, Yvp_consump))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_22\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_8 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_22 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.2995691196669474\n MAE: 0.4341264415337982\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.2107876513155427\n MAE: 0.8532101820645902\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_consump_gru_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU(L2 Reg)',\n               mean_squared_error(Yt_consump, Ytp_consump),\n               mean_absolute_error(Yt_consump, Ytp_consump),\n              training_RMSE_consump,\n               mean_squared_error(Yv_consump, Yvp_consump),\n               mean_absolute_error(Yv_consump, Yvp_consump),\n               validation_RMSE_consump]]\nprint(tabulate(error_table_consump_gru_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU(L2 Reg) │    0.299569 │    0.434126 │     0.547329 │   1.21079 │   0.85321 │    1.10036 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_prod.shape, \"-->\", Yt_prod.shape)\nprint(Xv_prod.shape, \"-->\", Yv_prod.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_prod.shape[1],Xt_prod.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_prod)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_prod,\n    Yt_prod,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_prod, Yv_prod),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_prod = model.predict(Xt_prod)\nYvp_prod = model.predict(Xv_prod)\n\n# REPORT\nregression_report(Yt_prod, Ytp_prod, Yv_prod, Yvp_prod)\n\n# compute RMSE using numpy\ntraining_RMSE_prod = np.sqrt(mean_squared_error(Yt_prod, Ytp_prod))\nvalidation_RMSE_prod = np.sqrt(mean_squared_error(Yv_prod, Yvp_prod))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_23\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_9 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_23 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 8ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.02248999120633133\n MAE: 0.1125636691296517\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.2939585365374952\n MAE: 0.49880377964020967\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_prod_gru_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU(L2 Reg)',\n               mean_squared_error(Yt_prod, Ytp_prod),\n               mean_absolute_error(Yt_prod, Ytp_prod),\n              training_RMSE_prod,\n               mean_squared_error(Yv_prod, Yvp_prod),\n               mean_absolute_error(Yv_prod, Yvp_prod),\n               validation_RMSE_prod]]\nprint(tabulate(error_table_prod_gru_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU(L2 Reg) │     0.02249 │    0.112564 │     0.149967 │  0.293959 │  0.498804 │   0.542179 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_imp.shape, \"-->\", Yt_imp.shape)\nprint(Xv_imp.shape, \"-->\", Yv_imp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_imp.shape[1],Xt_imp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_imp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_imp,\n    Yt_imp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_imp, Yv_imp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_imp = model.predict(Xt_imp)\nYvp_imp = model.predict(Xv_imp)\n\n# REPORT\nregression_report(Yt_imp, Ytp_imp, Yv_imp, Yvp_imp)\n\n# compute RMSE using numpy\ntraining_RMSE_imp = np.sqrt(mean_squared_error(Yt_imp, Ytp_imp))\nvalidation_RMSE_imp = np.sqrt(mean_squared_error(Yv_imp, Yvp_imp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_24\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_10 (GRU)                (None, 32)                3360      \n                                                                 \n dense_24 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 3ms/step\n3/3 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.18782569642597555\n MAE: 0.3343330888733994\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.22834866097912995\n MAE: 0.37341519140814167\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_imp_gru_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU(L2 Reg)',\n               mean_squared_error(Yt_imp, Ytp_imp),\n               mean_absolute_error(Yt_imp, Ytp_imp),\n               training_RMSE_imp,\n               mean_squared_error(Yv_imp, Yvp_imp),\n               mean_absolute_error(Yv_imp, Yvp_imp),\n               validation_RMSE_imp]]\nprint(tabulate(error_table_imp_gru_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU(L2 Reg) │    0.187826 │    0.334333 │     0.433389 │  0.228349 │  0.373415 │   0.477858 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_exp.shape, \"-->\", Yt_exp.shape)\nprint(Xv_exp.shape, \"-->\", Yv_exp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_exp.shape[1],Xt_exp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_exp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_exp,\n    Yt_exp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_exp, Yv_exp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_exp = model.predict(Xt_exp)\nYvp_exp = model.predict(Xv_exp)\n\n# REPORT\nregression_report(Yt_exp, Ytp_exp, Yv_exp, Yvp_exp)\n\n# compute RMSE using numpy\ntraining_RMSE_exp = np.sqrt(mean_squared_error(Yt_exp, Ytp_exp))\nvalidation_RMSE_exp = np.sqrt(mean_squared_error(Yv_exp, Yvp_exp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_25\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_11 (GRU)                (None, 32)                3360      \n                                                                 \n dense_25 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 4ms/step\n3/3 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.007668944289032949\n MAE: 0.06449698191446716\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 2.7026562417173494\n MAE: 1.4347720741135572\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_exp_gru_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU(L2 Reg)',\n               mean_squared_error(Yt_exp, Ytp_exp),\n               mean_absolute_error(Yt_exp, Ytp_exp),\n               training_RMSE_exp,\n               mean_squared_error(Yv_exp, Yvp_exp),\n               mean_absolute_error(Yv_exp, Yvp_exp),\n               validation_RMSE_exp]]\nprint(tabulate(error_table_exp_gru_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU(L2 Reg) │  0.00766894 │    0.064497 │    0.0875725 │   2.70266 │   1.43477 │    1.64398 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_stk.shape, \"-->\", Yt_stk.shape)\nprint(Xv_stk.shape, \"-->\", Yv_stk.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_stk.shape[1],Xt_stk.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_stk)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_stk,\n    Yt_stk,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_stk, Yv_stk),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_stk = model.predict(Xt_stk)\nYvp_stk = model.predict(Xv_stk)\n\n# REPORT\nregression_report(Yt_stk, Ytp_stk, Yv_stk, Yvp_stk)\n\n# compute RMSE using numpy\ntraining_RMSE_stk = np.sqrt(mean_squared_error(Yt_stk, Ytp_stk))\nvalidation_RMSE_stk = np.sqrt(mean_squared_error(Yv_stk, Yvp_stk))\n\n\n(3377, 5, 1) --> (3377, 1)\n(1123, 5, 1) --> (1123, 1)\nModel: \"sequential_26\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_12 (GRU)                (None, 32)                3360      \n                                                                 \n dense_26 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n106/106 [==============================] - 0s 2ms/step\n36/36 [==============================] - 0s 2ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.0027459609912201706\n MAE: 0.038920349685540014\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.044396008253210044\n MAE: 0.13976088675590678\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_stk_gru_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU(L2 Reg)',\n               mean_squared_error(Yt_stk, Ytp_stk),\n               mean_absolute_error(Yt_stk, Ytp_stk),\n               training_RMSE_stk,\n               mean_squared_error(Yv_stk, Yvp_stk),\n               mean_absolute_error(Yv_stk, Yvp_stk),\n               validation_RMSE_stk]]\nprint(tabulate(error_table_stk_gru_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU(L2 Reg) │  0.00274596 │   0.0389203 │    0.0524019 │  0.044396 │  0.139761 │   0.210704 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_emissions.shape, \"-->\", Yt_emissions.shape)\nprint(Xv_emissions.shape, \"-->\", Yv_emissions.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_emissions.shape[1],Xt_emissions.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_emissions)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_emissions,\n    Yt_emissions,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_emissions, Yv_emissions),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_emissions = model.predict(Xt_emissions)\nYvp_emissions = model.predict(Xv_emissions)\n\n# REPORT\nregression_report(Yt_emissions, Ytp_emissions, Yv_emissions, Yvp_emissions)\n\n# compute RMSE using numpy\ntraining_RMSE_emissions = np.sqrt(mean_squared_error(Yt_emissions, Ytp_emissions))\nvalidation_RMSE_emissions = np.sqrt(mean_squared_error(Yv_emissions, Yvp_emissions))\n\n\n(227, 5, 1) --> (227, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_27\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_13 (GRU)                (None, 32)                3360      \n                                                                 \n dense_27 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 4ms/step\n3/3 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.7672829841706863\n MAE: 0.44492241023064333\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.2503476954706598\n MAE: 0.44037885242740576\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_emissions_gru_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU(L2 Reg)',\n               mean_squared_error(Yt_emissions, Ytp_emissions),\n               mean_absolute_error(Yt_emissions, Ytp_emissions),\n               training_RMSE_emissions,\n               mean_squared_error(Yv_emissions, Yvp_emissions),\n               mean_absolute_error(Yv_emissions, Yvp_emissions),\n               validation_RMSE_emissions]]\nprint(tabulate(error_table_emissions_gru_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU(L2 Reg) │    0.767283 │    0.444922 │     0.875947 │   1.25035 │  0.440379 │    1.11819 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛"
  },
  {
    "objectID": "ts_dl.html#lstm",
    "href": "ts_dl.html#lstm",
    "title": "Deep Learning for TS",
    "section": "LSTM",
    "text": "LSTM\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt.shape, \"-->\", Yt.shape)\nprint(Xv.shape, \"-->\", Yv.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt,\n    Yt,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv, Yv),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_28\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, 32)                4352      \n                                                                 \n dense_28 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 4ms/step\n2/2 [==============================] - 0s 6ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.09286728965267183\n MAE: 0.2154747499972604\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.20651359469214828\n MAE: 0.33671482100973527\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_lstm = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM',\n               mean_squared_error(Yt, Ytp),\n               mean_absolute_error(Yt, Ytp),\n              training_RMSE,\n               mean_squared_error(Yv, Yvp),\n               mean_absolute_error(Yv, Yvp),\n               validation_RMSE]]\nprint(tabulate(error_table_lstm, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM    │   0.0928673 │    0.215475 │     0.304741 │  0.206514 │  0.336715 │   0.454438 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_consump.shape, \"-->\", Yt_consump.shape)\nprint(Xv_consump.shape, \"-->\", Yv_consump.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_consump.shape[1],Xt_consump.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_consump)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_consump,\n    Yt_consump,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_consump, Yv_consump),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_consump = model.predict(Xt_consump)\nYvp_consump = model.predict(Xv_consump)\n\n# REPORT\nregression_report(Yt_consump, Ytp_consump, Yv_consump, Yvp_consump)\n\n# compute RMSE using numpy\ntraining_RMSE_consump = np.sqrt(mean_squared_error(Yt_consump, Ytp_consump))\nvalidation_RMSE_consump = np.sqrt(mean_squared_error(Yv_consump, Yvp_consump))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_29\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_1 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_29 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 6ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.2970955864641625\n MAE: 0.437726763197135\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.0437340892837863\n MAE: 0.8793948418476977\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_consump_lstm = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM',\n               mean_squared_error(Yt_consump, Ytp_consump),\n               mean_absolute_error(Yt_consump, Ytp_consump),\n              training_RMSE_consump,\n               mean_squared_error(Yv_consump, Yvp_consump),\n               mean_absolute_error(Yv_consump, Yvp_consump),\n               validation_RMSE_consump]]\nprint(tabulate(error_table_consump_lstm, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM    │    0.297096 │    0.437727 │     0.545065 │   1.04373 │  0.879395 │    1.02163 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_prod.shape, \"-->\", Yt_prod.shape)\nprint(Xv_prod.shape, \"-->\", Yv_prod.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_prod.shape[1],Xt_prod.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_prod)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_prod,\n    Yt_prod,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_prod, Yv_prod),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_prod = model.predict(Xt_prod)\nYvp_prod = model.predict(Xv_prod)\n\n# REPORT\nregression_report(Yt_prod, Ytp_prod, Yv_prod, Yvp_prod)\n\n# compute RMSE using numpy\ntraining_RMSE_prod = np.sqrt(mean_squared_error(Yt_prod, Ytp_prod))\nvalidation_RMSE_prod = np.sqrt(mean_squared_error(Yv_prod, Yvp_prod))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_30\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_2 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_30 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.023363330218405497\n MAE: 0.11192040423436898\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.2358212359149498\n MAE: 0.4440431106812732\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_prod_lstm = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM',\n               mean_squared_error(Yt_prod, Ytp_prod),\n               mean_absolute_error(Yt_prod, Ytp_prod),\n              training_RMSE_prod,\n               mean_squared_error(Yv_prod, Yvp_prod),\n               mean_absolute_error(Yv_prod, Yvp_prod),\n               validation_RMSE_prod]]\nprint(tabulate(error_table_prod_lstm, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM    │   0.0233633 │     0.11192 │     0.152851 │  0.235821 │  0.444043 │   0.485614 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_imp.shape, \"-->\", Yt_imp.shape)\nprint(Xv_imp.shape, \"-->\", Yv_imp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_imp.shape[1],Xt_imp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_imp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_imp,\n    Yt_imp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_imp, Yv_imp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_imp = model.predict(Xt_imp)\nYvp_imp = model.predict(Xv_imp)\n\n# REPORT\nregression_report(Yt_imp, Ytp_imp, Yv_imp, Yvp_imp)\n\n# compute RMSE using numpy\ntraining_RMSE_imp = np.sqrt(mean_squared_error(Yt_imp, Ytp_imp))\nvalidation_RMSE_imp = np.sqrt(mean_squared_error(Yv_imp, Yvp_imp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_31\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_3 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_31 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 3ms/step\n3/3 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.19351637543097616\n MAE: 0.34250260930648363\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.21958086049481404\n MAE: 0.3749440361272188\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_imp_lstm = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM',\n               mean_squared_error(Yt_imp, Ytp_imp),\n               mean_absolute_error(Yt_imp, Ytp_imp),\n               training_RMSE_imp,\n               mean_squared_error(Yv_imp, Yvp_imp),\n               mean_absolute_error(Yv_imp, Yvp_imp),\n               validation_RMSE_imp]]\nprint(tabulate(error_table_imp_lstm, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM    │    0.193516 │    0.342503 │     0.439905 │  0.219581 │  0.374944 │   0.468595 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_imp.shape, \"-->\", Yt_imp.shape)\nprint(Xv_imp.shape, \"-->\", Yv_imp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_imp.shape[1],Xt_imp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_imp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_imp,\n    Yt_imp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_imp, Yv_imp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_imp = model.predict(Xt_imp)\nYvp_imp = model.predict(Xv_imp)\n\n# REPORT\nregression_report(Yt_imp, Ytp_imp, Yv_imp, Yvp_imp)\n\n# compute RMSE using numpy\ntraining_RMSE_imp = np.sqrt(mean_squared_error(Yt_imp, Ytp_imp))\nvalidation_RMSE_imp = np.sqrt(mean_squared_error(Yv_imp, Yvp_imp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_32\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_4 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_32 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 1s 4ms/step\n3/3 [==============================] - 0s 9ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.19189830266937993\n MAE: 0.34543021324045875\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.220367624040603\n MAE: 0.3758999369201629\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_exp_lstm = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM',\n               mean_squared_error(Yt_exp, Ytp_exp),\n               mean_absolute_error(Yt_exp, Ytp_exp),\n               training_RMSE_exp,\n               mean_squared_error(Yv_exp, Yvp_exp),\n               mean_absolute_error(Yv_exp, Yvp_exp),\n               validation_RMSE_exp]]\nprint(tabulate(error_table_exp_lstm, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM    │  0.00766894 │    0.064497 │    0.0875725 │   2.70266 │   1.43477 │    1.64398 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_stk.shape, \"-->\", Yt_stk.shape)\nprint(Xv_stk.shape, \"-->\", Yv_stk.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_stk.shape[1],Xt_stk.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_stk)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_stk,\n    Yt_stk,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_stk, Yv_stk),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_stk = model.predict(Xt_stk)\nYvp_stk = model.predict(Xv_stk)\n\n# REPORT\nregression_report(Yt_stk, Ytp_stk, Yv_stk, Yvp_stk)\n\n# compute RMSE using numpy\ntraining_RMSE_stk = np.sqrt(mean_squared_error(Yt_stk, Ytp_stk))\nvalidation_RMSE_stk = np.sqrt(mean_squared_error(Yv_stk, Yvp_stk))\n\n\n(3377, 5, 1) --> (3377, 1)\n(1123, 5, 1) --> (1123, 1)\nModel: \"sequential_33\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_5 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_33 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n106/106 [==============================] - 1s 4ms/step\n36/36 [==============================] - 0s 3ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.002367399844141305\n MAE: 0.03738482228573484\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.3444546925410365\n MAE: 0.3127807659043008\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_stk_lstm = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM',\n               mean_squared_error(Yt_stk, Ytp_stk),\n               mean_absolute_error(Yt_stk, Ytp_stk),\n               training_RMSE_stk,\n               mean_squared_error(Yv_stk, Yvp_stk),\n               mean_absolute_error(Yv_stk, Yvp_stk),\n               validation_RMSE_stk]]\nprint(tabulate(error_table_stk_lstm, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM    │   0.0023674 │   0.0373848 │    0.0486559 │  0.344455 │  0.312781 │   0.586903 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_emissions.shape, \"-->\", Yt_emissions.shape)\nprint(Xv_emissions.shape, \"-->\", Yv_emissions.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_emissions.shape[1],Xt_emissions.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_emissions)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_emissions,\n    Yt_emissions,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_emissions, Yv_emissions),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_emissions = model.predict(Xt_emissions)\nYvp_emissions = model.predict(Xv_emissions)\n\n# REPORT\nregression_report(Yt_emissions, Ytp_emissions, Yv_emissions, Yvp_emissions)\n\n# compute RMSE using numpy\ntraining_RMSE_emissions = np.sqrt(mean_squared_error(Yt_emissions, Ytp_emissions))\nvalidation_RMSE_emissions = np.sqrt(mean_squared_error(Yv_emissions, Yvp_emissions))\n\n\n(227, 5, 1) --> (227, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_34\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_6 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_34 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 1s 4ms/step\n3/3 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.8239359829777397\n MAE: 0.47986576628663985\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.3089513655204494\n MAE: 0.49688676717475977\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_emissions_lstm = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM',\n               mean_squared_error(Yt_emissions, Ytp_emissions),\n               mean_absolute_error(Yt_emissions, Ytp_emissions),\n               training_RMSE_emissions,\n               mean_squared_error(Yv_emissions, Yvp_emissions),\n               mean_absolute_error(Yv_emissions, Yvp_emissions),\n               validation_RMSE_emissions]]\nprint(tabulate(error_table_emissions_lstm, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM    │    0.823936 │    0.479866 │     0.907709 │   1.30895 │  0.496887 │    1.14409 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛"
  },
  {
    "objectID": "ts_dl.html#lstm-with-l2-regularization",
    "href": "ts_dl.html#lstm-with-l2-regularization",
    "title": "Deep Learning for TS",
    "section": "LSTM with L2 Regularization",
    "text": "LSTM with L2 Regularization\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt.shape, \"-->\", Yt.shape)\nprint(Xv.shape, \"-->\", Yv.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt,\n    Yt,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv, Yv),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_35\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_7 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_35 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 4ms/step\n2/2 [==============================] - 0s 10ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.09697849098699898\n MAE: 0.2159713205718824\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.18647708992320702\n MAE: 0.3025485515171489\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_lstm_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM(L2 Reg)',\n               mean_squared_error(Yt, Ytp),\n               mean_absolute_error(Yt, Ytp),\n               training_RMSE,\n               mean_squared_error(Yv, Yvp),\n               mean_absolute_error(Yv, Yvp),\n               validation_RMSE]]\nprint(tabulate(error_table_lstm_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM(L2 Reg) │   0.0969785 │    0.215971 │     0.311414 │  0.186477 │  0.302549 │    0.43183 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_consump.shape, \"-->\", Yt_consump.shape)\nprint(Xv_consump.shape, \"-->\", Yv_consump.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_consump.shape[1],Xt_consump.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_consump)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_consump,\n    Yt_consump,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_consump, Yv_consump),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_consump = model.predict(Xt_consump)\nYvp_consump = model.predict(Xv_consump)\n\n# REPORT\nregression_report(Yt_consump, Ytp_consump, Yv_consump, Yvp_consump)\n\n# compute RMSE using numpy\ntraining_RMSE_consump = np.sqrt(mean_squared_error(Yt_consump, Ytp_consump))\nvalidation_RMSE_consump = np.sqrt(mean_squared_error(Yv_consump, Yvp_consump))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_36\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_8 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_36 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 4ms/step\n2/2 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.2747615144003193\n MAE: 0.4093043864536077\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.2416227791187402\n MAE: 0.896378661044941\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_consump_lstm_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM(L2 Reg)',\n               mean_squared_error(Yt_consump, Ytp_consump),\n               mean_absolute_error(Yt_consump, Ytp_consump),\n              training_RMSE_consump,\n               mean_squared_error(Yv_consump, Yvp_consump),\n               mean_absolute_error(Yv_consump, Yvp_consump),\n               validation_RMSE_consump]]\nprint(tabulate(error_table_consump_lstm_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM(L2 Reg) │    0.274762 │    0.409304 │     0.524177 │   1.24162 │  0.896379 │    1.11428 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_prod.shape, \"-->\", Yt_prod.shape)\nprint(Xv_prod.shape, \"-->\", Yv_prod.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_prod.shape[1],Xt_prod.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_prod)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_prod,\n    Yt_prod,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_prod, Yv_prod),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_prod = model.predict(Xt_prod)\nYvp_prod = model.predict(Xv_prod)\n\n# REPORT\nregression_report(Yt_prod, Ytp_prod, Yv_prod, Yvp_prod)\n\n# compute RMSE using numpy\ntraining_RMSE_prod = np.sqrt(mean_squared_error(Yt_prod, Ytp_prod))\nvalidation_RMSE_prod = np.sqrt(mean_squared_error(Yv_prod, Yvp_prod))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_37\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_9 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_37 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 11ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.02313670990047248\n MAE: 0.11270792557147112\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.09841399937359264\n MAE: 0.28298587442587786\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_prod_lstm_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM(L2 Reg)',\n               mean_squared_error(Yt_prod, Ytp_prod),\n               mean_absolute_error(Yt_prod, Ytp_prod),\n              training_RMSE_prod,\n               mean_squared_error(Yv_prod, Yvp_prod),\n               mean_absolute_error(Yv_prod, Yvp_prod),\n               validation_RMSE_prod]]\nprint(tabulate(error_table_prod_lstm_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM(L2 Reg) │   0.0231367 │    0.112708 │     0.152108 │  0.098414 │  0.282986 │    0.31371 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_imp.shape, \"-->\", Yt_imp.shape)\nprint(Xv_imp.shape, \"-->\", Yv_imp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_imp.shape[1],Xt_imp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_imp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_imp,\n    Yt_imp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_imp, Yv_imp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_imp = model.predict(Xt_imp)\nYvp_imp = model.predict(Xv_imp)\n\n# REPORT\nregression_report(Yt_imp, Ytp_imp, Yv_imp, Yvp_imp)\n\n# compute RMSE using numpy\ntraining_RMSE_imp = np.sqrt(mean_squared_error(Yt_imp, Ytp_imp))\nvalidation_RMSE_imp = np.sqrt(mean_squared_error(Yv_imp, Yvp_imp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_38\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_10 (LSTM)              (None, 32)                4352      \n                                                                 \n dense_38 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 4ms/step\n3/3 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.1936177343659908\n MAE: 0.3472881397126535\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.22782918880664482\n MAE: 0.37552213994911376\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_imp_lstm_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM(L2 Reg)',\n               mean_squared_error(Yt_imp, Ytp_imp),\n               mean_absolute_error(Yt_imp, Ytp_imp),\n               training_RMSE_imp,\n               mean_squared_error(Yv_imp, Yvp_imp),\n               mean_absolute_error(Yv_imp, Yvp_imp),\n               validation_RMSE_imp]]\nprint(tabulate(error_table_imp_lstm_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM(L2 Reg) │    0.193618 │    0.347288 │      0.44002 │  0.227829 │  0.375522 │   0.477315 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_exp.shape, \"-->\", Yt_exp.shape)\nprint(Xv_exp.shape, \"-->\", Yv_exp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_exp.shape[1],Xt_exp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_exp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_exp,\n    Yt_exp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_exp, Yv_exp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_exp = model.predict(Xt_exp)\nYvp_exp = model.predict(Xv_exp)\n\n# REPORT\nregression_report(Yt_exp, Ytp_exp, Yv_exp, Yvp_exp)\n\n# compute RMSE using numpy\ntraining_RMSE_exp = np.sqrt(mean_squared_error(Yt_exp, Ytp_exp))\nvalidation_RMSE_exp = np.sqrt(mean_squared_error(Yv_exp, Yvp_exp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_39\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_11 (LSTM)              (None, 32)                4352      \n                                                                 \n dense_39 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 3ms/step\n3/3 [==============================] - 0s 4ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.006405860351627613\n MAE: 0.05985329218919866\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 2.236326883072915\n MAE: 1.305820683189038\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_exp_lstm_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM(L2 Reg)',\n               mean_squared_error(Yt_exp, Ytp_exp),\n               mean_absolute_error(Yt_exp, Ytp_exp),\n               training_RMSE_exp,\n               mean_squared_error(Yv_exp, Yvp_exp),\n               mean_absolute_error(Yv_exp, Yvp_exp),\n               validation_RMSE_exp]]\nprint(tabulate(error_table_exp_lstm_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM(L2 Reg) │  0.00640586 │   0.0598533 │    0.0800366 │   2.23633 │   1.30582 │    1.49544 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_stk.shape, \"-->\", Yt_stk.shape)\nprint(Xv_stk.shape, \"-->\", Yv_stk.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_stk.shape[1],Xt_stk.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_stk)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_stk,\n    Yt_stk,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_stk, Yv_stk),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_stk = model.predict(Xt_stk)\nYvp_stk = model.predict(Xv_stk)\n\n# REPORT\nregression_report(Yt_stk, Ytp_stk, Yv_stk, Yvp_stk)\n\n# compute RMSE using numpy\ntraining_RMSE_stk = np.sqrt(mean_squared_error(Yt_stk, Ytp_stk))\nvalidation_RMSE_stk = np.sqrt(mean_squared_error(Yv_stk, Yvp_stk))\n\n\n(3377, 5, 1) --> (3377, 1)\n(1123, 5, 1) --> (1123, 1)\nModel: \"sequential_40\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_12 (LSTM)              (None, 32)                4352      \n                                                                 \n dense_40 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n106/106 [==============================] - 0s 2ms/step\n36/36 [==============================] - 0s 2ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.002091984399747166\n MAE: 0.03466316595481215\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.373875263482317\n MAE: 0.2811309082041175\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_stk_lstm_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM(L2 Reg)',\n               mean_squared_error(Yt_stk, Ytp_stk),\n               mean_absolute_error(Yt_stk, Ytp_stk),\n               training_RMSE_stk,\n               mean_squared_error(Yv_stk, Yvp_stk),\n               mean_absolute_error(Yv_stk, Yvp_stk),\n               validation_RMSE_stk]]\nprint(tabulate(error_table_stk_lstm_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM(L2 Reg) │  0.00209198 │   0.0346632 │    0.0457382 │  0.373875 │  0.281131 │   0.611453 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_emissions.shape, \"-->\", Yt_emissions.shape)\nprint(Xv_emissions.shape, \"-->\", Yv_emissions.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2= 1e-4\ninput_shape=(Xt_emissions.shape[1],Xt_emissions.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_emissions)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_emissions,\n    Yt_emissions,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_emissions, Yv_emissions),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_emissions = model.predict(Xt_emissions)\nYvp_emissions = model.predict(Xv_emissions)\n\n# REPORT\nregression_report(Yt_emissions, Ytp_emissions, Yv_emissions, Yvp_emissions)\n\n# compute RMSE using numpy\ntraining_RMSE_emissions = np.sqrt(mean_squared_error(Yt_emissions, Ytp_emissions))\nvalidation_RMSE_emissions = np.sqrt(mean_squared_error(Yv_emissions, Yvp_emissions))\n\n\n(227, 5, 1) --> (227, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_41\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_13 (LSTM)              (None, 32)                4352      \n                                                                 \n dense_41 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 5ms/step\n3/3 [==============================] - 0s 6ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.7241078736992722\n MAE: 0.4202230640627933\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.17440960957364\n MAE: 0.5130328755168816\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_emissions_lstm_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM(L2 Reg)',\n               mean_squared_error(Yt_emissions, Ytp_emissions),\n               mean_absolute_error(Yt_emissions, Ytp_emissions),\n               training_RMSE_emissions,\n               mean_squared_error(Yv_emissions, Yvp_emissions),\n               mean_absolute_error(Yv_emissions, Yvp_emissions),\n               validation_RMSE_emissions]]\nprint(tabulate(error_table_emissions_lstm_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM(L2 Reg) │    0.724108 │    0.420223 │     0.850945 │   1.17441 │  0.513033 │     1.0837 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛"
  },
  {
    "objectID": "ts_dl.html#discussion",
    "href": "ts_dl.html#discussion",
    "title": "Deep Learning for TS",
    "section": "Discussion",
    "text": "Discussion\n\nANN Model Comparison\nQuestion 1: How do the results from the 3 different ANN models compare with each other in terms of accuracy and predictive power?\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │   0.0856424 │    0.215367 │     0.292647 │  0.150874 │  0.289132 │   0.388425 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │   0.0969527 │    0.198325 │     0.311372 │  0.147882 │  0.25337  │   0.384555 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │   0.0930746 │    0.212394 │     0.305081 │  0.183277 │  0.318754 │   0.428109 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │   0.0943274 │    0.213642 │     0.307128 │  0.183933 │  0.303652 │   0.428875 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │   0.0928673 │    0.215475 │     0.304741 │  0.206514 │  0.336715 │   0.454438 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │   0.0969785 │    0.215971 │     0.311414 │  0.186477 │  0.302549 │   0.43183  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nThe result shows that the RNNs generally perform better than GRU and LSTM models in terms of training and validation MSE, MAE, and RMSE. This suggests that RNNs are more accurate in fitting the training data and also generalize better to validation data. Introducing L2 regularization tends to slightly raise training MSE across all models, indicating a balance between model complexity and accuracy. However, it often leads to improved generalization, as evidenced by lower or comparable validation errors, particularly in GRU and LSTM models.\n\n\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │   0.123641  │    0.275929 │     0.351626 │  0.422054 │  0.522695 │   0.649657 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │   0.0878347 │    0.230794 │     0.296369 │  0.37951  │  0.478876 │   0.616043 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │   0.233557  │    0.368506 │     0.483277 │  0.592965 │  0.654887 │   0.770042 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │   0.299569  │    0.434126 │     0.547329 │  1.21079  │  0.85321  │   1.10036  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │   0.297096  │    0.437727 │     0.545065 │  1.04373  │  0.879395 │   1.02163  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │   0.274762  │    0.409304 │     0.524177 │  1.24162  │  0.896379 │   1.11428  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nThe result shows that RNNs, especially with L2 regularization, perform best with the lowest error metrics, suggesting higher accuracy and generalization. In contrast, GRU and LSTM models, particularly with regularization, exhibit higher errors, indicating less effective predictions and potential overfitting, especially in complex models where they fail to capture underlying patterns as efficiently.\n\n\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │   0.0227048 │    0.118012 │     0.150681 │  0.091456 │  0.2621   │   0.302417 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │   0.0181818 │    0.104524 │     0.13484  │  0.087305 │  0.259603 │   0.295474 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │   0.0229274 │    0.11412  │     0.151418 │  0.130364 │  0.328039 │   0.361059 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │   0.02249   │    0.112564 │     0.149967 │  0.293959 │  0.498804 │   0.542179 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │   0.0233633 │    0.11192  │     0.152851 │  0.235821 │  0.444043 │   0.485614 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │   0.0231367 │    0.112708 │     0.152108 │  0.098414 │  0.282986 │   0.31371  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nThe results show that the L2 regularized RNN outperform other models, having the lowest training and validation MSE and MAE, indicating they generalize better on unseen data. Conversely, GRUs and LSTMs, especially with L2 regularization, show higher error rates in validation, suggesting potential overfitting or less effectiveness in capturing complex patterns. Particularly, regularized LSTMs and GRUs show considerable increases in validation errors compared to their training performance, highlighting challenges in model stability across different data subsets.\n\n\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │    0.156048 │    0.31658  │     0.395029 │  0.185671 │  0.359929 │   0.430895 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │    0.122194 │    0.265084 │     0.349563 │  0.136967 │  0.296626 │   0.370091 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │    0.191979 │    0.345011 │     0.438155 │  0.222296 │  0.372967 │   0.471482 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │    0.187826 │    0.334333 │     0.433389 │  0.228349 │  0.373415 │   0.477858 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │    0.193516 │    0.342503 │     0.439905 │  0.219581 │  0.374944 │   0.468595 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │    0.193618 │    0.347288 │     0.44002  │  0.227829 │  0.375522 │   0.477315 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nThe results reveals that the RNN with L2 regularization shows superior performance, as they have lowest training and validation errors, which suggests better generalization on unseen data. In contrast, the GRU and LSTM models, particularly with L2 regularization, shows higher error, indicating potential overfitting. The regularized RNN model emerges as the most effective in managing overfitting and achieving higher predictive accuracy.\n\n\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │  0.00535037 │   0.0551332 │    0.0731462 │  0.758507 │  0.640908 │   0.870923 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │  0.00453961 │   0.0536078 │    0.0673766 │  1.22632  │  0.964119 │   1.10739  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │  0.00592626 │   0.0584674 │    0.0769822 │  0.889458 │  0.766761 │   0.943111 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │  0.00766894 │   0.064497  │    0.0875725 │  2.70266  │  1.43477  │   1.64398  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │  0.00766894 │   0.064497  │    0.0875725 │  2.70266  │  1.43477  │   1.64398  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │  0.00640586 │   0.0598533 │    0.0800366 │  2.23633  │  1.30582  │   1.49544  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nThe RNN and GRU models without regularization outperform on the training set but show higher errors on validation data, suggesting overfitting. L2 Regularized RNN and GRU, and both LSTM models (with and without regularization) show less effective training and prediction capabilities with higher error rates across both sets, indicating potential issues in balancing bias and variance.\n\n\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │  0.00385925 │   0.0493704 │    0.0621229 │  0.241482 │ 0.252935  │   0.491409 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │  0.0019726  │   0.0355592 │    0.044414  │  0.012517 │ 0.0799382 │   0.111879 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │  0.00248111 │   0.0379035 │    0.0498107 │  0.224071 │ 0.260936  │   0.473361 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │  0.00274596 │   0.0389203 │    0.0524019 │  0.044396 │ 0.139761  │   0.210704 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │  0.0023674  │   0.0373848 │    0.0486559 │  0.344455 │ 0.312781  │   0.586903 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │  0.00209198 │   0.0346632 │    0.0457382 │  0.373875 │ 0.281131  │   0.611453 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nThe RNN and GRU models without regularization show lower training errors but higher validation errors, suggesting potential overfitting. In contrast, the L2 regularized models, especially RNN, significantly improve in validation, indicating effective mitigation against overfitting. The LSTM models, regardless of regularization, while maintaining competitive training errors, do not perform as well in validation, indicating less effective generalization.\n\n\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │    0.30333  │    0.348757 │     0.550754 │   1.51433 │  0.968769 │    1.23058 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │    0.246717 │    0.342717 │     0.496706 │   1.00403 │  0.774277 │    1.00201 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │    0.826495 │    0.474815 │     0.909118 │   1.32186 │  0.484335 │    1.14972 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │    0.767283 │    0.444922 │     0.875947 │   1.25035 │  0.440379 │    1.11819 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │    0.823936 │    0.479866 │     0.907709 │   1.30895 │  0.496887 │    1.14409 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │    0.724108 │    0.420223 │     0.850945 │   1.17441 │  0.513033 │    1.0837  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nAcross the models, the RNN and LSTM with L2 regularization show the best improvement in validation scores, indicating better generalization compared to their non-regularized counterparts. This suggests that regularization helps manage overfitting, thus enhancing the models’ predictive performance on new data. Notably, all models exhibit higher errors on validation data, which could suggest either overfitting on training data or underfitting to complex patterns not captured during training. This highlights the importance of model tuning and possibly more complex architectures or training procedures to better capture underlying data patterns.\n\n\n\nQuestion 2 What effect does including regularization have on your results?\nI used L2 regularization in all 3 ANN models because it prevents overfitting by adding a penalty proportional to the square of the model coefficients, promoting smaller weights. This technique helps improve the model’s generalization on unseen data particularly in scenarios where the model is complex or the training data is noisy, which is the most common challenges for natural gas dynamics and CO2 emissions datasets.\nIncluding regularization in the above models improved validation metrics. For instance, in the natural gas prices dataset, the RNN with L2 regularization shows improved validation MSE and RMSE compared to the standard RNN, indicating it handles overfitting more effectively. But if we can see a notable increase in RMSE on the validation set compared to the training set across all datasets, suggesting that overfitting remains an issue even though regularization tries to mitigate it.\nQuestion 3 How far into the future can the deep learning model accurately predict the future?\nDeep learning models can accurately predict the near future based on factors like the dataset’s nature, the model’s complexity, and the forecast duration. These models are particularly effective in making short to medium-term predictions. However, their accuracy diminishes over longer periods. For datasets with variables like natural gas prices, consumption, and emissions—which are subject to numerous external influences and inherent volatility—predictions remain reliably precise for a short span, typically from a few days to a couple of months. Beyond this range, the predictions become increasingly uncertain and require frequent updates and adjustments to maintain accuracy.\n\n\nANN vs. Traditional Univariate TS Models\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\nAnn Models Error Summary\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │   0.0856424 │    0.215367 │     0.292647 │  0.150874 │  0.289132 │   0.388425 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │   0.0969527 │    0.198325 │     0.311372 │  0.147882 │  0.25337  │   0.384555 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │   0.0930746 │    0.212394 │     0.305081 │  0.183277 │  0.318754 │   0.428109 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │   0.0943274 │    0.213642 │     0.307128 │  0.183933 │  0.303652 │   0.428875 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │   0.0928673 │    0.215475 │     0.304741 │  0.206514 │  0.336715 │   0.454438 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │   0.0969785 │    0.215971 │     0.311414 │  0.186477 │  0.302549 │   0.43183  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nTraditional Univariate Models Error Summary\n\n\n╒═════════╤═════════════╤══════════╤══════════╤══════════╤═════════╤══════════╤═════════════╕\n│ Model   │          ME │     RMSE │      MAE │      MPE │    MAPE │     MASE │        ACF1 │\n╞═════════╪═════════════╪══════════╪══════════╪══════════╪═════════╪══════════╪═════════════╡\n│ ARIMA   │ -0.00927973 │ 0.758939 │ 0.512161 │ -2.63271 │ 11.3002 │ 0.362227 │  0.00500843 │\n├─────────┼─────────────┼──────────┼──────────┼──────────┼─────────┼──────────┼─────────────┤\n│ SARIMA  │ -0.00818136 │ 0.79182  │ 0.522379 │ -1.36221 │ 11.2339 │ 0.369454 │ -0.00177239 │\n╘═════════╧═════════════╧══════════╧══════════╧══════════╧═════════╧══════════╧═════════════╛\n\n\nRMSE Comparison\n\n\n╒══════════════╤══════════╕\n│ Model        │     RMSE │\n╞══════════════╪══════════╡\n│ RNN          │ 0.292647 │\n├──────────────┼──────────┤\n│ RNN(L2 Reg)  │ 0.311372 │\n├──────────────┼──────────┤\n│ GRU          │ 0.305081 │\n├──────────────┼──────────┤\n│ GRU(L2 Reg)  │ 0.307128 │\n├──────────────┼──────────┤\n│ LSTM         │ 0.304741 │\n├──────────────┼──────────┤\n│ LSTM(L2 Reg) │ 0.311414 │\n├──────────────┼──────────┤\n│ ARIMA        │ 0.758939 │\n├──────────────┼──────────┤\n│ SARIMA       │ 0.79182  │\n╘══════════════╧══════════╛\n\n\n\n\nAnn Models Error Summary\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │   0.123641  │    0.275929 │     0.351626 │  0.422054 │  0.522695 │   0.649657 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │   0.0878347 │    0.230794 │     0.296369 │  0.37951  │  0.478876 │   0.616043 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │   0.233557  │    0.368506 │     0.483277 │  0.592965 │  0.654887 │   0.770042 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │   0.299569  │    0.434126 │     0.547329 │  1.21079  │  0.85321  │   1.10036  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │   0.297096  │    0.437727 │     0.545065 │  1.04373  │  0.879395 │   1.02163  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │   0.274762  │    0.409304 │     0.524177 │  1.24162  │  0.896379 │   1.11428  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nTraditional Univariate Models Error Summary\n\n\n╒═════════╤═════════╤══════════╤══════════╤══════════╤═════════╤══════════╤═════════════╕\n│ Model   │      ME │     RMSE │      MAE │      MPE │    MAPE │     MASE │        ACF1 │\n╞═════════╪═════════╪══════════╪══════════╪══════════╪═════════╪══════════╪═════════════╡\n│ ARIMA   │ 26124.5 │ 233048   │ 179130   │ 0.199796 │ 7.93188 │ 1.53233  │  0.00778866 │\n├─────────┼─────────┼──────────┼──────────┼──────────┼─────────┼──────────┼─────────────┤\n│ SARIMA  │ 10818.3 │  98342.5 │  73926.5 │ 0.348918 │ 3.23593 │ 0.632388 │ -0.00785298 │\n╘═════════╧═════════╧══════════╧══════════╧══════════╧═════════╧══════════╧═════════════╛\n\n\nRMSE Comparison\n\n\n╒══════════════╤═══════════════╕\n│ Model        │          RMSE │\n╞══════════════╪═══════════════╡\n│ RNN          │      0.351626 │\n├──────────────┼───────────────┤\n│ RNN(L2 Reg)  │      0.296369 │\n├──────────────┼───────────────┤\n│ GRU          │      0.483277 │\n├──────────────┼───────────────┤\n│ GRU(L2 Reg)  │      0.547329 │\n├──────────────┼───────────────┤\n│ LSTM         │      0.545065 │\n├──────────────┼───────────────┤\n│ LSTM(L2 Reg) │      0.524177 │\n├──────────────┼───────────────┤\n│ ARIMA        │ 233048        │\n├──────────────┼───────────────┤\n│ SARIMA       │  98342.5      │\n╘══════════════╧═══════════════╛\n\n\n\n\nAnn Models Error Summary\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │   0.0227048 │    0.118012 │     0.150681 │  0.091456 │  0.2621   │   0.302417 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │   0.0181818 │    0.104524 │     0.13484  │  0.087305 │  0.259603 │   0.295474 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │   0.0229274 │    0.11412  │     0.151418 │  0.130364 │  0.328039 │   0.361059 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │   0.02249   │    0.112564 │     0.149967 │  0.293959 │  0.498804 │   0.542179 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │   0.0233633 │    0.11192  │     0.152851 │  0.235821 │  0.444043 │   0.485614 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │   0.0231367 │    0.112708 │     0.152108 │  0.098414 │  0.282986 │   0.31371  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nTraditional Univariate Models Error Summary\n\n\n╒═════════╤══════════╤═════════╤═════════╤══════════╤═════════╤══════════╤════════════╕\n│ Model   │       ME │    RMSE │     MAE │      MPE │    MAPE │     MASE │       ACF1 │\n╞═════════╪══════════╪═════════╪═════════╪══════════╪═════════╪══════════╪════════════╡\n│ ARIMA   │ 22028.8  │ 97945.8 │ 73544.6 │ 0.684651 │ 2.7679  │ 0.564422 │ -0.0177529 │\n├─────────┼──────────┼─────────┼─────────┼──────────┼─────────┼──────────┼────────────┤\n│ SARIMA  │  2289.61 │ 52995.2 │ 34932.6 │ 0.079716 │ 1.31955 │ 0.268092 │  0.0231852 │\n╘═════════╧══════════╧═════════╧═════════╧══════════╧═════════╧══════════╧════════════╛\n\n\nRMSE Comparison\n\n\n╒══════════════╤══════════════╕\n│ Model        │         RMSE │\n╞══════════════╪══════════════╡\n│ RNN          │     0.150681 │\n├──────────────┼──────────────┤\n│ RNN(L2 Reg)  │     0.13484  │\n├──────────────┼──────────────┤\n│ GRU          │     0.151418 │\n├──────────────┼──────────────┤\n│ GRU(L2 Reg)  │     0.149967 │\n├──────────────┼──────────────┤\n│ LSTM         │     0.152851 │\n├──────────────┼──────────────┤\n│ LSTM(L2 Reg) │     0.152108 │\n├──────────────┼──────────────┤\n│ ARIMA        │ 97945.8      │\n├──────────────┼──────────────┤\n│ SARIMA       │ 52995.2      │\n╘══════════════╧══════════════╛\n\n\n\n\nAnn Models Error Summary\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │    0.156048 │    0.31658  │     0.395029 │  0.185671 │  0.359929 │   0.430895 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │    0.122194 │    0.265084 │     0.349563 │  0.136967 │  0.296626 │   0.370091 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │    0.191979 │    0.345011 │     0.438155 │  0.222296 │  0.372967 │   0.471482 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │    0.187826 │    0.334333 │     0.433389 │  0.228349 │  0.373415 │   0.477858 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │    0.193516 │    0.342503 │     0.439905 │  0.219581 │  0.374944 │   0.468595 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │    0.193618 │    0.347288 │     0.44002  │  0.227829 │  0.375522 │   0.477315 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nTraditional Univariate Models Error Summary\n\n\n╒═════════╤═══════════╤═════════╤═════════╤═══════════╤═════════╤══════════╤═══════════╕\n│ Model   │        ME │    RMSE │     MAE │       MPE │    MAPE │     MASE │      ACF1 │\n╞═════════╪═══════════╪═════════╪═════════╪═══════════╪═════════╪══════════╪═══════════╡\n│ ARIMA   │ -129.508  │ 22945.8 │ 18087.9 │ -0.616726 │ 6.37179 │ 0.808696 │ 0.0337695 │\n├─────────┼───────────┼─────────┼─────────┼───────────┼─────────┼──────────┼───────────┤\n│ SARIMA  │   63.5282 │ 17693.6 │ 14186.8 │ -0.209191 │ 4.95986 │ 0.634282 │ 0.0273341 │\n╘═════════╧═══════════╧═════════╧═════════╧═══════════╧═════════╧══════════╧═══════════╛\n\n\nRMSE Comparison\n\n\n╒══════════════╤══════════════╕\n│ Model        │         RMSE │\n╞══════════════╪══════════════╡\n│ RNN          │     0.395029 │\n├──────────────┼──────────────┤\n│ RNN(L2 Reg)  │     0.349563 │\n├──────────────┼──────────────┤\n│ GRU          │     0.438155 │\n├──────────────┼──────────────┤\n│ GRU(L2 Reg)  │     0.433389 │\n├──────────────┼──────────────┤\n│ LSTM         │     0.439905 │\n├──────────────┼──────────────┤\n│ LSTM(L2 Reg) │     0.44002  │\n├──────────────┼──────────────┤\n│ ARIMA        │ 22945.8      │\n├──────────────┼──────────────┤\n│ SARIMA       │ 17693.6      │\n╘══════════════╧══════════════╛\n\n\n\n\nAnn Models Error Summary\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │  0.00535037 │   0.0551332 │    0.0731462 │  0.758507 │  0.640908 │   0.870923 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │  0.00453961 │   0.0536078 │    0.0673766 │  1.22632  │  0.964119 │   1.10739  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │  0.00592626 │   0.0584674 │    0.0769822 │  0.889458 │  0.766761 │   0.943111 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │  0.00766894 │   0.064497  │    0.0875725 │  2.70266  │  1.43477  │   1.64398  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │  0.00766894 │   0.064497  │    0.0875725 │  2.70266  │  1.43477  │   1.64398  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │  0.00640586 │   0.0598533 │    0.0800366 │  2.23633  │  1.30582  │   1.49544  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nTraditional Univariate Models Error Summary\n\n\n╒═════════╤═══════════╤═════════╤══════════╤═══════════╤══════════╤══════════╤════════════╕\n│ Model   │        ME │    RMSE │      MAE │       MPE │     MAPE │     MASE │       ACF1 │\n╞═════════╪═══════════╪═════════╪══════════╪═══════════╪══════════╪══════════╪════════════╡\n│ ARIMA   │ 2304.26   │ 21024.2 │ 12413.9  │  0.373212 │ 10.4586  │ 0.450189 │ -0.010844  │\n├─────────┼───────────┼─────────┼──────────┼───────────┼──────────┼──────────┼────────────┤\n│ SARIMA  │   88.1159 │ 16309.7 │  9630.99 │ -0.372932 │  9.00983 │ 0.349268 │ -0.0809737 │\n╘═════════╧═══════════╧═════════╧══════════╧═══════════╧══════════╧══════════╧════════════╛\n\n\nRMSE Comparison\n\n\n╒══════════════╤═══════════════╕\n│ Model        │          RMSE │\n╞══════════════╪═══════════════╡\n│ RNN          │     0.0731462 │\n├──────────────┼───────────────┤\n│ RNN(L2 Reg)  │     0.0673766 │\n├──────────────┼───────────────┤\n│ GRU          │     0.0769822 │\n├──────────────┼───────────────┤\n│ GRU(L2 Reg)  │     0.0875725 │\n├──────────────┼───────────────┤\n│ LSTM         │     0.0875725 │\n├──────────────┼───────────────┤\n│ LSTM(L2 Reg) │     0.0800366 │\n├──────────────┼───────────────┤\n│ ARIMA        │ 21024.2       │\n├──────────────┼───────────────┤\n│ SARIMA       │ 16309.7       │\n╘══════════════╧═══════════════╛\n\n\n\n\nAnn Models Error Summary\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │  0.00385925 │   0.0493704 │    0.0621229 │  0.241482 │ 0.252935  │   0.491409 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │  0.0019726  │   0.0355592 │    0.044414  │  0.012517 │ 0.0799382 │   0.111879 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │  0.00248111 │   0.0379035 │    0.0498107 │  0.224071 │ 0.260936  │   0.473361 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │  0.00274596 │   0.0389203 │    0.0524019 │  0.044396 │ 0.139761  │   0.210704 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │  0.0023674  │   0.0373848 │    0.0486559 │  0.344455 │ 0.312781  │   0.586903 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │  0.00209198 │   0.0346632 │    0.0457382 │  0.373875 │ 0.281131  │   0.611453 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nTraditional Univariate Models Error Summary\n\n\n╒═════════╤═══════════╤═════════╤══════════╤═══════════╤═════════╤═══════════╤════════════╕\n│ Model   │        ME │    RMSE │      MAE │       MPE │    MAPE │      MASE │       ACF1 │\n╞═════════╪═══════════╪═════════╪══════════╪═══════════╪═════════╪═══════════╪════════════╡\n│ ARIMA   │ 0.0269576 │ 1.15102 │ 0.686711 │ 0.0246547 │ 1.19127 │ 0.0519912 │ 0.00422394 │\n╘═════════╧═══════════╧═════════╧══════════╧═══════════╧═════════╧═══════════╧════════════╛\n\n\nRMSE Comparison\n\n\n╒══════════════╤═══════════╕\n│ Model        │      RMSE │\n╞══════════════╪═══════════╡\n│ RNN          │ 0.0621229 │\n├──────────────┼───────────┤\n│ RNN(L2 Reg)  │ 0.044414  │\n├──────────────┼───────────┤\n│ GRU          │ 0.0498107 │\n├──────────────┼───────────┤\n│ GRU(L2 Reg)  │ 0.0524019 │\n├──────────────┼───────────┤\n│ LSTM         │ 0.0486559 │\n├──────────────┼───────────┤\n│ LSTM(L2 Reg) │ 0.0457382 │\n├──────────────┼───────────┤\n│ ARIMA        │ 1.15102   │\n╘══════════════╧═══════════╛\n\n\n\n\nAnn Models Error Summary\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │    0.30333  │    0.348757 │     0.550754 │   1.51433 │  0.968769 │    1.23058 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │    0.246717 │    0.342717 │     0.496706 │   1.00403 │  0.774277 │    1.00201 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │    0.826495 │    0.474815 │     0.909118 │   1.32186 │  0.484335 │    1.14972 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │    0.767283 │    0.444922 │     0.875947 │   1.25035 │  0.440379 │    1.11819 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │    0.823936 │    0.479866 │     0.907709 │   1.30895 │  0.496887 │    1.14409 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │    0.724108 │    0.420223 │     0.850945 │   1.17441 │  0.513033 │    1.0837  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nTraditional Univariate Models Error Summary\n\n\n╒═════════╤══════════╤═════════╤═════════╤══════════╤═════════╤══════════╤════════════╕\n│ Model   │       ME │    RMSE │     MAE │      MPE │    MAPE │     MASE │       ACF1 │\n╞═════════╪══════════╪═════════╪═════════╪══════════╪═════════╪══════════╪════════════╡\n│ ARIMA   │ -2.50447 │ 240.128 │ 153.942 │ -54.1323 │ 110.854 │ 0.768645 │ -0.0062189 │\n╘═════════╧══════════╧═════════╧═════════╧══════════╧═════════╧══════════╧════════════╛\n\n\nRMSE Comparison\n\n\n╒══════════════╤════════════╕\n│ Model        │       RMSE │\n╞══════════════╪════════════╡\n│ RNN          │   0.550754 │\n├──────────────┼────────────┤\n│ RNN(L2 Reg)  │   0.496706 │\n├──────────────┼────────────┤\n│ GRU          │   0.909118 │\n├──────────────┼────────────┤\n│ GRU(L2 Reg)  │   0.875947 │\n├──────────────┼────────────┤\n│ LSTM         │   0.907709 │\n├──────────────┼────────────┤\n│ LSTM(L2 Reg) │   0.850945 │\n├──────────────┼────────────┤\n│ ARIMA        │ 240.128    │\n╘══════════════╧════════════╛\n\n\n\n\n\nQuestion 4 How does your deep learning modeling compare to the traditional single-variable time-series ARMA/ARIMA models from HW-3?\nThe results show that deep learning models often excel in capturing complex patterns but may require techniques like regularization to manage overfitting and ensure they generalize well to new, unseen datasets. In comparison, traditional time-series models like ARIMA might not capture complex patterns as effectively but can provide stable and consistent predictions across different scenarios.\nFor example, the RNN model in natural gas prices dataset has a training MSE of about 0.086 and a validation MSE of about 0.151. This shows that while the model predicts fairly accurately on the training data, its predictions become less reliable on unseen data. In contrast, the traditional ARIMA model has an RMSE of about 0.759, suggesting that while it may not capture complex patterns as well as the RNN, it maintains a moderate level of prediction error across different data. Moreover, the GRU model with L2 regularization in the natural gas consumption dataset shows a training RMSE of about 0.547 and a validation RMSE of approximately 1.100. This example indicates significant overfitting, as the model’s error nearly doubles when applied to new data. On the other hand, the SARIMA model has a much higher RMSE of about 98,342 on the training set, which is extraordinarily high and suggests that the model might not be well-suited for this dataset without adjustments or considering model inadequacies. Furthermore, in natural gas production dataset, the LSTM model with L2 regularization shows smaller training and validation RMSEs of about 0.152 and 0.313, respectively. This demonstrates that the regularization helps in managing overfitting effectively, keeping the model’s performance more consistent across new data. Conversely, the ARIMA model here has a significantly lower RMSE of 97,945, highlighting a potential good fit for simpler models in stable data environments.\nModel Comparison\nRMSE Comparison: Lookin at the RMSE values, deep learning models such as RNN, LSTM, and their L2 regularized versions tend to have lower RMSE on training datasets, indicating a strong fit to the data. For example, in the natural gas prices dataset, the LSTM model shows a training RMSE of 0.304 compared to its validation RMSE of 0.454, reflecting overfitting. In contrast, traditional models like ARIMA and SARIMA in the same dataset have higher RMSE values (0.759 and 0.792, respectively) on the training set, suggesting that they are less flexible in capturing the complexities of the data but may generalize better on unseen data. However, the deep learning models’ validation RMSEs are considerably closer to their training RMSEs than those of traditional models, indicating potential for better generalization with proper tuning.\nForecasting Comparison: Deep learning models, particularly LSTM and GRU, have shown capabilities in modeling complex temporal sequences effectively, which is evident from their performance on various datasets. For example, the LSTM model in the natural gas imports dataset achieves a lower validation RMSE of 0.468 compared to the traditional ARIMA model’s RMSE of 22945.8, highlighting the superiority of LSTM in handling complex patterns. Similarly, in the natural gas production dataset, the LSTM model reduces the validation RMSE to 0.313, outperforming the traditional models. This demonstrates that deep learning models are particularly effective in scenarios where data has non-linear relationships and requires capturing long-term dependencies. Moreover, traditional models like ARIMA and SARIMAX still hold substantial value, particularly in scenarios where understanding the influence of external variables is critical."
  },
  {
    "objectID": "ts_dl.html#rnn-with-l2-regularization",
    "href": "ts_dl.html#rnn-with-l2-regularization",
    "title": "Deep Learning for TS",
    "section": "RNN with L2 Regularization",
    "text": "RNN with L2 Regularization\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt.shape, \"-->\", Yt.shape)\nprint(Xv.shape, \"-->\", Yv.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt,\n    Yt,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv, Yv),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_7 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_7 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 8ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.09695274568972209\n MAE: 0.19832510495281233\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.14788218490913857\n MAE: 0.2533704014187368\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_rnn_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN(L2 Reg)',\n               mean_squared_error(Yt, Ytp),\n               mean_absolute_error(Yt, Ytp),\n               training_RMSE,\n               mean_squared_error(Yv, Yvp),\n               mean_absolute_error(Yv, Yvp),\n               validation_RMSE]]\nprint(tabulate(error_table_rnn_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN(L2 Reg) │   0.0969527 │    0.198325 │     0.311372 │  0.147882 │   0.25337 │   0.384555 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_consump.shape, \"-->\", Yt_consump.shape)\nprint(Xv_consump.shape, \"-->\", Yv_consump.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_consump.shape[1],Xt_consump.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_consump)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_consump,\n    Yt_consump,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_consump, Yv_consump),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_consump = model.predict(Xt_consump)\nYvp_consump = model.predict(Xv_consump)\n\n# REPORT\nregression_report(Yt_consump, Ytp_consump, Yv_consump, Yvp_consump)\n\n# compute RMSE using numpy\ntraining_RMSE_consump = np.sqrt(mean_squared_error(Yt_consump, Ytp_consump))\nvalidation_RMSE_consump = np.sqrt(mean_squared_error(Yv_consump, Yvp_consump))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_8\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_8 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_8 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 9ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.08783466579054877\n MAE: 0.23079411155968446\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.37950958354797426\n MAE: 0.4788762563089349\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_consump_rnn_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN(L2 Reg)',\n               mean_squared_error(Yt_consump, Ytp_consump),\n               mean_absolute_error(Yt_consump, Ytp_consump),\n              training_RMSE_consump,\n               mean_squared_error(Yv_consump, Yvp_consump),\n               mean_absolute_error(Yv_consump, Yvp_consump),\n               validation_RMSE_consump]]\nprint(tabulate(error_table_consump_rnn_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN(L2 Reg) │   0.0878347 │    0.230794 │     0.296369 │   0.37951 │  0.478876 │   0.616043 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_prod.shape, \"-->\", Yt_prod.shape)\nprint(Xv_prod.shape, \"-->\", Yv_prod.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_prod.shape[1],Xt_prod.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_prod)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_prod,\n    Yt_prod,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_prod, Yv_prod),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_prod = model.predict(Xt_prod)\nYvp_prod = model.predict(Xv_prod)\n\n# REPORT\nregression_report(Yt_prod, Ytp_prod, Yv_prod, Yvp_prod)\n\n# compute RMSE using numpy\ntraining_RMSE_prod = np.sqrt(mean_squared_error(Yt_prod, Ytp_prod))\nvalidation_RMSE_prod = np.sqrt(mean_squared_error(Yv_prod, Yvp_prod))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_9\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_9 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_9 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.01818179822574183\n MAE: 0.10452351690930663\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.08730496132160037\n MAE: 0.2596031001030938\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_prod_rnn_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN(L2 Reg)',\n               mean_squared_error(Yt_prod, Ytp_prod),\n               mean_absolute_error(Yt_prod, Ytp_prod),\n              training_RMSE_prod,\n               mean_squared_error(Yv_prod, Yvp_prod),\n               mean_absolute_error(Yv_prod, Yvp_prod),\n               validation_RMSE_prod]]\nprint(tabulate(error_table_prod_rnn_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN(L2 Reg) │   0.0181818 │    0.104524 │      0.13484 │  0.087305 │  0.259603 │   0.295474 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_imp.shape, \"-->\", Yt_imp.shape)\nprint(Xv_imp.shape, \"-->\", Yv_imp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_imp.shape[1],Xt_imp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_imp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_imp,\n    Yt_imp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_imp, Yv_imp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_imp = model.predict(Xt_imp)\nYvp_imp = model.predict(Xv_imp)\n\n# REPORT\nregression_report(Yt_imp, Ytp_imp, Yv_imp, Yvp_imp)\n\n# compute RMSE using numpy\ntraining_RMSE_imp = np.sqrt(mean_squared_error(Yt_imp, Ytp_imp))\nvalidation_RMSE_imp = np.sqrt(mean_squared_error(Yv_imp, Yvp_imp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_10\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_10 (SimpleRNN)   (None, 32)                1088      \n                                                                 \n dense_10 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 2ms/step\n3/3 [==============================] - 0s 4ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.12219420752197113\n MAE: 0.2650835268729244\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.13696709684218533\n MAE: 0.29662623805609023\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_imp_rnn_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN(L2 Reg)',\n               mean_squared_error(Yt_imp, Ytp_imp),\n               mean_absolute_error(Yt_imp, Ytp_imp),\n               training_RMSE_imp,\n               mean_squared_error(Yv_imp, Yvp_imp),\n               mean_absolute_error(Yv_imp, Yvp_imp),\n               validation_RMSE_imp]]\nprint(tabulate(error_table_imp_rnn_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN(L2 Reg) │    0.122194 │    0.265084 │     0.349563 │  0.136967 │  0.296626 │   0.370091 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_exp.shape, \"-->\", Yt_exp.shape)\nprint(Xv_exp.shape, \"-->\", Yv_exp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_exp.shape[1],Xt_exp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_exp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_exp,\n    Yt_exp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_exp, Yv_exp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_exp = model.predict(Xt_exp)\nYvp_exp = model.predict(Xv_exp)\n\n# REPORT\nregression_report(Yt_exp, Ytp_exp, Yv_exp, Yvp_exp)\n\n# compute RMSE using numpy\ntraining_RMSE_exp = np.sqrt(mean_squared_error(Yt_exp, Ytp_exp))\nvalidation_RMSE_exp = np.sqrt(mean_squared_error(Yv_exp, Yvp_exp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_11\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_11 (SimpleRNN)   (None, 32)                1088      \n                                                                 \n dense_11 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 3ms/step\n3/3 [==============================] - 0s 6ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.00453961182804625\n MAE: 0.0536077609618094\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.2263178960178174\n MAE: 0.9641191578617606\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_exp_rnn_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN(L2 Reg)',\n               mean_squared_error(Yt_exp, Ytp_exp),\n               mean_absolute_error(Yt_exp, Ytp_exp),\n               training_RMSE_exp,\n               mean_squared_error(Yv_exp, Yvp_exp),\n               mean_absolute_error(Yv_exp, Yvp_exp),\n               validation_RMSE_exp]]\nprint(tabulate(error_table_exp_rnn_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN(L2 Reg) │  0.00453961 │   0.0536078 │    0.0673766 │   1.22632 │  0.964119 │    1.10739 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_stk.shape, \"-->\", Yt_stk.shape)\nprint(Xv_stk.shape, \"-->\", Yv_stk.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_stk.shape[1],Xt_stk.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_stk)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_stk,\n    Yt_stk,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_stk, Yv_stk),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_stk = model.predict(Xt_stk)\nYvp_stk = model.predict(Xv_stk)\n\n# REPORT\nregression_report(Yt_stk, Ytp_stk, Yv_stk, Yvp_stk)\n\n# compute RMSE using numpy\ntraining_RMSE_stk = np.sqrt(mean_squared_error(Yt_stk, Ytp_stk))\nvalidation_RMSE_stk = np.sqrt(mean_squared_error(Yv_stk, Yvp_stk))\n\n\n(3377, 5, 1) --> (3377, 1)\n(1123, 5, 1) --> (1123, 1)\nModel: \"sequential_12\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_12 (SimpleRNN)   (None, 32)                1088      \n                                                                 \n dense_12 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n106/106 [==============================] - 0s 2ms/step\n36/36 [==============================] - 0s 2ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.001972599895865288\n MAE: 0.03555915816751766\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.012517007405626432\n MAE: 0.07993816860155734\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_stk_rnn_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN(L2 Reg)',\n               mean_squared_error(Yt_stk, Ytp_stk),\n               mean_absolute_error(Yt_stk, Ytp_stk),\n               training_RMSE_stk,\n               mean_squared_error(Yv_stk, Yvp_stk),\n               mean_absolute_error(Yv_stk, Yvp_stk),\n               validation_RMSE_stk]]\nprint(tabulate(error_table_stk_rnn_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN(L2 Reg) │   0.0019726 │   0.0355592 │     0.044414 │  0.012517 │ 0.0799382 │   0.111879 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_emissions.shape, \"-->\", Yt_emissions.shape)\nprint(Xv_emissions.shape, \"-->\", Yv_emissions.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_emissions.shape[1],Xt_emissions.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_emissions)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_emissions,\n    Yt_emissions,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_emissions, Yv_emissions),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_emissions = model.predict(Xt_emissions)\nYvp_emissions = model.predict(Xv_emissions)\n\n# REPORT\nregression_report(Yt_emissions, Ytp_emissions, Yv_emissions, Yvp_emissions)\n\n# compute RMSE using numpy\ntraining_RMSE_emissions = np.sqrt(mean_squared_error(Yt_emissions, Ytp_emissions))\nvalidation_RMSE_emissions = np.sqrt(mean_squared_error(Yv_emissions, Yvp_emissions))\n\n\n(227, 5, 1) --> (227, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_13\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_13 (SimpleRNN)   (None, 32)                1088      \n                                                                 \n dense_13 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 3ms/step\n3/3 [==============================] - 0s 4ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.2467168729013631\n MAE: 0.3427165033846242\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.0040270495300536\n MAE: 0.7742765141142125\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_emissions_rnn_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN(L2 Reg)',\n               mean_squared_error(Yt_emissions, Ytp_emissions),\n               mean_absolute_error(Yt_emissions, Ytp_emissions),\n               training_RMSE_emissions,\n               mean_squared_error(Yv_emissions, Yvp_emissions),\n               mean_absolute_error(Yv_emissions, Yvp_emissions),\n               validation_RMSE_emissions]]\nprint(tabulate(error_table_emissions_rnn_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN(L2 Reg) │    0.246717 │    0.342717 │     0.496706 │   1.00403 │  0.774277 │    1.00201 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛"
  },
  {
    "objectID": "ts_dl.html#load-data",
    "href": "ts_dl.html#load-data",
    "title": "Deep Learning for TS",
    "section": "Load data",
    "text": "Load data\nLet’s load the time series dataset and explore it.\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport yfinance as yf\nimport plotly.express as px\nimport statsmodels.api as sm\nfrom IPython.display import IFrame,display, Markdown\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom tabulate import tabulate\n\nprices = pd.read_csv(\"datasets/eda/Henry_Hub_Natural_Gas_Spot_Price.csv\")\nprices = prices.rename(columns={'Month':'t', 'Henry Hub Natural Gas Spot Price Dollars per Million Btu':'y'})\nprices = prices[['t', 'y']]\nt = np.array([*range(0, prices.shape[0])])\nx = np.array(prices[\"y\"]).reshape(t.shape[0], 1)\nfeature_columns = [0]  # columns to use as features\ntarget_columns = [0]  # columns to use as targets\n\ntable_shape_price = [['Variable','Type', 'Shape'], [\"t\", type(t), t.shape], [\"x\", type(x), x.shape]]\nprint(tabulate(table_shape_price, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒════════════╤═════════════════════════╤══════════╕\n│ Variable   │ Type                    │ Shape    │\n╞════════════╪═════════════════════════╪══════════╡\n│ t          │ <class 'numpy.ndarray'> │ (216,)   │\n├────────────┼─────────────────────────┼──────────┤\n│ x          │ <class 'numpy.ndarray'> │ (216, 1) │\n╘════════════╧═════════════════════════╧══════════╛\n\n\n\n\n\n\nView Code\nconsumption = pd.read_csv(\"datasets/eda/Consumption.csv\")\nconsumption = consumption.rename(columns={'Month':'t', 'Consumption':'y'})\nconsumption = consumption[['t', 'y']]\nt_consump = np.array([*range(0, consumption.shape[0])])\nx_consump = np.array(consumption[\"y\"]).reshape(t_consump.shape[0], 1)\nfeature_columns_consump = [0]  # columns to use as features\ntarget_columns_consump = [0]  # columns to use as targets\n\ntable_shape_consumption = [['Variable','Type', 'Shape'], [\"t\", type(t_consump), t_consump.shape], [\"x\", type(x_consump), x_consump.shape]]\nprint(tabulate(table_shape_price, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒════════════╤═════════════════════════╤══════════╕\n│ Variable   │ Type                    │ Shape    │\n╞════════════╪═════════════════════════╪══════════╡\n│ t          │ <class 'numpy.ndarray'> │ (216,)   │\n├────────────┼─────────────────────────┼──────────┤\n│ x          │ <class 'numpy.ndarray'> │ (216, 1) │\n╘════════════╧═════════════════════════╧══════════╛\n\n\n\n\n\n\nView Code\nproduction = pd.read_csv(\"datasets/eda/Production.csv\")\nproduction = production.rename(columns={'Month':'t', 'Production':'y'})\nproduction = production[['t', 'y']]\nt_prod = np.array([*range(0, production.shape[0])])\nx_prod = np.array(production[\"y\"]).reshape(t_prod.shape[0], 1)\nfeature_columns_prod = [0]  # columns to use as features\ntarget_columns_prod = [0]  # columns to use as targets\n\ntable_shape_prod = [['Variable','Type', 'Shape'], [\"t\", type(t_prod), t_prod.shape], [\"x\", type(x_prod), x_prod.shape]]\nprint(tabulate(table_shape_price, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒════════════╤═════════════════════════╤══════════╕\n│ Variable   │ Type                    │ Shape    │\n╞════════════╪═════════════════════════╪══════════╡\n│ t          │ <class 'numpy.ndarray'> │ (216,)   │\n├────────────┼─────────────────────────┼──────────┤\n│ x          │ <class 'numpy.ndarray'> │ (216, 1) │\n╘════════════╧═════════════════════════╧══════════╛\n\n\n\n\n\n\nView Code\nimports = pd.read_csv(\"datasets/eda/Imports.csv\")\nimports = imports.rename(columns={'Month':'t', 'Imports':'y'})\nimports = imports[['t', 'y']]\nt_imp = np.array([*range(0, imports.shape[0])])\nx_imp = np.array(imports[\"y\"]).reshape(t_imp.shape[0], 1)\nfeature_columns_imp = [0]  # columns to use as features\ntarget_columns_imp = [0]  # columns to use as targets\n\ntable_shape_imp = [['Variable','Type', 'Shape'], [\"t\", type(t_imp), t_imp.shape], [\"x\", type(x_imp), x_imp.shape]]\nprint(tabulate(table_shape_price, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒════════════╤═════════════════════════╤══════════╕\n│ Variable   │ Type                    │ Shape    │\n╞════════════╪═════════════════════════╪══════════╡\n│ t          │ <class 'numpy.ndarray'> │ (216,)   │\n├────────────┼─────────────────────────┼──────────┤\n│ x          │ <class 'numpy.ndarray'> │ (216, 1) │\n╘════════════╧═════════════════════════╧══════════╛\n\n\n\n\n\n\nView Code\nexports = pd.read_csv(\"datasets/eda/Exports.csv\")\nexports = exports.rename(columns={'Month':'t', 'Exports':'y'})\nexports = exports[['t', 'y']]\nt_exp = np.array([*range(0, exports.shape[0])])\nx_exp = np.array(exports[\"y\"]).reshape(t_exp.shape[0], 1)\nfeature_columns_exp = [0]  # columns to use as features\ntarget_columns_exp = [0]  # columns to use as targets\n\ntable_shape_exp = [['Variable','Type', 'Shape'], [\"t\", type(t_exp), t_exp.shape], [\"x\", type(x_exp), x_exp.shape]]\nprint(tabulate(table_shape_price, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒════════════╤═════════════════════════╤══════════╕\n│ Variable   │ Type                    │ Shape    │\n╞════════════╪═════════════════════════╪══════════╡\n│ t          │ <class 'numpy.ndarray'> │ (216,)   │\n├────────────┼─────────────────────────┼──────────┤\n│ x          │ <class 'numpy.ndarray'> │ (216, 1) │\n╘════════════╧═════════════════════════╧══════════╛\n\n\n\n\n\n\nView Code\nstocks = yf.download(\"CVX\", '2005-01-01','2022-12-01')\nstocks = stocks.reset_index()\nstocks = stocks.rename(columns={'Date':'t', 'Adj Close':'y'})\nstocks = stocks[['t', 'y']]\nt_stk = np.array([*range(0,stocks.shape[0])])\nx_stk = np.array(stocks['y']).reshape(t_stk.shape[0],1)\nfeature_columns_stk = [0]\ntarget_columns_stk = [0]\n\ntable_shape_stk = [['Variable','Type', 'Shape'], [\"t\", type(t_stk), t_stk.shape], [\"x\", type(x_stk), x_stk.shape]]\nprint(tabulate(table_shape_price, headers='firstrow', tablefmt='fancy_grid'))\n\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n╒════════════╤═════════════════════════╤══════════╕\n│ Variable   │ Type                    │ Shape    │\n╞════════════╪═════════════════════════╪══════════╡\n│ t          │ <class 'numpy.ndarray'> │ (216,)   │\n├────────────┼─────────────────────────┼──────────┤\n│ x          │ <class 'numpy.ndarray'> │ (216, 1) │\n╘════════════╧═════════════════════════╧══════════╛\n\n\n\n\n\n\n\n\n\nView Code\nemissions = pd.read_csv(\"datasets/eda/co2_emissions.csv\")\nemissions['Date'] = pd.to_datetime(emissions['Date'],format = \"%m/%d/%y\")\nemissions = emissions.rename(columns={'Date':'t', 'co2_value':'y'})\nemissions = emissions[['t', 'y']]\nt_emissions = np.array([*range(0, emissions.shape[0])])\nx_emissions = np.array(emissions[\"y\"]).reshape(t_emissions.shape[0], 1)\nfeature_columns_emissions = [0]  # columns to use as features\ntarget_columns_emissions = [0]  # columns to use as targets\n\ntable_shape_emissions = [['Variable','Type', 'Shape'], [\"t\", type(t_emissions), t_emissions.shape], [\"x\", type(x_emissions), x_emissions.shape]]\nprint(tabulate(table_shape_emissions, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒════════════╤═════════════════════════╤══════════╕\n│ Variable   │ Type                    │ Shape    │\n╞════════════╪═════════════════════════╪══════════╡\n│ t          │ <class 'numpy.ndarray'> │ (310,)   │\n├────────────┼─────────────────────────┼──────────┤\n│ x          │ <class 'numpy.ndarray'> │ (310, 1) │\n╘════════════╧═════════════════════════╧══════════╛"
  },
  {
    "objectID": "dl.html#load-data",
    "href": "dl.html#load-data",
    "title": "Deep Learning for TS",
    "section": "Load data",
    "text": "Load data\nLet’s load the time series dataset and explore it.\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport yfinance as yf\nimport plotly.express as px\nimport statsmodels.api as sm\nfrom IPython.display import IFrame,display, Markdown\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom tabulate import tabulate\n\nprices = pd.read_csv(\"datasets/eda/Henry_Hub_Natural_Gas_Spot_Price.csv\")\nprices = prices.rename(columns={'Month':'t', 'Henry Hub Natural Gas Spot Price Dollars per Million Btu':'y'})\nprices = prices[['t', 'y']]\nt = np.array([*range(0, prices.shape[0])])\nx = np.array(prices[\"y\"]).reshape(t.shape[0], 1)\nfeature_columns = [0]  # columns to use as features\ntarget_columns = [0]  # columns to use as targets\n\ntable_shape_price = [['Variable','Type', 'Shape'], [\"t\", type(t), t.shape], [\"x\", type(x), x.shape]]\nprint(tabulate(table_shape_price, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒════════════╤═════════════════════════╤══════════╕\n│ Variable   │ Type                    │ Shape    │\n╞════════════╪═════════════════════════╪══════════╡\n│ t          │ <class 'numpy.ndarray'> │ (216,)   │\n├────────────┼─────────────────────────┼──────────┤\n│ x          │ <class 'numpy.ndarray'> │ (216, 1) │\n╘════════════╧═════════════════════════╧══════════╛\n\n\n\n\n\n\nView Code\nconsumption = pd.read_csv(\"datasets/eda/Consumption.csv\")\nconsumption = consumption.rename(columns={'Month':'t', 'Consumption':'y'})\nconsumption = consumption[['t', 'y']]\nt_consump = np.array([*range(0, consumption.shape[0])])\nx_consump = np.array(consumption[\"y\"]).reshape(t_consump.shape[0], 1)\nfeature_columns_consump = [0]  # columns to use as features\ntarget_columns_consump = [0]  # columns to use as targets\n\ntable_shape_consumption = [['Variable','Type', 'Shape'], [\"t\", type(t_consump), t_consump.shape], [\"x\", type(x_consump), x_consump.shape]]\nprint(tabulate(table_shape_price, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒════════════╤═════════════════════════╤══════════╕\n│ Variable   │ Type                    │ Shape    │\n╞════════════╪═════════════════════════╪══════════╡\n│ t          │ <class 'numpy.ndarray'> │ (216,)   │\n├────────────┼─────────────────────────┼──────────┤\n│ x          │ <class 'numpy.ndarray'> │ (216, 1) │\n╘════════════╧═════════════════════════╧══════════╛\n\n\n\n\n\n\nView Code\nproduction = pd.read_csv(\"datasets/eda/Production.csv\")\nproduction = production.rename(columns={'Month':'t', 'Production':'y'})\nproduction = production[['t', 'y']]\nt_prod = np.array([*range(0, production.shape[0])])\nx_prod = np.array(production[\"y\"]).reshape(t_prod.shape[0], 1)\nfeature_columns_prod = [0]  # columns to use as features\ntarget_columns_prod = [0]  # columns to use as targets\n\ntable_shape_prod = [['Variable','Type', 'Shape'], [\"t\", type(t_prod), t_prod.shape], [\"x\", type(x_prod), x_prod.shape]]\nprint(tabulate(table_shape_price, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒════════════╤═════════════════════════╤══════════╕\n│ Variable   │ Type                    │ Shape    │\n╞════════════╪═════════════════════════╪══════════╡\n│ t          │ <class 'numpy.ndarray'> │ (216,)   │\n├────────────┼─────────────────────────┼──────────┤\n│ x          │ <class 'numpy.ndarray'> │ (216, 1) │\n╘════════════╧═════════════════════════╧══════════╛\n\n\n\n\n\n\nView Code\nimports = pd.read_csv(\"datasets/eda/Imports.csv\")\nimports = imports.rename(columns={'Month':'t', 'Imports':'y'})\nimports = imports[['t', 'y']]\nt_imp = np.array([*range(0, imports.shape[0])])\nx_imp = np.array(imports[\"y\"]).reshape(t_imp.shape[0], 1)\nfeature_columns_imp = [0]  # columns to use as features\ntarget_columns_imp = [0]  # columns to use as targets\n\ntable_shape_imp = [['Variable','Type', 'Shape'], [\"t\", type(t_imp), t_imp.shape], [\"x\", type(x_imp), x_imp.shape]]\nprint(tabulate(table_shape_price, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒════════════╤═════════════════════════╤══════════╕\n│ Variable   │ Type                    │ Shape    │\n╞════════════╪═════════════════════════╪══════════╡\n│ t          │ <class 'numpy.ndarray'> │ (216,)   │\n├────────────┼─────────────────────────┼──────────┤\n│ x          │ <class 'numpy.ndarray'> │ (216, 1) │\n╘════════════╧═════════════════════════╧══════════╛\n\n\n\n\n\n\nView Code\nexports = pd.read_csv(\"datasets/eda/Exports.csv\")\nexports = exports.rename(columns={'Month':'t', 'Exports':'y'})\nexports = exports[['t', 'y']]\nt_exp = np.array([*range(0, exports.shape[0])])\nx_exp = np.array(exports[\"y\"]).reshape(t_exp.shape[0], 1)\nfeature_columns_exp = [0]  # columns to use as features\ntarget_columns_exp = [0]  # columns to use as targets\n\ntable_shape_exp = [['Variable','Type', 'Shape'], [\"t\", type(t_exp), t_exp.shape], [\"x\", type(x_exp), x_exp.shape]]\nprint(tabulate(table_shape_price, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒════════════╤═════════════════════════╤══════════╕\n│ Variable   │ Type                    │ Shape    │\n╞════════════╪═════════════════════════╪══════════╡\n│ t          │ <class 'numpy.ndarray'> │ (216,)   │\n├────────────┼─────────────────────────┼──────────┤\n│ x          │ <class 'numpy.ndarray'> │ (216, 1) │\n╘════════════╧═════════════════════════╧══════════╛\n\n\n\n\n\n\nView Code\nstocks = yf.download(\"CVX\", '2005-01-01','2022-12-01')\nstocks = stocks.reset_index()\nstocks = stocks.rename(columns={'Date':'t', 'Adj Close':'y'})\nstocks = stocks[['t', 'y']]\nt_stk = np.array([*range(0,stocks.shape[0])])\nx_stk = np.array(stocks['y']).reshape(t_stk.shape[0],1)\nfeature_columns_stk = [0]\ntarget_columns_stk = [0]\n\ntable_shape_stk = [['Variable','Type', 'Shape'], [\"t\", type(t_stk), t_stk.shape], [\"x\", type(x_stk), x_stk.shape]]\nprint(tabulate(table_shape_price, headers='firstrow', tablefmt='fancy_grid'))\n\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n╒════════════╤═════════════════════════╤══════════╕\n│ Variable   │ Type                    │ Shape    │\n╞════════════╪═════════════════════════╪══════════╡\n│ t          │ <class 'numpy.ndarray'> │ (216,)   │\n├────────────┼─────────────────────────┼──────────┤\n│ x          │ <class 'numpy.ndarray'> │ (216, 1) │\n╘════════════╧═════════════════════════╧══════════╛\n\n\n\n\n\n\n\n\n\nView Code\nemissions = pd.read_csv(\"datasets/eda/co2_emissions.csv\")\nemissions['Date'] = pd.to_datetime(emissions['Date'],format = \"%m/%d/%y\")\nemissions = emissions.rename(columns={'Date':'t', 'co2_value':'y'})\nemissions = emissions[['t', 'y']]\nt_emissions = np.array([*range(0, emissions.shape[0])])\nx_emissions = np.array(emissions[\"y\"]).reshape(t_emissions.shape[0], 1)\nfeature_columns_emissions = [0]  # columns to use as features\ntarget_columns_emissions = [0]  # columns to use as targets\n\ntable_shape_emissions = [['Variable','Type', 'Shape'], [\"t\", type(t_emissions), t_emissions.shape], [\"x\", type(x_emissions), x_emissions.shape]]\nprint(tabulate(table_shape_emissions, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒════════════╤═════════════════════════╤══════════╕\n│ Variable   │ Type                    │ Shape    │\n╞════════════╪═════════════════════════╪══════════╡\n│ t          │ <class 'numpy.ndarray'> │ (310,)   │\n├────────────┼─────────────────────────┼──────────┤\n│ x          │ <class 'numpy.ndarray'> │ (310, 1) │\n╘════════════╧═════════════════════════╧══════════╛"
  },
  {
    "objectID": "dl.html#visualize-raw-data",
    "href": "dl.html#visualize-raw-data",
    "title": "Deep Learning for TS",
    "section": "Visualize “raw” data",
    "text": "Visualize “raw” data\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\nfig, ax = plt.subplots()\nfor i in range(0, x.shape[1]):\n    ax.plot(t, x[:, i], \"o\", alpha=0.5)\n    ax.plot(t, x[:, i], \"-\")\nax.plot(t, 0 * x[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nfig, ax = plt.subplots()\nfor i in range(0, x_consump.shape[1]):\n    ax.plot(t_consump, x_consump[:, i], \"o\", alpha=0.5)\n    ax.plot(t_consump, x_consump[:, i], \"-\")\nax.plot(t_consump, 0 * x_consump[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nfig, ax = plt.subplots()\nfor i in range(0, x_prod.shape[1]):\n    ax.plot(t_prod, x_prod[:, i], \"o\", alpha=0.5)\n    ax.plot(t_prod, x_prod[:, i], \"-\")\nax.plot(t_prod, 0 * x_prod[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nfig, ax = plt.subplots()\nfor i in range(0, x_imp.shape[1]):\n    ax.plot(t_imp, x_imp[:, i], \"o\", alpha=0.5)\n    ax.plot(t_imp, x_imp[:, i], \"-\")\nax.plot(t_imp, 0 * x_imp[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nfig, ax = plt.subplots()\nfor i in range(0, x_exp.shape[1]):\n    ax.plot(t_exp, x_exp[:, i], \"o\", alpha=0.5)\n    ax.plot(t_exp, x_exp[:, i], \"-\")\nax.plot(t_exp, 0 * x_exp[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nfig, ax = plt.subplots()\nfor i in range(0, x_stk.shape[1]):\n    ax.plot(t_stk, x_stk[:, i], \"o\", alpha=0.5)\n    ax.plot(t_stk, x_stk[:, i], \"-\")\nax.plot(t_stk, 0 * x_stk[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nfig, ax = plt.subplots()\nfor i in range(0, x_emissions.shape[1]):\n    ax.plot(t_emissions, x_emissions[:, i], \"o\", alpha=0.5)\n    ax.plot(t_emissions, x_emissions[:, i], \"-\")\nax.plot(t_emissions, 0 * x_emissions[:, 0], \"-\")  # add baseline for reference\nplt.show()"
  },
  {
    "objectID": "dl.html#normalize",
    "href": "dl.html#normalize",
    "title": "Deep Learning for TS",
    "section": "Normalize",
    "text": "Normalize\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\nprint(\"----Shape of statistical measures and the normalized Data----\")\nprint(np.mean(x, axis=0).shape, np.std(x, axis=0).shape)\nx = (x - np.mean(x, axis=0)) / np.std(x, axis=0)\nprint(x.shape)\n\n\n----Shape of statistical measures and the normalized Data----\n(1,) (1,)\n(216, 1)\n\n\n\n\nView Code\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x.shape[1]):\n    ax.plot(t, x[:, i], \"o\")\n    ax.plot(t, x[:, i], \"-\")\nax.plot(t, 0 * x[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nprint(\"----Shape of statistical measures and the normalized Data----\")\nprint(np.mean(x_consump, axis=0).shape, np.std(x_consump, axis=0).shape)\nx_consump = (x_consump - np.mean(x_consump, axis=0)) / np.std(x_consump, axis=0)\nprint(x_consump.shape)\n\n\n----Shape of statistical measures and the normalized Data----\n(1,) (1,)\n(216, 1)\n\n\n\n\nView Code\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_consump.shape[1]):\n    ax.plot(t_consump, x_consump[:, i], \"o\")\n    ax.plot(t_consump, x_consump[:, i], \"-\")\nax.plot(t_consump, 0 * x_consump[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nprint(\"----Shape of statistical measures and the normalized Data----\")\nprint(np.mean(x_prod, axis=0).shape, np.std(x_prod, axis=0).shape)\nx_prod = (x_prod - np.mean(x_prod, axis=0)) / np.std(x_prod, axis=0)\nprint(x_prod.shape)\n\n\n----Shape of statistical measures and the normalized Data----\n(1,) (1,)\n(216, 1)\n\n\n\n\nView Code\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_prod.shape[1]):\n    ax.plot(t_prod, x_prod[:, i], \"o\")\n    ax.plot(t_prod, x_prod[:, i], \"-\")\nax.plot(t_prod, 0 * x_prod[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nprint(\"----Shape of statistical measures and the normalized Data----\")\nprint(np.mean(x_imp, axis=0).shape, np.std(x_imp, axis=0).shape)\nx_imp = (x_imp - np.mean(x_imp, axis=0)) / np.std(x_imp, axis=0)\nprint(x_imp.shape)\n\n\n----Shape of statistical measures and the normalized Data----\n(1,) (1,)\n(312, 1)\n\n\n\n\nView Code\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_imp.shape[1]):\n    ax.plot(t_imp, x_imp[:, i], \"o\")\n    ax.plot(t_imp, x_imp[:, i], \"-\")\nax.plot(t_imp, 0 * x_imp[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nprint(\"----Shape of statistical measures and the normalized Data----\")\nprint(np.mean(x_exp, axis=0).shape, np.std(x_exp, axis=0).shape)\nx_exp = (x_exp - np.mean(x_exp, axis=0)) / np.std(x_exp, axis=0)\nprint(x_exp.shape)\n\n\n----Shape of statistical measures and the normalized Data----\n(1,) (1,)\n(312, 1)\n\n\n\n\nView Code\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_exp.shape[1]):\n    ax.plot(t_exp, x_exp[:, i], \"o\")\n    ax.plot(t_exp, x_exp[:, i], \"-\")\nax.plot(t_exp, 0 * x_exp[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nprint(\"----Shape of statistical measures and the normalized Data----\")\nprint(np.mean(x_stk, axis=0).shape, np.std(x_stk, axis=0).shape)\nx_stk = (x_stk - np.mean(x_stk, axis=0)) / np.std(x_stk, axis=0)\nprint(x_stk.shape)\n\n\n----Shape of statistical measures and the normalized Data----\n(1,) (1,)\n(4510, 1)\n\n\n\n\nView Code\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_stk.shape[1]):\n    ax.plot(t_stk, x_stk[:, i], \"o\")\n    ax.plot(t_stk, x_stk[:, i], \"-\")\nax.plot(t_stk, 0 * x_stk[:, 0], \"-\")  # add baseline for reference\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nprint(\"----Shape of statistical measures and the normalized Data----\")\nprint(np.mean(x_emissions, axis=0).shape, np.std(x_emissions, axis=0).shape)\nx_emissions = (x_emissions - np.mean(x_emissions, axis=0)) / np.std(x_emissions, axis=0)\nprint(x_emissions.shape)\n\n\n----Shape of statistical measures and the normalized Data----\n(1,) (1,)\n(310, 1)\n\n\n\n\nView Code\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_emissions.shape[1]):\n    ax.plot(t_emissions, x_emissions[:, i], \"o\")\n    ax.plot(t_emissions, x_emissions[:, i], \"-\")\nax.plot(t_emissions, 0 * x_emissions[:, 0], \"-\")  # add baseline for reference\nplt.show()"
  },
  {
    "objectID": "dl.html#split",
    "href": "dl.html#split",
    "title": "Deep Learning for TS",
    "section": "Split",
    "text": "Split\nPartition into training and validation\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\nsplit_fraction = 0.75\ncut = int(split_fraction * x.shape[0])\ntt = t[0:cut]\nxt = x[0:cut]\ntv = t[cut:]\nxv = x[cut:]\n\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x.shape[1]):\n    ax.plot(tt, xt[:, i], \"ro\", alpha=0.25)\n    ax.plot(tt, xt[:, i], \"g-\")\nfor i in range(0, x.shape[1]):\n    ax.plot(tv, xv[:, i], \"bo\", alpha=0.25)\n    ax.plot(tv, xv[:, i], \"g-\")\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nsplit_fraction = 0.75\ncut_consump = int(split_fraction * x_consump.shape[0])\ntt_consump = t_consump[0:cut_consump]\nxt_consump = x_consump[0:cut_consump]\ntv_consump = t_consump[cut_consump:]\nxv_consump = x_consump[cut_consump:]\n\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_consump.shape[1]):\n    ax.plot(tt_consump, xt_consump[:, i], \"ro\", alpha=0.25)\n    ax.plot(tt_consump, xt_consump[:, i], \"g-\")\nfor i in range(0, x_consump.shape[1]):\n    ax.plot(tv_consump, xv_consump[:, i], \"bo\", alpha=0.25)\n    ax.plot(tv_consump, xv_consump[:, i], \"g-\")\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nsplit_fraction = 0.75\ncut_prod = int(split_fraction * x_prod.shape[0])\ntt_prod = t_prod[0:cut_prod]\nxt_prod = x_prod[0:cut_prod]\ntv_prod = t_prod[cut_prod:]\nxv_prod = x_prod[cut_prod:]\n\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_prod.shape[1]):\n    ax.plot(tt_prod, xt_prod[:, i], \"ro\", alpha=0.25)\n    ax.plot(tt_prod, xt_prod[:, i], \"g-\")\nfor i in range(0, x_prod.shape[1]):\n    ax.plot(tv_prod, xv_prod[:, i], \"bo\", alpha=0.25)\n    ax.plot(tv_prod, xv_prod[:, i], \"g-\")\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nsplit_fraction = 0.75\ncut_imp = int(split_fraction * x_imp.shape[0])\ntt_imp = t_imp[0:cut_imp]\nxt_imp = x_imp[0:cut_imp]\ntv_imp = t_imp[cut_imp:]\nxv_imp = x_imp[cut_imp:]\n\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_imp.shape[1]):\n    ax.plot(tt_imp, xt_imp[:, i], \"ro\", alpha=0.25)\n    ax.plot(tt_imp, xt_imp[:, i], \"g-\")\nfor i in range(0, x_imp.shape[1]):\n    ax.plot(tv_imp, xv_imp[:, i], \"bo\", alpha=0.25)\n    ax.plot(tv_imp, xv_imp[:, i], \"g-\")\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nsplit_fraction = 0.75\ncut_exp = int(split_fraction * x_exp.shape[0])\ntt_exp = t_exp[0:cut_exp]\nxt_exp = x_exp[0:cut_exp]\ntv_exp = t_exp[cut_exp:]\nxv_exp = x_exp[cut_exp:]\n\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_exp.shape[1]):\n    ax.plot(tt_exp, xt_exp[:, i], \"ro\", alpha=0.25)\n    ax.plot(tt_exp, xt_exp[:, i], \"g-\")\nfor i in range(0, x_exp.shape[1]):\n    ax.plot(tv_exp, xv_exp[:, i], \"bo\", alpha=0.25)\n    ax.plot(tv_exp, xv_exp[:, i], \"g-\")\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nsplit_fraction = 0.75\ncut_stk = int(split_fraction * x_stk.shape[0])\ntt_stk = t_stk[0:cut_stk]\nxt_stk = x_stk[0:cut_stk]\ntv_stk = t_stk[cut_stk:]\nxv_stk = x_stk[cut_stk:]\n\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_stk.shape[1]):\n    ax.plot(tt_stk, xt_stk[:, i], \"ro\", alpha=0.25)\n    ax.plot(tt_stk, xt_stk[:, i], \"g-\")\nfor i in range(0, x_stk.shape[1]):\n    ax.plot(tv_stk, xv_stk[:, i], \"bo\", alpha=0.25)\n    ax.plot(tv_stk, xv_stk[:, i], \"g-\")\nplt.show()\n\n\n\n\n\n\n\n\n\nView Code\nsplit_fraction = 0.75\ncut_emissions = int(split_fraction * x_emissions.shape[0])\ntt_emissions = t_emissions[0:cut_emissions]\nxt_emissions = x_emissions[0:cut_emissions]\ntv_emissions = t_emissions[cut_emissions:]\nxv_emissions = x_emissions[cut_emissions:]\n\n# visualize normalized data\nfig, ax = plt.subplots()\nfor i in range(0, x_emissions.shape[1]):\n    ax.plot(tt_emissions, xt_emissions[:, i], \"ro\", alpha=0.25)\n    ax.plot(tt_emissions, xt_emissions[:, i], \"g-\")\nfor i in range(0, x_emissions.shape[1]):\n    ax.plot(tv_emissions, xv_emissions[:, i], \"bo\", alpha=0.25)\n    ax.plot(tv_emissions, xv_emissions[:, i], \"g-\")\nplt.show()"
  },
  {
    "objectID": "dl.html#mini-batch-creation",
    "href": "dl.html#mini-batch-creation",
    "title": "Deep Learning for TS",
    "section": "Mini-batch creation",
    "text": "Mini-batch creation\nFunction to form time-series mini-batches\n\n\nView Code\n# function to form time-series mini-batches\n# inputs are the mini-batching hyper-params\n\n# x-> 304 x 3\n\n\ndef form_arrays(\n    x,\n    lookback=3,\n    delay=1,\n    step=1,\n    feature_columns=[0],\n    target_columns=[0],\n    unique=False,\n    verbose=False,\n):\n    # verbose=True --> report and plot for debugging\n    # unique=True --> don't re-sample:\n    # x1,x2,x3 --> x4 then x4,x5,x6 --> x7 instead of x2,x3,x4 --> x5\n\n    # initialize\n    i_start = 0\n    count = 0\n\n    # initialize output arrays with samples\n    x_out = []\n    y_out = []\n\n    # sequentially build mini-batch samples\n    while i_start + lookback + delay < x.shape[0]:\n\n        # define index bounds\n        i_stop = i_start + lookback\n        i_pred = i_stop + delay\n\n        # report if desired\n        if verbose and count < 2:\n            print(\"indice range:\", i_start, i_stop, \"-->\", i_pred)\n\n        # define arrays:\n        # method-1: buggy due to indexing from left\n        # numpy's slicing --> start:stop:step\n        # xtmp=x[i_start:i_stop+1:steps]\n\n        # method-2: non-vectorized but cleaner\n        indices_to_keep = []\n        j = i_stop\n        while j >= i_start:\n            indices_to_keep.append(j)\n            j = j - step\n\n        # create mini-batch sample\n        xtmp = x[indices_to_keep, :]  # isolate relevant indices\n        xtmp = xtmp[:, feature_columns]  # isolate desire features\n        ytmp = x[i_pred, target_columns]\n        x_out.append(xtmp)\n        y_out.append(ytmp)\n\n        # report if desired\n        if verbose and count < 2:\n            print(xtmp, \"-->\", ytmp)\n        if verbose and count < 2:\n            print(\"shape:\", xtmp.shape, \"-->\", ytmp.shape)\n\n        # PLOT FIRST SAMPLE IF DESIRED FOR DEBUGGING\n        if verbose and count < 2:\n            fig, ax = plt.subplots()\n            ax.plot(x, \"b-\")\n            ax.plot(x, \"bx\")\n            ax.plot(indices_to_keep, xtmp, \"go\")\n            ax.plot(i_pred * np.ones(len(target_columns)), ytmp, \"ro\")\n            plt.show()\n\n        # UPDATE START POINT\n        if unique:\n            i_start += lookback\n        i_start += 1\n        count += 1\n\n    return np.array(x_out), np.array(y_out)\n\n\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\nL=4; S=1; D=1\n\nXt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=True)\n\n# validation\nXv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=True)\n\nprint(\"training:\",Xt.shape,Yt.shape)\nprint(\"validation:\",Xv.shape,Yv.shape)\n\n\nindice range: 0 4 --> 5\n[[0.85610218]\n [1.15450362]\n [1.06801045]\n [0.71338845]\n [0.71771311]] --> [1.16315293]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[1.16315293]\n [0.85610218]\n [1.15450362]\n [1.06801045]\n [0.71338845]] --> [1.35776256]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 0 4 --> 5\n[[-0.17316653]\n [-0.52346387]\n [-0.64455431]\n [-0.66185294]\n [-0.7180735 ]] --> [-0.19478983]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-0.19478983]\n [-0.17316653]\n [-0.52346387]\n [-0.64455431]\n [-0.66185294]] --> [-0.59698306]\nshape: (5, 1) --> (1,)\n\n\n\n\n\ntraining: (157, 5, 1) (157, 1)\nvalidation: (49, 5, 1) (49, 1)\n\n\n\n\n\n\nView Code\nL=4; S=1; D=1\n\nXt_consump,Yt_consump=form_arrays(xt_consump,lookback=L,delay=D,step=S,feature_columns=feature_columns_consump,target_columns=target_columns_consump,unique=False,verbose=True)\n\n# validation\nXv_consump,Yv_consump=form_arrays(xv_consump,lookback=L,delay=D,step=S,feature_columns=feature_columns_consump,target_columns=target_columns_consump,unique=False,verbose=True)\n\nprint(\"training:\",Xt_consump.shape,Yt_consump.shape)\nprint(\"validation:\",Xv_consump.shape,Yv_consump.shape)\n\n\nindice range: 0 4 --> 5\n[[-1.43677685e+00]\n [-1.01144131e+00]\n [-1.48583565e-04]\n [ 7.80761951e-02]\n [ 7.48623565e-01]] --> [-1.41257488]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-1.41257488e+00]\n [-1.43677685e+00]\n [-1.01144131e+00]\n [-1.48583565e-04]\n [ 7.80761951e-02]] --> [-1.09191406]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 0 4 --> 5\n[[ 1.07264267]\n [ 0.16822216]\n [-0.10471185]\n [ 0.22706476]\n [ 0.30462503]] --> [1.66895021]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[ 1.66895021]\n [ 1.07264267]\n [ 0.16822216]\n [-0.10471185]\n [ 0.22706476]] --> [2.5622339]\nshape: (5, 1) --> (1,)\n\n\n\n\n\ntraining: (157, 5, 1) (157, 1)\nvalidation: (49, 5, 1) (49, 1)\n\n\n\n\n\n\nView Code\nL=4; S=1; D=1\n\nXt_prod,Yt_prod=form_arrays(xt_prod,lookback=L,delay=D,step=S,feature_columns=feature_columns_prod,target_columns=target_columns_prod,unique=False,verbose=True)\n\n# validation\nXv_prod,Yv_prod=form_arrays(xv_prod,lookback=L,delay=D,step=S,feature_columns=feature_columns_prod,target_columns=target_columns_prod,unique=False,verbose=True)\n\nprint(\"training:\",Xt_prod.shape,Yt_prod.shape)\nprint(\"validation:\",Xv_prod.shape,Yv_prod.shape)\n\n\nindice range: 0 4 --> 5\n[[-1.1702813 ]\n [-1.2273701 ]\n [-1.04221774]\n [-1.4269967 ]\n [-1.12554455]] --> [-1.23928047]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-1.23928047]\n [-1.1702813 ]\n [-1.2273701 ]\n [-1.04221774]\n [-1.4269967 ]] --> [-1.21881347]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 0 4 --> 5\n[[1.11922115]\n [1.21958425]\n [0.92508149]\n [1.02074385]\n [0.89624295]] --> [1.36032606]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[1.36032606]\n [1.11922115]\n [1.21958425]\n [0.92508149]\n [1.02074385]] --> [1.33331651]\nshape: (5, 1) --> (1,)\n\n\n\n\n\ntraining: (157, 5, 1) (157, 1)\nvalidation: (49, 5, 1) (49, 1)\n\n\n\n\n\n\nView Code\nL=4; S=1; D=1\n\nXt_imp,Yt_imp=form_arrays(xt_imp,lookback=L,delay=D,step=S,feature_columns=feature_columns_imp,target_columns=target_columns_imp,unique=False,verbose=True)\n\n# validation\nXv_imp,Yv_imp=form_arrays(xv_imp,lookback=L,delay=D,step=S,feature_columns=feature_columns_imp,target_columns=target_columns_imp,unique=False,verbose=True)\n\nprint(\"training:\",Xt_imp.shape,Yt_imp.shape)\nprint(\"validation:\",Xv_imp.shape,Yv_imp.shape)\n\n\nindice range: 0 4 --> 5\n[[-0.81519714]\n [-0.88043638]\n [-0.53520443]\n [-0.83698639]\n [-0.14415448]] --> [-0.99167718]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-0.99167718]\n [-0.81519714]\n [-0.88043638]\n [-0.53520443]\n [-0.83698639]] --> [-0.92788811]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 0 4 --> 5\n[[-1.01139213]\n [-1.00395772]\n [-0.87469078]\n [-0.4434583 ]\n [-0.3876727 ]] --> [-0.09820799]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-0.09820799]\n [-1.01139213]\n [-1.00395772]\n [-0.87469078]\n [-0.4434583 ]] --> [0.10123611]\nshape: (5, 1) --> (1,)\n\n\n\n\n\ntraining: (229, 5, 1) (229, 1)\nvalidation: (73, 5, 1) (73, 1)\n\n\n\n\n\n\nView Code\nL=4; S=1; D=1\n\nXt_exp,Yt_exp=form_arrays(xt_exp,lookback=L,delay=D,step=S,feature_columns=feature_columns_exp,target_columns=target_columns_exp,unique=False,verbose=True)\n\n# validation\nXv_exp,Yv_exp=form_arrays(xv_exp,lookback=L,delay=D,step=S,feature_columns=feature_columns_exp,target_columns=target_columns_exp,unique=False,verbose=True)\n\nprint(\"training:\",Xt_exp.shape,Yt_exp.shape)\nprint(\"validation:\",Xv_exp.shape,Yv_exp.shape)\n\n\nindice range: 0 4 --> 5\n[[-0.90925709]\n [-0.88599358]\n [-0.87117889]\n [-0.89479982]\n [-0.89735727]] --> [-0.91276973]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-0.91276973]\n [-0.90925709]\n [-0.88599358]\n [-0.87117889]\n [-0.89479982]] --> [-0.90901675]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 0 4 --> 5\n[[0.43307505]\n [0.11599498]\n [0.27326245]\n [0.34869168]\n [0.1912763 ]] --> [0.57430149]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[0.57430149]\n [0.43307505]\n [0.11599498]\n [0.27326245]\n [0.34869168]] --> [0.69215349]\nshape: (5, 1) --> (1,)\n\n\n\n\n\ntraining: (229, 5, 1) (229, 1)\nvalidation: (73, 5, 1) (73, 1)\n\n\n\n\n\n\nView Code\nL=4; S=1; D=1\n\nXt_stk,Yt_stk=form_arrays(xt_stk,lookback=L,delay=D,step=S,feature_columns=feature_columns_stk,target_columns=target_columns_stk,unique=False,verbose=True)\n\n# validation\nXv_stk,Yv_stk=form_arrays(xv_stk,lookback=L,delay=D,step=S,feature_columns=feature_columns_stk,target_columns=target_columns_stk,unique=False,verbose=True)\n\nprint(\"training:\",Xt_stk.shape,Yt_stk.shape)\nprint(\"validation:\",Xv_stk.shape,Yv_stk.shape)\n\n\nindice range: 0 4 --> 5\n[[-1.54942093]\n [-1.53950359]\n [-1.55403739]\n [-1.55967994]\n [-1.55369545]] --> [-1.54548849]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-1.54548849]\n [-1.54942093]\n [-1.53950359]\n [-1.55403739]\n [-1.55967994]] --> [-1.54514621]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 0 4 --> 5\n[[0.97752522]\n [1.04526005]\n [1.06061717]\n [1.06089093]\n [1.0740536 ]] --> [1.030451]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[1.030451  ]\n [0.97752522]\n [1.04526005]\n [1.06061717]\n [1.06089093]] --> [1.01866013]\nshape: (5, 1) --> (1,)\n\n\n\n\n\ntraining: (3377, 5, 1) (3377, 1)\nvalidation: (1123, 5, 1) (1123, 1)\n\n\n\n\n\n\nView Code\nL=4; S=1; D=1\n\nXt_emissions,Yt_emissions=form_arrays(xt_emissions,lookback=L,delay=D,step=S,feature_columns=feature_columns_emissions,target_columns=target_columns_emissions,unique=False,verbose=True)\n\n# validation\nXv_emissions,Yv_emissions=form_arrays(xv_emissions,lookback=L,delay=D,step=S,feature_columns=feature_columns_emissions,target_columns=target_columns_emissions,unique=False,verbose=True)\n\nprint(\"training:\",Xt_emissions.shape,Yt_emissions.shape)\nprint(\"validation:\",Xv_emissions.shape,Yv_emissions.shape)\n\n\nindice range: 0 4 --> 5\n[[-0.34947117]\n [-0.33473844]\n [-0.29320189]\n [-0.25213225]\n [-0.22136124]] --> [-0.36881048]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-0.36881048]\n [-0.34947117]\n [-0.33473844]\n [-0.29320189]\n [-0.25213225]] --> [-0.3599988]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 0 4 --> 5\n[[-0.17532518]\n [-0.18899325]\n [-0.0868485 ]\n [ 3.74965096]\n [-0.12675237]] --> [-0.24779927]\nshape: (5, 1) --> (1,)\n\n\n\n\n\nindice range: 1 5 --> 6\n[[-0.24779927]\n [-0.17532518]\n [-0.18899325]\n [-0.0868485 ]\n [ 3.74965096]] --> [-0.29592599]\nshape: (5, 1) --> (1,)\n\n\n\n\n\ntraining: (227, 5, 1) (227, 1)\nvalidation: (73, 5, 1) (73, 1)"
  },
  {
    "objectID": "dl.html#utility-function",
    "href": "dl.html#utility-function",
    "title": "Deep Learning for TS",
    "section": "Utility function",
    "text": "Utility function\nFunction for reporting results\n\n\nView Code\nfrom sklearn.metrics import (\n    mean_squared_error,\n    mean_absolute_percentage_error,\n    mean_absolute_error,\n)\n\n\n# UTILITY FUNCTION\ndef regression_report(yt, ytp, yv, yvp):\n    print(\"---------- Regression report ----------\")\n\n    print(\"TRAINING:\")\n    print(\" MSE:\", mean_squared_error(yt, ytp))\n    print(\" MAE:\", mean_absolute_error(yt, ytp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n\n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yt, ytp, \"ro\")\n    ax.plot(yt, yt, \"b-\")\n    ax.set(\n        xlabel=\"y_data\",\n        ylabel=\"y_predicted\",\n        title=\"Training data parity plot (line y=x represents a perfect fit)\",\n    )\n    plt.show()\n\n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    frac_plot = 1.0\n    upper = int(frac_plot * yt.shape[0])\n    # print(int(0.5*yt.shape[0]))\n    fig, ax = plt.subplots()\n    ax.plot(yt[0:upper], \"b-\")\n    ax.plot(ytp[0:upper], \"r-\", alpha=0.5)\n    ax.plot(ytp[0:upper], \"ro\", alpha=0.25)\n    ax.set(\n        xlabel=\"index\",\n        ylabel=\"y(t (blue=actual & red=prediction)\",\n        title=\"Training: Time-series prediction\",\n    )\n    plt.show()\n\n    print(\"VALIDATION:\")\n    print(\" MSE:\", mean_squared_error(yv, yvp))\n    print(\" MAE:\", mean_absolute_error(yv, yvp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n\n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yv, yvp, \"ro\")\n    ax.plot(yv, yv, \"b-\")\n    ax.set(\n        xlabel=\"y_data\",\n        ylabel=\"y_predicted\",\n        title=\"Validation data parity plot (line y=x represents a perfect fit)\",\n    )\n    plt.show()\n\n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    upper = int(frac_plot * yv.shape[0])\n    fig, ax = plt.subplots()\n    ax.plot(yv[0:upper], \"b-\")\n    ax.plot(yvp[0:upper], \"r-\", alpha=0.5)\n    ax.plot(yvp[0:upper], \"ro\", alpha=0.25)\n    ax.set(\n        xlabel=\"index\",\n        ylabel=\"y(t) (blue=actual & red=prediction)\",\n        title=\"Validation: Time-series prediction\",\n    )\n    plt.show()\n\ndef history_plot(history):\n    FS=18   #FONT SIZE\n    # PLOTTING THE TRAINING AND VALIDATION LOSS\n    history_dict = history.history\n    loss_values = history_dict[\"loss\"]\n    val_loss_values = history_dict[\"val_loss\"]\n    epochs = range(1, len(loss_values) + 1)\n    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n    plt.title(\"Training and validation loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()"
  },
  {
    "objectID": "dl.html#reshape",
    "href": "dl.html#reshape",
    "title": "Deep Learning for TS",
    "section": "Reshape",
    "text": "Reshape\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\nprint(\"---------- Data setup ----------\")\n# RECALL\nprint(\"training:\", Xt.shape, Yt.shape)\nprint(\"validation:\", Xv.shape, Yv.shape)\n\n# RESHAPE INTO A DATA FRAME\nXt1 = Xt.reshape(Xt.shape[0], Xt.shape[1] * Xt.shape[2])\nXv1 = Xv.reshape(Xv.shape[0], Xv.shape[1] * Xv.shape[2])\n\n# NEW SIZES\nprint(Xt1.shape, \"-->\", Yt.shape)\nprint(Xv1.shape, \"-->\", Yv.shape)\n\n\n---------- Data setup ----------\ntraining: (157, 5, 1) (157, 1)\nvalidation: (49, 5, 1) (49, 1)\n(157, 5) --> (157, 1)\n(49, 5) --> (49, 1)\n\n\n\n\n\n\nView Code\nprint(\"---------- Data setup ----------\")\n# RECALL\nprint(\"training:\", Xt_consump.shape, Yt_consump.shape)\nprint(\"validation:\", Xv_consump.shape, Yv_consump.shape)\n\n# RESHAPE INTO A DATA FRAME\nXt1_consump = Xt_consump.reshape(Xt_consump.shape[0], Xt_consump.shape[1] * Xt_consump.shape[2])\nXv1_consump = Xv_consump.reshape(Xv_consump.shape[0], Xv_consump.shape[1] * Xv_consump.shape[2])\n\n# NEW SIZES\nprint(Xt1_consump.shape, \"-->\", Yt_consump.shape)\nprint(Xv1_consump.shape, \"-->\", Yv_consump.shape)\n\n\n---------- Data setup ----------\ntraining: (157, 5, 1) (157, 1)\nvalidation: (49, 5, 1) (49, 1)\n(157, 5) --> (157, 1)\n(49, 5) --> (49, 1)\n\n\n\n\n\n\nView Code\nprint(\"---------- Data setup ----------\")\n# RECALL\nprint(\"training:\", Xt_prod.shape, Yt_prod.shape)\nprint(\"validation:\", Xv_prod.shape, Yv_prod.shape)\n\n# RESHAPE INTO A DATA FRAME\nXt1_prod = Xt_prod.reshape(Xt_prod.shape[0], Xt_prod.shape[1] * Xt_prod.shape[2])\nXv1_prod = Xv_prod.reshape(Xv_prod.shape[0], Xv_prod.shape[1] * Xv_prod.shape[2])\n\n# NEW SIZES\nprint(Xt1_prod.shape, \"-->\", Yt_prod.shape)\nprint(Xv1_prod.shape, \"-->\", Yv_prod.shape)\n\n\n---------- Data setup ----------\ntraining: (157, 5, 1) (157, 1)\nvalidation: (49, 5, 1) (49, 1)\n(157, 5) --> (157, 1)\n(49, 5) --> (49, 1)\n\n\n\n\n\n\nView Code\nprint(\"---------- Data setup ----------\")\n# RECALL\nprint(\"training:\", Xt_imp.shape, Yt_imp.shape)\nprint(\"validation:\", Xv_imp.shape, Yv_imp.shape)\n\n# RESHAPE INTO A DATA FRAME\nXt1_imp = Xt_imp.reshape(Xt_imp.shape[0], Xt_imp.shape[1] * Xt_imp.shape[2])\nXv1_imp = Xv_imp.reshape(Xv_imp.shape[0], Xv_imp.shape[1] * Xv_imp.shape[2])\n\n# NEW SIZES\nprint(Xt1_imp.shape, \"-->\", Yt_imp.shape)\nprint(Xv1_imp.shape, \"-->\", Yv_imp.shape)\n\n\n---------- Data setup ----------\ntraining: (229, 5, 1) (229, 1)\nvalidation: (73, 5, 1) (73, 1)\n(229, 5) --> (229, 1)\n(73, 5) --> (73, 1)\n\n\n\n\n\n\nView Code\nprint(\"---------- Data setup ----------\")\n# RECALL\nprint(\"training:\", Xt_exp.shape, Yt_exp.shape)\nprint(\"validation:\", Xv_exp.shape, Yv_exp.shape)\n\n# RESHAPE INTO A DATA FRAME\nXt1_exp = Xt_exp.reshape(Xt_exp.shape[0], Xt_exp.shape[1] * Xt_exp.shape[2])\nXv1_exp = Xv_exp.reshape(Xv_exp.shape[0], Xv_exp.shape[1] * Xv_exp.shape[2])\n\n# NEW SIZES\nprint(Xt1_exp.shape, \"-->\", Yt_exp.shape)\nprint(Xv1_exp.shape, \"-->\", Yv_exp.shape)\n\n\n---------- Data setup ----------\ntraining: (229, 5, 1) (229, 1)\nvalidation: (73, 5, 1) (73, 1)\n(229, 5) --> (229, 1)\n(73, 5) --> (73, 1)\n\n\n\n\n\n\nView Code\nprint(\"---------- Data setup ----------\")\n# RECALL\nprint(\"training:\", Xt_stk.shape, Yt_stk.shape)\nprint(\"validation:\", Xv_stk.shape, Yv_stk.shape)\n\n# RESHAPE INTO A DATA FRAME\nXt1_stk = Xt_stk.reshape(Xt_stk.shape[0], Xt_stk.shape[1] * Xt_stk.shape[2])\nXv1_stk = Xv_stk.reshape(Xv_stk.shape[0], Xv_stk.shape[1] * Xv_stk.shape[2])\n\n# NEW SIZES\nprint(Xt1_stk.shape, \"-->\", Yt_stk.shape)\nprint(Xv1_stk.shape, \"-->\", Yv_stk.shape)\n\n\n---------- Data setup ----------\ntraining: (3377, 5, 1) (3377, 1)\nvalidation: (1123, 5, 1) (1123, 1)\n(3377, 5) --> (3377, 1)\n(1123, 5) --> (1123, 1)\n\n\n\n\n\n\nView Code\nprint(\"---------- Data setup ----------\")\n# RECALL\nprint(\"training:\", Xt_emissions.shape, Yt_emissions.shape)\nprint(\"validation:\", Xv_emissions.shape, Yv_emissions.shape)\n\n# RESHAPE INTO A DATA FRAME\nXt1_emissions = Xt_emissions.reshape(Xt_emissions.shape[0], Xt_emissions.shape[1] * Xt_emissions.shape[2])\nXv1_emissions = Xv_emissions.reshape(Xv_emissions.shape[0], Xv_emissions.shape[1] * Xv_emissions.shape[2])\n\n# NEW SIZES\nprint(Xt1_emissions.shape, \"-->\", Yt_emissions.shape)\nprint(Xv1_emissions.shape, \"-->\", Yv_emissions.shape)\n\n\n---------- Data setup ----------\ntraining: (227, 5, 1) (227, 1)\nvalidation: (73, 5, 1) (73, 1)\n(227, 5) --> (227, 1)\n(73, 5) --> (73, 1)"
  },
  {
    "objectID": "dl.html#rnn",
    "href": "dl.html#rnn",
    "title": "Deep Learning for TS",
    "section": "RNN",
    "text": "RNN\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt.shape, \"-->\", Yt.shape)\nprint(Xv.shape, \"-->\", Yv.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\n                     )\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt,\n    Yt,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv, Yv),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn (SimpleRNN)      (None, 32)                1088      \n                                                                 \n dense (Dense)               (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 10ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.08564239257871006\n MAE: 0.2153671237872524\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.15087434726218102\n MAE: 0.28913158219267315\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN',\n               mean_squared_error(Yt, Ytp),\n               mean_absolute_error(Yt, Ytp),\n              training_RMSE,\n               mean_squared_error(Yv, Yvp),\n               mean_absolute_error(Yv, Yvp),\n               validation_RMSE]]\nprint(tabulate(error_table, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN     │   0.0856424 │    0.215367 │     0.292647 │  0.150874 │  0.289132 │   0.388425 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_consump.shape, \"-->\", Yt_consump.shape)\nprint(Xv_consump.shape, \"-->\", Yv_consump.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_consump.shape[1],Xt_consump.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_consump)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\n                     )\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_consump,\n    Yt_consump,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_consump, Yv_consump),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_consump = model.predict(Xt_consump)\nYvp_consump = model.predict(Xv_consump)\n\n# REPORT\nregression_report(Yt_consump, Ytp_consump, Yv_consump, Yvp_consump)\n\n# compute RMSE using numpy\ntraining_RMSE_consump = np.sqrt(mean_squared_error(Yt_consump, Ytp_consump))\nvalidation_RMSE_consump = np.sqrt(mean_squared_error(Yv_consump, Yvp_consump))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_1 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_1 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 6ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.12364105531700466\n MAE: 0.27592941675879085\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.4220541753851497\n MAE: 0.5226954378234901\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_consump = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN',\n               mean_squared_error(Yt_consump, Ytp_consump),\n               mean_absolute_error(Yt_consump, Ytp_consump),\n              training_RMSE_consump,\n               mean_squared_error(Yv_consump, Yvp_consump),\n               mean_absolute_error(Yv_consump, Yvp_consump),\n               validation_RMSE_consump]]\nprint(tabulate(error_table_consump, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN     │    0.123641 │    0.275929 │     0.351626 │  0.422054 │  0.522695 │   0.649657 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_prod.shape, \"-->\", Yt_prod.shape)\nprint(Xv_prod.shape, \"-->\", Yv_prod.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_prod.shape[1],Xt_prod.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_prod)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\n                     )\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_prod,\n    Yt_prod,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_prod, Yv_prod),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_prod = model.predict(Xt_prod)\nYvp_prod = model.predict(Xv_prod)\n\n# REPORT\nregression_report(Yt_prod, Ytp_prod, Yv_prod, Yvp_prod)\n\n# compute RMSE using numpy\ntraining_RMSE_prod = np.sqrt(mean_squared_error(Yt_prod, Ytp_prod))\nvalidation_RMSE_prod = np.sqrt(mean_squared_error(Yv_prod, Yvp_prod))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_2 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_2 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.022704780666281316\n MAE: 0.11801223130892244\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.09145595200163269\n MAE: 0.2620996457905679\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_prod = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN',\n               mean_squared_error(Yt_prod, Ytp_prod),\n               mean_absolute_error(Yt_prod, Ytp_prod),\n              training_RMSE_prod,\n               mean_squared_error(Yv_prod, Yvp_prod),\n               mean_absolute_error(Yv_prod, Yvp_prod),\n               validation_RMSE_prod]]\nprint(tabulate(error_table_prod, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN     │   0.0227048 │    0.118012 │     0.150681 │  0.091456 │    0.2621 │   0.302417 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_imp.shape, \"-->\", Yt_imp.shape)\nprint(Xv_imp.shape, \"-->\", Yv_imp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_imp.shape[1],Xt_imp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_imp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\n                     )\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_imp,\n    Yt_imp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_imp, Yv_imp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_imp = model.predict(Xt_imp)\nYvp_imp = model.predict(Xv_imp)\n\n# REPORT\nregression_report(Yt_imp, Ytp_imp, Yv_imp, Yvp_imp)\n\n# compute RMSE using numpy\ntraining_RMSE_imp = np.sqrt(mean_squared_error(Yt_imp, Ytp_imp))\nvalidation_RMSE_imp = np.sqrt(mean_squared_error(Yv_imp, Yvp_imp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_3 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_3 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 1s 6ms/step\n3/3 [==============================] - 0s 8ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.1560476381351537\n MAE: 0.3165801404532817\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.18567062606530266\n MAE: 0.35992918817697195\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_imp = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN',\n               mean_squared_error(Yt_imp, Ytp_imp),\n               mean_absolute_error(Yt_imp, Ytp_imp),\n               training_RMSE_imp,\n               mean_squared_error(Yv_imp, Yvp_imp),\n               mean_absolute_error(Yv_imp, Yvp_imp),\n               validation_RMSE_imp]]\nprint(tabulate(error_table_imp, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN     │    0.156048 │     0.31658 │     0.395029 │  0.185671 │  0.359929 │   0.430895 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_exp.shape, \"-->\", Yt_exp.shape)\nprint(Xv_exp.shape, \"-->\", Yv_exp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_exp.shape[1],Xt_exp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_exp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\n                     )\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_exp,\n    Yt_exp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_exp, Yv_exp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_exp = model.predict(Xt_exp)\nYvp_exp = model.predict(Xv_exp)\n\n# REPORT\nregression_report(Yt_exp, Ytp_exp, Yv_exp, Yvp_exp)\n\n# compute RMSE using numpy\ntraining_RMSE_exp = np.sqrt(mean_squared_error(Yt_exp, Ytp_exp))\nvalidation_RMSE_exp = np.sqrt(mean_squared_error(Yv_exp, Yvp_exp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_4 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_4 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 3ms/step\n3/3 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.005350366835801499\n MAE: 0.055133177214101235\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.7585070258742818\n MAE: 0.6409075051411177\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_exp = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN',\n               mean_squared_error(Yt_exp, Ytp_exp),\n               mean_absolute_error(Yt_exp, Ytp_exp),\n               training_RMSE_exp,\n               mean_squared_error(Yv_exp, Yvp_exp),\n               mean_absolute_error(Yv_exp, Yvp_exp),\n               validation_RMSE_exp]]\nprint(tabulate(error_table_exp, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN     │  0.00535037 │   0.0551332 │    0.0731462 │  0.758507 │  0.640908 │   0.870923 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_stk.shape, \"-->\", Yt_stk.shape)\nprint(Xv_stk.shape, \"-->\", Yv_stk.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_stk.shape[1],Xt_stk.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_stk)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\n                     )\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_stk,\n    Yt_stk,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_stk, Yv_stk),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_stk = model.predict(Xt_stk)\nYvp_stk = model.predict(Xv_stk)\n\n# REPORT\nregression_report(Yt_stk, Ytp_stk, Yv_stk, Yvp_stk)\n\n# compute RMSE using numpy\ntraining_RMSE_stk = np.sqrt(mean_squared_error(Yt_stk, Ytp_stk))\nvalidation_RMSE_stk = np.sqrt(mean_squared_error(Yv_stk, Yvp_stk))\n\n\n(3377, 5, 1) --> (3377, 1)\n(1123, 5, 1) --> (1123, 1)\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_5 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_5 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n106/106 [==============================] - 0s 2ms/step\n36/36 [==============================] - 0s 2ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.003859248605304056\n MAE: 0.04937039958418123\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.2414823291143868\n MAE: 0.2529345975930672\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_stk = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN',\n               mean_squared_error(Yt_stk, Ytp_stk),\n               mean_absolute_error(Yt_stk, Ytp_stk),\n               training_RMSE_stk,\n               mean_squared_error(Yv_stk, Yvp_stk),\n               mean_absolute_error(Yv_stk, Yvp_stk),\n               validation_RMSE_stk]]\nprint(tabulate(error_table_stk, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN     │  0.00385925 │   0.0493704 │    0.0621229 │  0.241482 │  0.252935 │   0.491409 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_emissions.shape, \"-->\", Yt_emissions.shape)\nprint(Xv_emissions.shape, \"-->\", Yv_emissions.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_emissions.shape[1],Xt_emissions.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_emissions)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\n                     )\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_emissions,\n    Yt_emissions,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_emissions, Yv_emissions),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_emissions = model.predict(Xt_emissions)\nYvp_emissions = model.predict(Xv_emissions)\n\n# REPORT\nregression_report(Yt_emissions, Ytp_emissions, Yv_emissions, Yvp_emissions)\n\n# compute RMSE using numpy\ntraining_RMSE_emissions = np.sqrt(mean_squared_error(Yt_emissions, Ytp_emissions))\nvalidation_RMSE_emissions = np.sqrt(mean_squared_error(Yv_emissions, Yvp_emissions))\n\n\n(227, 5, 1) --> (227, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_6 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_6 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 4ms/step\n3/3 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.3033295428161593\n MAE: 0.3487574632804835\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.5143298833546888\n MAE: 0.9687692360570798\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_emissions = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN',\n               mean_squared_error(Yt_emissions, Ytp_emissions),\n               mean_absolute_error(Yt_emissions, Ytp_emissions),\n               training_RMSE_emissions,\n               mean_squared_error(Yv_emissions, Yvp_emissions),\n               mean_absolute_error(Yv_emissions, Yvp_emissions),\n               validation_RMSE_emissions]]\nprint(tabulate(error_table_emissions, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN     │     0.30333 │    0.348757 │     0.550754 │   1.51433 │  0.968769 │    1.23058 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛"
  },
  {
    "objectID": "dl.html#rnn-with-l2-regularization",
    "href": "dl.html#rnn-with-l2-regularization",
    "title": "Deep Learning for TS",
    "section": "RNN with L2 Regularization",
    "text": "RNN with L2 Regularization\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt.shape, \"-->\", Yt.shape)\nprint(Xv.shape, \"-->\", Yv.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt,\n    Yt,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv, Yv),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_7 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_7 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 8ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.09695274568972209\n MAE: 0.19832510495281233\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.14788218490913857\n MAE: 0.2533704014187368\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_rnn_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN(L2 Reg)',\n               mean_squared_error(Yt, Ytp),\n               mean_absolute_error(Yt, Ytp),\n               training_RMSE,\n               mean_squared_error(Yv, Yvp),\n               mean_absolute_error(Yv, Yvp),\n               validation_RMSE]]\nprint(tabulate(error_table_rnn_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN(L2 Reg) │   0.0969527 │    0.198325 │     0.311372 │  0.147882 │   0.25337 │   0.384555 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_consump.shape, \"-->\", Yt_consump.shape)\nprint(Xv_consump.shape, \"-->\", Yv_consump.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_consump.shape[1],Xt_consump.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_consump)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_consump,\n    Yt_consump,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_consump, Yv_consump),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_consump = model.predict(Xt_consump)\nYvp_consump = model.predict(Xv_consump)\n\n# REPORT\nregression_report(Yt_consump, Ytp_consump, Yv_consump, Yvp_consump)\n\n# compute RMSE using numpy\ntraining_RMSE_consump = np.sqrt(mean_squared_error(Yt_consump, Ytp_consump))\nvalidation_RMSE_consump = np.sqrt(mean_squared_error(Yv_consump, Yvp_consump))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_8\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_8 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_8 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 9ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.08783466579054877\n MAE: 0.23079411155968446\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.37950958354797426\n MAE: 0.4788762563089349\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_consump_rnn_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN(L2 Reg)',\n               mean_squared_error(Yt_consump, Ytp_consump),\n               mean_absolute_error(Yt_consump, Ytp_consump),\n              training_RMSE_consump,\n               mean_squared_error(Yv_consump, Yvp_consump),\n               mean_absolute_error(Yv_consump, Yvp_consump),\n               validation_RMSE_consump]]\nprint(tabulate(error_table_consump_rnn_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN(L2 Reg) │   0.0878347 │    0.230794 │     0.296369 │   0.37951 │  0.478876 │   0.616043 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_prod.shape, \"-->\", Yt_prod.shape)\nprint(Xv_prod.shape, \"-->\", Yv_prod.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_prod.shape[1],Xt_prod.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_prod)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_prod,\n    Yt_prod,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_prod, Yv_prod),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_prod = model.predict(Xt_prod)\nYvp_prod = model.predict(Xv_prod)\n\n# REPORT\nregression_report(Yt_prod, Ytp_prod, Yv_prod, Yvp_prod)\n\n# compute RMSE using numpy\ntraining_RMSE_prod = np.sqrt(mean_squared_error(Yt_prod, Ytp_prod))\nvalidation_RMSE_prod = np.sqrt(mean_squared_error(Yv_prod, Yvp_prod))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_9\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_9 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_9 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.01818179822574183\n MAE: 0.10452351690930663\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.08730496132160037\n MAE: 0.2596031001030938\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_prod_rnn_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN(L2 Reg)',\n               mean_squared_error(Yt_prod, Ytp_prod),\n               mean_absolute_error(Yt_prod, Ytp_prod),\n              training_RMSE_prod,\n               mean_squared_error(Yv_prod, Yvp_prod),\n               mean_absolute_error(Yv_prod, Yvp_prod),\n               validation_RMSE_prod]]\nprint(tabulate(error_table_prod_rnn_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN(L2 Reg) │   0.0181818 │    0.104524 │      0.13484 │  0.087305 │  0.259603 │   0.295474 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_imp.shape, \"-->\", Yt_imp.shape)\nprint(Xv_imp.shape, \"-->\", Yv_imp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_imp.shape[1],Xt_imp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_imp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_imp,\n    Yt_imp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_imp, Yv_imp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_imp = model.predict(Xt_imp)\nYvp_imp = model.predict(Xv_imp)\n\n# REPORT\nregression_report(Yt_imp, Ytp_imp, Yv_imp, Yvp_imp)\n\n# compute RMSE using numpy\ntraining_RMSE_imp = np.sqrt(mean_squared_error(Yt_imp, Ytp_imp))\nvalidation_RMSE_imp = np.sqrt(mean_squared_error(Yv_imp, Yvp_imp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_10\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_10 (SimpleRNN)   (None, 32)                1088      \n                                                                 \n dense_10 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 2ms/step\n3/3 [==============================] - 0s 4ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.12219420752197113\n MAE: 0.2650835268729244\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.13696709684218533\n MAE: 0.29662623805609023\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_imp_rnn_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN(L2 Reg)',\n               mean_squared_error(Yt_imp, Ytp_imp),\n               mean_absolute_error(Yt_imp, Ytp_imp),\n               training_RMSE_imp,\n               mean_squared_error(Yv_imp, Yvp_imp),\n               mean_absolute_error(Yv_imp, Yvp_imp),\n               validation_RMSE_imp]]\nprint(tabulate(error_table_imp_rnn_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN(L2 Reg) │    0.122194 │    0.265084 │     0.349563 │  0.136967 │  0.296626 │   0.370091 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_exp.shape, \"-->\", Yt_exp.shape)\nprint(Xv_exp.shape, \"-->\", Yv_exp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_exp.shape[1],Xt_exp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_exp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_exp,\n    Yt_exp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_exp, Yv_exp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_exp = model.predict(Xt_exp)\nYvp_exp = model.predict(Xv_exp)\n\n# REPORT\nregression_report(Yt_exp, Ytp_exp, Yv_exp, Yvp_exp)\n\n# compute RMSE using numpy\ntraining_RMSE_exp = np.sqrt(mean_squared_error(Yt_exp, Ytp_exp))\nvalidation_RMSE_exp = np.sqrt(mean_squared_error(Yv_exp, Yvp_exp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_11\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_11 (SimpleRNN)   (None, 32)                1088      \n                                                                 \n dense_11 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 3ms/step\n3/3 [==============================] - 0s 6ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.00453961182804625\n MAE: 0.0536077609618094\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.2263178960178174\n MAE: 0.9641191578617606\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_exp_rnn_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN(L2 Reg)',\n               mean_squared_error(Yt_exp, Ytp_exp),\n               mean_absolute_error(Yt_exp, Ytp_exp),\n               training_RMSE_exp,\n               mean_squared_error(Yv_exp, Yvp_exp),\n               mean_absolute_error(Yv_exp, Yvp_exp),\n               validation_RMSE_exp]]\nprint(tabulate(error_table_exp_rnn_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN(L2 Reg) │  0.00453961 │   0.0536078 │    0.0673766 │   1.22632 │  0.964119 │    1.10739 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_stk.shape, \"-->\", Yt_stk.shape)\nprint(Xv_stk.shape, \"-->\", Yv_stk.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_stk.shape[1],Xt_stk.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_stk)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_stk,\n    Yt_stk,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_stk, Yv_stk),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_stk = model.predict(Xt_stk)\nYvp_stk = model.predict(Xv_stk)\n\n# REPORT\nregression_report(Yt_stk, Ytp_stk, Yv_stk, Yvp_stk)\n\n# compute RMSE using numpy\ntraining_RMSE_stk = np.sqrt(mean_squared_error(Yt_stk, Ytp_stk))\nvalidation_RMSE_stk = np.sqrt(mean_squared_error(Yv_stk, Yvp_stk))\n\n\n(3377, 5, 1) --> (3377, 1)\n(1123, 5, 1) --> (1123, 1)\nModel: \"sequential_12\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_12 (SimpleRNN)   (None, 32)                1088      \n                                                                 \n dense_12 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n106/106 [==============================] - 0s 2ms/step\n36/36 [==============================] - 0s 2ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.001972599895865288\n MAE: 0.03555915816751766\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.012517007405626432\n MAE: 0.07993816860155734\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_stk_rnn_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN(L2 Reg)',\n               mean_squared_error(Yt_stk, Ytp_stk),\n               mean_absolute_error(Yt_stk, Ytp_stk),\n               training_RMSE_stk,\n               mean_squared_error(Yv_stk, Yvp_stk),\n               mean_absolute_error(Yv_stk, Yvp_stk),\n               validation_RMSE_stk]]\nprint(tabulate(error_table_stk_rnn_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN(L2 Reg) │   0.0019726 │   0.0355592 │     0.044414 │  0.012517 │ 0.0799382 │   0.111879 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_emissions.shape, \"-->\", Yt_emissions.shape)\nprint(Xv_emissions.shape, \"-->\", Yv_emissions.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_emissions.shape[1],Xt_emissions.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_emissions)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=False\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_emissions,\n    Yt_emissions,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_emissions, Yv_emissions),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_emissions = model.predict(Xt_emissions)\nYvp_emissions = model.predict(Xv_emissions)\n\n# REPORT\nregression_report(Yt_emissions, Ytp_emissions, Yv_emissions, Yvp_emissions)\n\n# compute RMSE using numpy\ntraining_RMSE_emissions = np.sqrt(mean_squared_error(Yt_emissions, Ytp_emissions))\nvalidation_RMSE_emissions = np.sqrt(mean_squared_error(Yv_emissions, Yvp_emissions))\n\n\n(227, 5, 1) --> (227, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_13\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_13 (SimpleRNN)   (None, 32)                1088      \n                                                                 \n dense_13 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 3ms/step\n3/3 [==============================] - 0s 4ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.2467168729013631\n MAE: 0.3427165033846242\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.0040270495300536\n MAE: 0.7742765141142125\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_emissions_rnn_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['RNN(L2 Reg)',\n               mean_squared_error(Yt_emissions, Ytp_emissions),\n               mean_absolute_error(Yt_emissions, Ytp_emissions),\n               training_RMSE_emissions,\n               mean_squared_error(Yv_emissions, Yvp_emissions),\n               mean_absolute_error(Yv_emissions, Yvp_emissions),\n               validation_RMSE_emissions]]\nprint(tabulate(error_table_emissions_rnn_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN(L2 Reg) │    0.246717 │    0.342717 │     0.496706 │   1.00403 │  0.774277 │    1.00201 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛"
  },
  {
    "objectID": "dl.html#gru",
    "href": "dl.html#gru",
    "title": "Deep Learning for TS",
    "section": "GRU",
    "text": "GRU\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt.shape, \"-->\", Yt.shape)\nprint(Xv.shape, \"-->\", Yv.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt,\n    Yt,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv, Yv),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_14\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru (GRU)                   (None, 32)                3360      \n                                                                 \n dense_14 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 1s 4ms/step\n2/2 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.09307459561413757\n MAE: 0.21239367581368906\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.1832772696117805\n MAE: 0.3187538903683385\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_gru = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU',\n               mean_squared_error(Yt, Ytp),\n               mean_absolute_error(Yt, Ytp),\n              training_RMSE,\n               mean_squared_error(Yv, Yvp),\n               mean_absolute_error(Yv, Yvp),\n               validation_RMSE]]\nprint(tabulate(error_table_gru, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU     │   0.0930746 │    0.212394 │     0.305081 │  0.183277 │  0.318754 │   0.428109 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_consump.shape, \"-->\", Yt_consump.shape)\nprint(Xv_consump.shape, \"-->\", Yv_consump.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_consump.shape[1],Xt_consump.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_consump)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_consump,\n    Yt_consump,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_consump, Yv_consump),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_consump = model.predict(Xt_consump)\nYvp_consump = model.predict(Xv_consump)\n\n# REPORT\nregression_report(Yt_consump, Ytp_consump, Yv_consump, Yvp_consump)\n\n# compute RMSE using numpy\ntraining_RMSE_consump = np.sqrt(mean_squared_error(Yt_consump, Ytp_consump))\nvalidation_RMSE_consump = np.sqrt(mean_squared_error(Yv_consump, Yvp_consump))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_15\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_1 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_15 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.23355664103124701\n MAE: 0.3685060510260475\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.5929653026508562\n MAE: 0.6548865234409081\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_consump_gru = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU',\n               mean_squared_error(Yt_consump, Ytp_consump),\n               mean_absolute_error(Yt_consump, Ytp_consump),\n              training_RMSE_consump,\n               mean_squared_error(Yv_consump, Yvp_consump),\n               mean_absolute_error(Yv_consump, Yvp_consump),\n               validation_RMSE_consump]]\nprint(tabulate(error_table_consump_gru, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU     │    0.233557 │    0.368506 │     0.483277 │  0.592965 │  0.654887 │   0.770042 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_prod.shape, \"-->\", Yt_prod.shape)\nprint(Xv_prod.shape, \"-->\", Yv_prod.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_prod.shape[1],Xt_prod.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_prod)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_prod,\n    Yt_prod,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_prod, Yv_prod),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_prod = model.predict(Xt_prod)\nYvp_prod = model.predict(Xv_prod)\n\n# REPORT\nregression_report(Yt_prod, Ytp_prod, Yv_prod, Yvp_prod)\n\n# compute RMSE using numpy\ntraining_RMSE_prod = np.sqrt(mean_squared_error(Yt_prod, Ytp_prod))\nvalidation_RMSE_prod = np.sqrt(mean_squared_error(Yv_prod, Yvp_prod))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_16\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_2 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_16 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.02292736260911077\n MAE: 0.11412029267993612\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.1303638047994717\n MAE: 0.32803943068324254\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_prod_gru = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU',\n               mean_squared_error(Yt_prod, Ytp_prod),\n               mean_absolute_error(Yt_prod, Ytp_prod),\n              training_RMSE_prod,\n               mean_squared_error(Yv_prod, Yvp_prod),\n               mean_absolute_error(Yv_prod, Yvp_prod),\n               validation_RMSE_prod]]\nprint(tabulate(error_table_prod_gru, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU     │   0.0229274 │     0.11412 │     0.151418 │  0.130364 │  0.328039 │   0.361059 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_imp.shape, \"-->\", Yt_imp.shape)\nprint(Xv_imp.shape, \"-->\", Yv_imp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_imp.shape[1],Xt_imp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_imp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_imp,\n    Yt_imp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_imp, Yv_imp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_imp = model.predict(Xt_imp)\nYvp_imp = model.predict(Xv_imp)\n\n# REPORT\nregression_report(Yt_imp, Ytp_imp, Yv_imp, Yvp_imp)\n\n# compute RMSE using numpy\ntraining_RMSE_imp = np.sqrt(mean_squared_error(Yt_imp, Ytp_imp))\nvalidation_RMSE_imp = np.sqrt(mean_squared_error(Yv_imp, Yvp_imp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_17\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_3 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_17 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 3ms/step\n3/3 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.19197940418155066\n MAE: 0.3450106572232673\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.2222956142443242\n MAE: 0.37296668297977176\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_imp_gru = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU',\n               mean_squared_error(Yt_imp, Ytp_imp),\n               mean_absolute_error(Yt_imp, Ytp_imp),\n               training_RMSE_imp,\n               mean_squared_error(Yv_imp, Yvp_imp),\n               mean_absolute_error(Yv_imp, Yvp_imp),\n               validation_RMSE_imp]]\nprint(tabulate(error_table_imp_gru, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU     │    0.191979 │    0.345011 │     0.438155 │  0.222296 │  0.372967 │   0.471482 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_exp.shape, \"-->\", Yt_exp.shape)\nprint(Xv_exp.shape, \"-->\", Yv_exp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_exp.shape[1],Xt_exp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_exp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_exp,\n    Yt_exp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_exp, Yv_exp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_exp = model.predict(Xt_exp)\nYvp_exp = model.predict(Xv_exp)\n\n# REPORT\nregression_report(Yt_exp, Ytp_exp, Yv_exp, Yvp_exp)\n\n# compute RMSE using numpy\ntraining_RMSE_exp = np.sqrt(mean_squared_error(Yt_exp, Ytp_exp))\nvalidation_RMSE_exp = np.sqrt(mean_squared_error(Yv_exp, Yvp_exp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_18\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_4 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_18 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 3ms/step\n3/3 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.005926264873573486\n MAE: 0.05846742314216646\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.8894577127439768\n MAE: 0.7667608707053157\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_exp_gru = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU',\n               mean_squared_error(Yt_exp, Ytp_exp),\n               mean_absolute_error(Yt_exp, Ytp_exp),\n               training_RMSE_exp,\n               mean_squared_error(Yv_exp, Yvp_exp),\n               mean_absolute_error(Yv_exp, Yvp_exp),\n               validation_RMSE_exp]]\nprint(tabulate(error_table_exp_gru, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU     │  0.00592626 │   0.0584674 │    0.0769822 │  0.889458 │  0.766761 │   0.943111 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_stk.shape, \"-->\", Yt_stk.shape)\nprint(Xv_stk.shape, \"-->\", Yv_stk.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_stk.shape[1],Xt_stk.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_stk)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_stk,\n    Yt_stk,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_stk, Yv_stk),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_stk = model.predict(Xt_stk)\nYvp_stk = model.predict(Xv_stk)\n\n# REPORT\nregression_report(Yt_stk, Ytp_stk, Yv_stk, Yvp_stk)\n\n# compute RMSE using numpy\ntraining_RMSE_stk = np.sqrt(mean_squared_error(Yt_stk, Ytp_stk))\nvalidation_RMSE_stk = np.sqrt(mean_squared_error(Yv_stk, Yvp_stk))\n\n\n(3377, 5, 1) --> (3377, 1)\n(1123, 5, 1) --> (1123, 1)\nModel: \"sequential_19\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_5 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_19 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n106/106 [==============================] - 1s 2ms/step\n36/36 [==============================] - 0s 3ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.0024811101772368917\n MAE: 0.03790346794311486\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.22407084458733548\n MAE: 0.2609358026053276\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_stk_gru = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU',\n               mean_squared_error(Yt_stk, Ytp_stk),\n               mean_absolute_error(Yt_stk, Ytp_stk),\n               training_RMSE_stk,\n               mean_squared_error(Yv_stk, Yvp_stk),\n               mean_absolute_error(Yv_stk, Yvp_stk),\n               validation_RMSE_stk]]\nprint(tabulate(error_table_stk_gru, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU     │  0.00248111 │   0.0379035 │    0.0498107 │  0.224071 │  0.260936 │   0.473361 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_emissions.shape, \"-->\", Yt_emissions.shape)\nprint(Xv_emissions.shape, \"-->\", Yv_emissions.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_emissions.shape[1],Xt_emissions.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_emissions)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_emissions,\n    Yt_emissions,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_emissions, Yv_emissions),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_emissions = model.predict(Xt_emissions)\nYvp_emissions = model.predict(Xv_emissions)\n\n# REPORT\nregression_report(Yt_emissions, Ytp_emissions, Yv_emissions, Yvp_emissions)\n\n# compute RMSE using numpy\ntraining_RMSE_emissions = np.sqrt(mean_squared_error(Yt_emissions, Ytp_emissions))\nvalidation_RMSE_emissions = np.sqrt(mean_squared_error(Yv_emissions, Yvp_emissions))\n\n\n(227, 5, 1) --> (227, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_20\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_6 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_20 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 1s 4ms/step\n3/3 [==============================] - 0s 8ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.826495245899882\n MAE: 0.4748150256824064\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.3218618767871289\n MAE: 0.4843349026939558\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_emissions_gru = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU',\n               mean_squared_error(Yt_emissions, Ytp_emissions),\n               mean_absolute_error(Yt_emissions, Ytp_emissions),\n               training_RMSE_emissions,\n               mean_squared_error(Yv_emissions, Yvp_emissions),\n               mean_absolute_error(Yv_emissions, Yvp_emissions),\n               validation_RMSE_emissions]]\nprint(tabulate(error_table_emissions_gru, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU     │    0.826495 │    0.474815 │     0.909118 │   1.32186 │  0.484335 │    1.14972 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛"
  },
  {
    "objectID": "dl.html#gru-with-l2-regularization",
    "href": "dl.html#gru-with-l2-regularization",
    "title": "Deep Learning for TS",
    "section": "GRU with L2 Regularization",
    "text": "GRU with L2 Regularization\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt.shape, \"-->\", Yt.shape)\nprint(Xv.shape, \"-->\", Yv.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt,\n    Yt,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv, Yv),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_21\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_7 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_21 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 6ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.09432739660797214\n MAE: 0.2136415814493819\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.1839333523380131\n MAE: 0.3036520450568859\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_gru_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU(L2 Reg)',\n               mean_squared_error(Yt, Ytp),\n               mean_absolute_error(Yt, Ytp),\n              training_RMSE,\n               mean_squared_error(Yv, Yvp),\n               mean_absolute_error(Yv, Yvp),\n               validation_RMSE]]\nprint(tabulate(error_table_gru_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU(L2 Reg) │   0.0943274 │    0.213642 │     0.307128 │  0.183933 │  0.303652 │   0.428875 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_consump.shape, \"-->\", Yt_consump.shape)\nprint(Xv_consump.shape, \"-->\", Yv_consump.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_consump.shape[1],Xt_consump.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_consump)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_consump,\n    Yt_consump,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_consump, Yv_consump),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_consump = model.predict(Xt_consump)\nYvp_consump = model.predict(Xv_consump)\n\n# REPORT\nregression_report(Yt_consump, Ytp_consump, Yv_consump, Yvp_consump)\n\n# compute RMSE using numpy\ntraining_RMSE_consump = np.sqrt(mean_squared_error(Yt_consump, Ytp_consump))\nvalidation_RMSE_consump = np.sqrt(mean_squared_error(Yv_consump, Yvp_consump))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_22\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_8 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_22 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.2995691196669474\n MAE: 0.4341264415337982\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.2107876513155427\n MAE: 0.8532101820645902\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_consump_gru_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU(L2 Reg)',\n               mean_squared_error(Yt_consump, Ytp_consump),\n               mean_absolute_error(Yt_consump, Ytp_consump),\n              training_RMSE_consump,\n               mean_squared_error(Yv_consump, Yvp_consump),\n               mean_absolute_error(Yv_consump, Yvp_consump),\n               validation_RMSE_consump]]\nprint(tabulate(error_table_consump_gru_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU(L2 Reg) │    0.299569 │    0.434126 │     0.547329 │   1.21079 │   0.85321 │    1.10036 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_prod.shape, \"-->\", Yt_prod.shape)\nprint(Xv_prod.shape, \"-->\", Yv_prod.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_prod.shape[1],Xt_prod.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_prod)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_prod,\n    Yt_prod,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_prod, Yv_prod),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_prod = model.predict(Xt_prod)\nYvp_prod = model.predict(Xv_prod)\n\n# REPORT\nregression_report(Yt_prod, Ytp_prod, Yv_prod, Yvp_prod)\n\n# compute RMSE using numpy\ntraining_RMSE_prod = np.sqrt(mean_squared_error(Yt_prod, Ytp_prod))\nvalidation_RMSE_prod = np.sqrt(mean_squared_error(Yv_prod, Yvp_prod))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_23\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_9 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_23 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 8ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.02248999120633133\n MAE: 0.1125636691296517\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.2939585365374952\n MAE: 0.49880377964020967\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_prod_gru_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU(L2 Reg)',\n               mean_squared_error(Yt_prod, Ytp_prod),\n               mean_absolute_error(Yt_prod, Ytp_prod),\n              training_RMSE_prod,\n               mean_squared_error(Yv_prod, Yvp_prod),\n               mean_absolute_error(Yv_prod, Yvp_prod),\n               validation_RMSE_prod]]\nprint(tabulate(error_table_prod_gru_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU(L2 Reg) │     0.02249 │    0.112564 │     0.149967 │  0.293959 │  0.498804 │   0.542179 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_imp.shape, \"-->\", Yt_imp.shape)\nprint(Xv_imp.shape, \"-->\", Yv_imp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_imp.shape[1],Xt_imp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_imp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_imp,\n    Yt_imp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_imp, Yv_imp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_imp = model.predict(Xt_imp)\nYvp_imp = model.predict(Xv_imp)\n\n# REPORT\nregression_report(Yt_imp, Ytp_imp, Yv_imp, Yvp_imp)\n\n# compute RMSE using numpy\ntraining_RMSE_imp = np.sqrt(mean_squared_error(Yt_imp, Ytp_imp))\nvalidation_RMSE_imp = np.sqrt(mean_squared_error(Yv_imp, Yvp_imp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_24\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_10 (GRU)                (None, 32)                3360      \n                                                                 \n dense_24 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 3ms/step\n3/3 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.18782569642597555\n MAE: 0.3343330888733994\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.22834866097912995\n MAE: 0.37341519140814167\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_imp_gru_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU(L2 Reg)',\n               mean_squared_error(Yt_imp, Ytp_imp),\n               mean_absolute_error(Yt_imp, Ytp_imp),\n               training_RMSE_imp,\n               mean_squared_error(Yv_imp, Yvp_imp),\n               mean_absolute_error(Yv_imp, Yvp_imp),\n               validation_RMSE_imp]]\nprint(tabulate(error_table_imp_gru_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU(L2 Reg) │    0.187826 │    0.334333 │     0.433389 │  0.228349 │  0.373415 │   0.477858 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_exp.shape, \"-->\", Yt_exp.shape)\nprint(Xv_exp.shape, \"-->\", Yv_exp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_exp.shape[1],Xt_exp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_exp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_exp,\n    Yt_exp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_exp, Yv_exp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_exp = model.predict(Xt_exp)\nYvp_exp = model.predict(Xv_exp)\n\n# REPORT\nregression_report(Yt_exp, Ytp_exp, Yv_exp, Yvp_exp)\n\n# compute RMSE using numpy\ntraining_RMSE_exp = np.sqrt(mean_squared_error(Yt_exp, Ytp_exp))\nvalidation_RMSE_exp = np.sqrt(mean_squared_error(Yv_exp, Yvp_exp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_25\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_11 (GRU)                (None, 32)                3360      \n                                                                 \n dense_25 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 4ms/step\n3/3 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.007668944289032949\n MAE: 0.06449698191446716\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 2.7026562417173494\n MAE: 1.4347720741135572\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_exp_gru_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU(L2 Reg)',\n               mean_squared_error(Yt_exp, Ytp_exp),\n               mean_absolute_error(Yt_exp, Ytp_exp),\n               training_RMSE_exp,\n               mean_squared_error(Yv_exp, Yvp_exp),\n               mean_absolute_error(Yv_exp, Yvp_exp),\n               validation_RMSE_exp]]\nprint(tabulate(error_table_exp_gru_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU(L2 Reg) │  0.00766894 │    0.064497 │    0.0875725 │   2.70266 │   1.43477 │    1.64398 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_stk.shape, \"-->\", Yt_stk.shape)\nprint(Xv_stk.shape, \"-->\", Yv_stk.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_stk.shape[1],Xt_stk.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_stk)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_stk,\n    Yt_stk,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_stk, Yv_stk),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_stk = model.predict(Xt_stk)\nYvp_stk = model.predict(Xv_stk)\n\n# REPORT\nregression_report(Yt_stk, Ytp_stk, Yv_stk, Yvp_stk)\n\n# compute RMSE using numpy\ntraining_RMSE_stk = np.sqrt(mean_squared_error(Yt_stk, Ytp_stk))\nvalidation_RMSE_stk = np.sqrt(mean_squared_error(Yv_stk, Yvp_stk))\n\n\n(3377, 5, 1) --> (3377, 1)\n(1123, 5, 1) --> (1123, 1)\nModel: \"sequential_26\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_12 (GRU)                (None, 32)                3360      \n                                                                 \n dense_26 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n106/106 [==============================] - 0s 2ms/step\n36/36 [==============================] - 0s 2ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.0027459609912201706\n MAE: 0.038920349685540014\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.044396008253210044\n MAE: 0.13976088675590678\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_stk_gru_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU(L2 Reg)',\n               mean_squared_error(Yt_stk, Ytp_stk),\n               mean_absolute_error(Yt_stk, Ytp_stk),\n               training_RMSE_stk,\n               mean_squared_error(Yv_stk, Yvp_stk),\n               mean_absolute_error(Yv_stk, Yvp_stk),\n               validation_RMSE_stk]]\nprint(tabulate(error_table_stk_gru_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU(L2 Reg) │  0.00274596 │   0.0389203 │    0.0524019 │  0.044396 │  0.139761 │   0.210704 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_emissions.shape, \"-->\", Yt_emissions.shape)\nprint(Xv_emissions.shape, \"-->\", Yv_emissions.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_emissions.shape[1],Xt_emissions.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_emissions)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_emissions,\n    Yt_emissions,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_emissions, Yv_emissions),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_emissions = model.predict(Xt_emissions)\nYvp_emissions = model.predict(Xv_emissions)\n\n# REPORT\nregression_report(Yt_emissions, Ytp_emissions, Yv_emissions, Yvp_emissions)\n\n# compute RMSE using numpy\ntraining_RMSE_emissions = np.sqrt(mean_squared_error(Yt_emissions, Ytp_emissions))\nvalidation_RMSE_emissions = np.sqrt(mean_squared_error(Yv_emissions, Yvp_emissions))\n\n\n(227, 5, 1) --> (227, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_27\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_13 (GRU)                (None, 32)                3360      \n                                                                 \n dense_27 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 4ms/step\n3/3 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.7672829841706863\n MAE: 0.44492241023064333\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.2503476954706598\n MAE: 0.44037885242740576\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_emissions_gru_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['GRU(L2 Reg)',\n               mean_squared_error(Yt_emissions, Ytp_emissions),\n               mean_absolute_error(Yt_emissions, Ytp_emissions),\n               training_RMSE_emissions,\n               mean_squared_error(Yv_emissions, Yvp_emissions),\n               mean_absolute_error(Yv_emissions, Yvp_emissions),\n               validation_RMSE_emissions]]\nprint(tabulate(error_table_emissions_gru_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model       │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ GRU(L2 Reg) │    0.767283 │    0.444922 │     0.875947 │   1.25035 │  0.440379 │    1.11819 │\n╘═════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛"
  },
  {
    "objectID": "dl.html#lstm",
    "href": "dl.html#lstm",
    "title": "Deep Learning for TS",
    "section": "LSTM",
    "text": "LSTM\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt.shape, \"-->\", Yt.shape)\nprint(Xv.shape, \"-->\", Yv.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt,\n    Yt,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv, Yv),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_28\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, 32)                4352      \n                                                                 \n dense_28 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 4ms/step\n2/2 [==============================] - 0s 6ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.09286728965267183\n MAE: 0.2154747499972604\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.20651359469214828\n MAE: 0.33671482100973527\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_lstm = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM',\n               mean_squared_error(Yt, Ytp),\n               mean_absolute_error(Yt, Ytp),\n              training_RMSE,\n               mean_squared_error(Yv, Yvp),\n               mean_absolute_error(Yv, Yvp),\n               validation_RMSE]]\nprint(tabulate(error_table_lstm, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM    │   0.0928673 │    0.215475 │     0.304741 │  0.206514 │  0.336715 │   0.454438 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_consump.shape, \"-->\", Yt_consump.shape)\nprint(Xv_consump.shape, \"-->\", Yv_consump.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_consump.shape[1],Xt_consump.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_consump)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_consump,\n    Yt_consump,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_consump, Yv_consump),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_consump = model.predict(Xt_consump)\nYvp_consump = model.predict(Xv_consump)\n\n# REPORT\nregression_report(Yt_consump, Ytp_consump, Yv_consump, Yvp_consump)\n\n# compute RMSE using numpy\ntraining_RMSE_consump = np.sqrt(mean_squared_error(Yt_consump, Ytp_consump))\nvalidation_RMSE_consump = np.sqrt(mean_squared_error(Yv_consump, Yvp_consump))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_29\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_1 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_29 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 6ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.2970955864641625\n MAE: 0.437726763197135\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.0437340892837863\n MAE: 0.8793948418476977\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_consump_lstm = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM',\n               mean_squared_error(Yt_consump, Ytp_consump),\n               mean_absolute_error(Yt_consump, Ytp_consump),\n              training_RMSE_consump,\n               mean_squared_error(Yv_consump, Yvp_consump),\n               mean_absolute_error(Yv_consump, Yvp_consump),\n               validation_RMSE_consump]]\nprint(tabulate(error_table_consump_lstm, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM    │    0.297096 │    0.437727 │     0.545065 │   1.04373 │  0.879395 │    1.02163 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_prod.shape, \"-->\", Yt_prod.shape)\nprint(Xv_prod.shape, \"-->\", Yv_prod.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_prod.shape[1],Xt_prod.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_prod)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_prod,\n    Yt_prod,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_prod, Yv_prod),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_prod = model.predict(Xt_prod)\nYvp_prod = model.predict(Xv_prod)\n\n# REPORT\nregression_report(Yt_prod, Ytp_prod, Yv_prod, Yvp_prod)\n\n# compute RMSE using numpy\ntraining_RMSE_prod = np.sqrt(mean_squared_error(Yt_prod, Ytp_prod))\nvalidation_RMSE_prod = np.sqrt(mean_squared_error(Yv_prod, Yvp_prod))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_30\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_2 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_30 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 3ms/step\n2/2 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.023363330218405497\n MAE: 0.11192040423436898\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.2358212359149498\n MAE: 0.4440431106812732\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_prod_lstm = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM',\n               mean_squared_error(Yt_prod, Ytp_prod),\n               mean_absolute_error(Yt_prod, Ytp_prod),\n              training_RMSE_prod,\n               mean_squared_error(Yv_prod, Yvp_prod),\n               mean_absolute_error(Yv_prod, Yvp_prod),\n               validation_RMSE_prod]]\nprint(tabulate(error_table_prod_lstm, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM    │   0.0233633 │     0.11192 │     0.152851 │  0.235821 │  0.444043 │   0.485614 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_imp.shape, \"-->\", Yt_imp.shape)\nprint(Xv_imp.shape, \"-->\", Yv_imp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_imp.shape[1],Xt_imp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_imp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_imp,\n    Yt_imp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_imp, Yv_imp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_imp = model.predict(Xt_imp)\nYvp_imp = model.predict(Xv_imp)\n\n# REPORT\nregression_report(Yt_imp, Ytp_imp, Yv_imp, Yvp_imp)\n\n# compute RMSE using numpy\ntraining_RMSE_imp = np.sqrt(mean_squared_error(Yt_imp, Ytp_imp))\nvalidation_RMSE_imp = np.sqrt(mean_squared_error(Yv_imp, Yvp_imp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_31\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_3 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_31 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 3ms/step\n3/3 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.19351637543097616\n MAE: 0.34250260930648363\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.21958086049481404\n MAE: 0.3749440361272188\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_imp_lstm = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM',\n               mean_squared_error(Yt_imp, Ytp_imp),\n               mean_absolute_error(Yt_imp, Ytp_imp),\n               training_RMSE_imp,\n               mean_squared_error(Yv_imp, Yvp_imp),\n               mean_absolute_error(Yv_imp, Yvp_imp),\n               validation_RMSE_imp]]\nprint(tabulate(error_table_imp_lstm, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM    │    0.193516 │    0.342503 │     0.439905 │  0.219581 │  0.374944 │   0.468595 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_imp.shape, \"-->\", Yt_imp.shape)\nprint(Xv_imp.shape, \"-->\", Yv_imp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_imp.shape[1],Xt_imp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_imp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_imp,\n    Yt_imp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_imp, Yv_imp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_imp = model.predict(Xt_imp)\nYvp_imp = model.predict(Xv_imp)\n\n# REPORT\nregression_report(Yt_imp, Ytp_imp, Yv_imp, Yvp_imp)\n\n# compute RMSE using numpy\ntraining_RMSE_imp = np.sqrt(mean_squared_error(Yt_imp, Ytp_imp))\nvalidation_RMSE_imp = np.sqrt(mean_squared_error(Yv_imp, Yvp_imp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_32\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_4 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_32 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 1s 4ms/step\n3/3 [==============================] - 0s 9ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.19189830266937993\n MAE: 0.34543021324045875\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.220367624040603\n MAE: 0.3758999369201629\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_exp_lstm = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM',\n               mean_squared_error(Yt_exp, Ytp_exp),\n               mean_absolute_error(Yt_exp, Ytp_exp),\n               training_RMSE_exp,\n               mean_squared_error(Yv_exp, Yvp_exp),\n               mean_absolute_error(Yv_exp, Yvp_exp),\n               validation_RMSE_exp]]\nprint(tabulate(error_table_exp_lstm, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM    │  0.00766894 │    0.064497 │    0.0875725 │   2.70266 │   1.43477 │    1.64398 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_stk.shape, \"-->\", Yt_stk.shape)\nprint(Xv_stk.shape, \"-->\", Yv_stk.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_stk.shape[1],Xt_stk.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_stk)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_stk,\n    Yt_stk,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_stk, Yv_stk),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_stk = model.predict(Xt_stk)\nYvp_stk = model.predict(Xv_stk)\n\n# REPORT\nregression_report(Yt_stk, Ytp_stk, Yv_stk, Yvp_stk)\n\n# compute RMSE using numpy\ntraining_RMSE_stk = np.sqrt(mean_squared_error(Yt_stk, Ytp_stk))\nvalidation_RMSE_stk = np.sqrt(mean_squared_error(Yv_stk, Yvp_stk))\n\n\n(3377, 5, 1) --> (3377, 1)\n(1123, 5, 1) --> (1123, 1)\nModel: \"sequential_33\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_5 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_33 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n106/106 [==============================] - 1s 4ms/step\n36/36 [==============================] - 0s 3ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.002367399844141305\n MAE: 0.03738482228573484\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.3444546925410365\n MAE: 0.3127807659043008\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_stk_lstm = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM',\n               mean_squared_error(Yt_stk, Ytp_stk),\n               mean_absolute_error(Yt_stk, Ytp_stk),\n               training_RMSE_stk,\n               mean_squared_error(Yv_stk, Yvp_stk),\n               mean_absolute_error(Yv_stk, Yvp_stk),\n               validation_RMSE_stk]]\nprint(tabulate(error_table_stk_lstm, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM    │   0.0023674 │   0.0373848 │    0.0486559 │  0.344455 │  0.312781 │   0.586903 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_emissions.shape, \"-->\", Yt_emissions.shape)\nprint(Xv_emissions.shape, \"-->\", Yv_emissions.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt_emissions.shape[1],Xt_emissions.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_emissions)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\n# recurrent_regularizer=regularizers.L2(L2),\n# activation='relu'\n)\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_emissions,\n    Yt_emissions,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_emissions, Yv_emissions),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_emissions = model.predict(Xt_emissions)\nYvp_emissions = model.predict(Xv_emissions)\n\n# REPORT\nregression_report(Yt_emissions, Ytp_emissions, Yv_emissions, Yvp_emissions)\n\n# compute RMSE using numpy\ntraining_RMSE_emissions = np.sqrt(mean_squared_error(Yt_emissions, Ytp_emissions))\nvalidation_RMSE_emissions = np.sqrt(mean_squared_error(Yv_emissions, Yvp_emissions))\n\n\n(227, 5, 1) --> (227, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_34\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_6 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_34 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 1s 4ms/step\n3/3 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.8239359829777397\n MAE: 0.47986576628663985\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.3089513655204494\n MAE: 0.49688676717475977\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_emissions_lstm = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM',\n               mean_squared_error(Yt_emissions, Ytp_emissions),\n               mean_absolute_error(Yt_emissions, Ytp_emissions),\n               training_RMSE_emissions,\n               mean_squared_error(Yv_emissions, Yvp_emissions),\n               mean_absolute_error(Yv_emissions, Yvp_emissions),\n               validation_RMSE_emissions]]\nprint(tabulate(error_table_emissions_lstm, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒═════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model   │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞═════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM    │    0.823936 │    0.479866 │     0.907709 │   1.30895 │  0.496887 │    1.14409 │\n╘═════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛"
  },
  {
    "objectID": "dl.html#lstm-with-l2-regularization",
    "href": "dl.html#lstm-with-l2-regularization",
    "title": "Deep Learning for TS",
    "section": "LSTM with L2 Regularization",
    "text": "LSTM with L2 Regularization\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt.shape, \"-->\", Yt.shape)\nprint(Xv.shape, \"-->\", Yv.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt,\n    Yt,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv, Yv),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp = model.predict(Xt)\nYvp = model.predict(Xv)\n\n# REPORT\nregression_report(Yt, Ytp, Yv, Yvp)\n\n# compute RMSE using numpy\ntraining_RMSE = np.sqrt(mean_squared_error(Yt, Ytp))\nvalidation_RMSE = np.sqrt(mean_squared_error(Yv, Yvp))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_35\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_7 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_35 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 4ms/step\n2/2 [==============================] - 0s 10ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.09697849098699898\n MAE: 0.2159713205718824\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.18647708992320702\n MAE: 0.3025485515171489\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_lstm_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM(L2 Reg)',\n               mean_squared_error(Yt, Ytp),\n               mean_absolute_error(Yt, Ytp),\n               training_RMSE,\n               mean_squared_error(Yv, Yvp),\n               mean_absolute_error(Yv, Yvp),\n               validation_RMSE]]\nprint(tabulate(error_table_lstm_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM(L2 Reg) │   0.0969785 │    0.215971 │     0.311414 │  0.186477 │  0.302549 │    0.43183 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_consump.shape, \"-->\", Yt_consump.shape)\nprint(Xv_consump.shape, \"-->\", Yv_consump.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_consump.shape[1],Xt_consump.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_consump)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_consump,\n    Yt_consump,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_consump, Yv_consump),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_consump = model.predict(Xt_consump)\nYvp_consump = model.predict(Xv_consump)\n\n# REPORT\nregression_report(Yt_consump, Ytp_consump, Yv_consump, Yvp_consump)\n\n# compute RMSE using numpy\ntraining_RMSE_consump = np.sqrt(mean_squared_error(Yt_consump, Ytp_consump))\nvalidation_RMSE_consump = np.sqrt(mean_squared_error(Yv_consump, Yvp_consump))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_36\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_8 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_36 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 4ms/step\n2/2 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.2747615144003193\n MAE: 0.4093043864536077\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.2416227791187402\n MAE: 0.896378661044941\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_consump_lstm_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM(L2 Reg)',\n               mean_squared_error(Yt_consump, Ytp_consump),\n               mean_absolute_error(Yt_consump, Ytp_consump),\n              training_RMSE_consump,\n               mean_squared_error(Yv_consump, Yvp_consump),\n               mean_absolute_error(Yv_consump, Yvp_consump),\n               validation_RMSE_consump]]\nprint(tabulate(error_table_consump_lstm_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM(L2 Reg) │    0.274762 │    0.409304 │     0.524177 │   1.24162 │  0.896379 │    1.11428 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_prod.shape, \"-->\", Yt_prod.shape)\nprint(Xv_prod.shape, \"-->\", Yv_prod.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_prod.shape[1],Xt_prod.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_prod)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_prod,\n    Yt_prod,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_prod, Yv_prod),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_prod = model.predict(Xt_prod)\nYvp_prod = model.predict(Xv_prod)\n\n# REPORT\nregression_report(Yt_prod, Ytp_prod, Yv_prod, Yvp_prod)\n\n# compute RMSE using numpy\ntraining_RMSE_prod = np.sqrt(mean_squared_error(Yt_prod, Ytp_prod))\nvalidation_RMSE_prod = np.sqrt(mean_squared_error(Yv_prod, Yvp_prod))\n\n\n(157, 5, 1) --> (157, 1)\n(49, 5, 1) --> (49, 1)\nModel: \"sequential_37\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_9 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_37 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n5/5 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 11ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.02313670990047248\n MAE: 0.11270792557147112\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.09841399937359264\n MAE: 0.28298587442587786\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_prod_lstm_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM(L2 Reg)',\n               mean_squared_error(Yt_prod, Ytp_prod),\n               mean_absolute_error(Yt_prod, Ytp_prod),\n              training_RMSE_prod,\n               mean_squared_error(Yv_prod, Yvp_prod),\n               mean_absolute_error(Yv_prod, Yvp_prod),\n               validation_RMSE_prod]]\nprint(tabulate(error_table_prod_lstm_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM(L2 Reg) │   0.0231367 │    0.112708 │     0.152108 │  0.098414 │  0.282986 │    0.31371 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_imp.shape, \"-->\", Yt_imp.shape)\nprint(Xv_imp.shape, \"-->\", Yv_imp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_imp.shape[1],Xt_imp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_imp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_imp,\n    Yt_imp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_imp, Yv_imp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_imp = model.predict(Xt_imp)\nYvp_imp = model.predict(Xv_imp)\n\n# REPORT\nregression_report(Yt_imp, Ytp_imp, Yv_imp, Yvp_imp)\n\n# compute RMSE using numpy\ntraining_RMSE_imp = np.sqrt(mean_squared_error(Yt_imp, Ytp_imp))\nvalidation_RMSE_imp = np.sqrt(mean_squared_error(Yv_imp, Yvp_imp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_38\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_10 (LSTM)              (None, 32)                4352      \n                                                                 \n dense_38 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 4ms/step\n3/3 [==============================] - 0s 7ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.1936177343659908\n MAE: 0.3472881397126535\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.22782918880664482\n MAE: 0.37552213994911376\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_imp_lstm_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM(L2 Reg)',\n               mean_squared_error(Yt_imp, Ytp_imp),\n               mean_absolute_error(Yt_imp, Ytp_imp),\n               training_RMSE_imp,\n               mean_squared_error(Yv_imp, Yvp_imp),\n               mean_absolute_error(Yv_imp, Yvp_imp),\n               validation_RMSE_imp]]\nprint(tabulate(error_table_imp_lstm_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM(L2 Reg) │    0.193618 │    0.347288 │      0.44002 │  0.227829 │  0.375522 │   0.477315 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_exp.shape, \"-->\", Yt_exp.shape)\nprint(Xv_exp.shape, \"-->\", Yv_exp.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_exp.shape[1],Xt_exp.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_exp)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_exp,\n    Yt_exp,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_exp, Yv_exp),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_exp = model.predict(Xt_exp)\nYvp_exp = model.predict(Xv_exp)\n\n# REPORT\nregression_report(Yt_exp, Ytp_exp, Yv_exp, Yvp_exp)\n\n# compute RMSE using numpy\ntraining_RMSE_exp = np.sqrt(mean_squared_error(Yt_exp, Ytp_exp))\nvalidation_RMSE_exp = np.sqrt(mean_squared_error(Yv_exp, Yvp_exp))\n\n\n(229, 5, 1) --> (229, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_39\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_11 (LSTM)              (None, 32)                4352      \n                                                                 \n dense_39 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 3ms/step\n3/3 [==============================] - 0s 4ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.006405860351627613\n MAE: 0.05985329218919866\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 2.236326883072915\n MAE: 1.305820683189038\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_exp_lstm_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM(L2 Reg)',\n               mean_squared_error(Yt_exp, Ytp_exp),\n               mean_absolute_error(Yt_exp, Ytp_exp),\n               training_RMSE_exp,\n               mean_squared_error(Yv_exp, Yvp_exp),\n               mean_absolute_error(Yv_exp, Yvp_exp),\n               validation_RMSE_exp]]\nprint(tabulate(error_table_exp_lstm_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM(L2 Reg) │  0.00640586 │   0.0598533 │    0.0800366 │   2.23633 │   1.30582 │    1.49544 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_stk.shape, \"-->\", Yt_stk.shape)\nprint(Xv_stk.shape, \"-->\", Yv_stk.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=1e-4\ninput_shape=(Xt_stk.shape[1],Xt_stk.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_stk)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_stk,\n    Yt_stk,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_stk, Yv_stk),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_stk = model.predict(Xt_stk)\nYvp_stk = model.predict(Xv_stk)\n\n# REPORT\nregression_report(Yt_stk, Ytp_stk, Yv_stk, Yvp_stk)\n\n# compute RMSE using numpy\ntraining_RMSE_stk = np.sqrt(mean_squared_error(Yt_stk, Ytp_stk))\nvalidation_RMSE_stk = np.sqrt(mean_squared_error(Yv_stk, Yvp_stk))\n\n\n(3377, 5, 1) --> (3377, 1)\n(1123, 5, 1) --> (1123, 1)\nModel: \"sequential_40\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_12 (LSTM)              (None, 32)                4352      \n                                                                 \n dense_40 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n106/106 [==============================] - 0s 2ms/step\n36/36 [==============================] - 0s 2ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.002091984399747166\n MAE: 0.03466316595481215\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 0.373875263482317\n MAE: 0.2811309082041175\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_stk_lstm_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM(L2 Reg)',\n               mean_squared_error(Yt_stk, Ytp_stk),\n               mean_absolute_error(Yt_stk, Ytp_stk),\n               training_RMSE_stk,\n               mean_squared_error(Yv_stk, Yvp_stk),\n               mean_absolute_error(Yv_stk, Yvp_stk),\n               validation_RMSE_stk]]\nprint(tabulate(error_table_stk_lstm_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM(L2 Reg) │  0.00209198 │   0.0346632 │    0.0457382 │  0.373875 │  0.281131 │   0.611453 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\n\n\n\n\nView Code\n# NEW SIZES\nprint(Xt_emissions.shape, \"-->\", Yt_emissions.shape)\nprint(Xv_emissions.shape, \"-->\", Yv_emissions.shape)\n\n# HYPERPARAMETERS\noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\"\nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2= 1e-4\ninput_shape=(Xt_emissions.shape[1],Xt_emissions.shape[2])\n\n\n# ------ Choose the batch size ------\n# batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_emissions)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n# model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape,\n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          )\n\n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR\nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n# # COMPILING THE MODEL\nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(\n    Xt_emissions,\n    Yt_emissions,\n    epochs=numbers_epochs,\n    batch_size=batch_size,\n    verbose=False,\n    validation_data=(Xv_emissions, Yv_emissions),\n)\n# History plot\nhistory_plot(history)\n\n# Predictions\nYtp_emissions = model.predict(Xt_emissions)\nYvp_emissions = model.predict(Xv_emissions)\n\n# REPORT\nregression_report(Yt_emissions, Ytp_emissions, Yv_emissions, Yvp_emissions)\n\n# compute RMSE using numpy\ntraining_RMSE_emissions = np.sqrt(mean_squared_error(Yt_emissions, Ytp_emissions))\nvalidation_RMSE_emissions = np.sqrt(mean_squared_error(Yv_emissions, Yvp_emissions))\n\n\n(227, 5, 1) --> (227, 1)\n(73, 5, 1) --> (73, 1)\nModel: \"sequential_41\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_13 (LSTM)              (None, 32)                4352      \n                                                                 \n dense_41 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n\n\n\n\n\n8/8 [==============================] - 0s 5ms/step\n3/3 [==============================] - 0s 6ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.7241078736992722\n MAE: 0.4202230640627933\n\n\n\n\n\n\n\n\nVALIDATION:\n MSE: 1.17440960957364\n MAE: 0.5130328755168816\n\n\n\n\n\n\n\n\nError Summary\n\n\nView Code\nerror_table_emissions_lstm_l2 = [['Model','train_MSE','train_MAE','train_RMSE','val_MSE','val_MAE','val_RMSE'],\n              ['LSTM(L2 Reg)',\n               mean_squared_error(Yt_emissions, Ytp_emissions),\n               mean_absolute_error(Yt_emissions, Ytp_emissions),\n               training_RMSE_emissions,\n               mean_squared_error(Yv_emissions, Yvp_emissions),\n               mean_absolute_error(Yv_emissions, Yvp_emissions),\n               validation_RMSE_emissions]]\nprint(tabulate(error_table_emissions_lstm_l2, headers='firstrow', tablefmt='fancy_grid'))\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ LSTM(L2 Reg) │    0.724108 │    0.420223 │     0.850945 │   1.17441 │  0.513033 │     1.0837 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛"
  },
  {
    "objectID": "dl.html#discussion",
    "href": "dl.html#discussion",
    "title": "Deep Learning for TS",
    "section": "Discussion",
    "text": "Discussion\n\nANN Model Comparison\nQuestion 1: How do the results from the 3 different ANN models compare with each other in terms of accuracy and predictive power?\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │   0.0856424 │    0.215367 │     0.292647 │  0.150874 │  0.289132 │   0.388425 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │   0.0969527 │    0.198325 │     0.311372 │  0.147882 │  0.25337  │   0.384555 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │   0.0930746 │    0.212394 │     0.305081 │  0.183277 │  0.318754 │   0.428109 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │   0.0943274 │    0.213642 │     0.307128 │  0.183933 │  0.303652 │   0.428875 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │   0.0928673 │    0.215475 │     0.304741 │  0.206514 │  0.336715 │   0.454438 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │   0.0969785 │    0.215971 │     0.311414 │  0.186477 │  0.302549 │   0.43183  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nThe result shows that the RNNs generally perform better than GRU and LSTM models in terms of training and validation MSE, MAE, and RMSE. This suggests that RNNs are more accurate in fitting the training data and also generalize better to validation data. Introducing L2 regularization tends to slightly raise training MSE across all models, indicating a balance between model complexity and accuracy. However, it often leads to improved generalization, as evidenced by lower or comparable validation errors, particularly in GRU and LSTM models.\n\n\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │   0.123641  │    0.275929 │     0.351626 │  0.422054 │  0.522695 │   0.649657 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │   0.0878347 │    0.230794 │     0.296369 │  0.37951  │  0.478876 │   0.616043 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │   0.233557  │    0.368506 │     0.483277 │  0.592965 │  0.654887 │   0.770042 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │   0.299569  │    0.434126 │     0.547329 │  1.21079  │  0.85321  │   1.10036  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │   0.297096  │    0.437727 │     0.545065 │  1.04373  │  0.879395 │   1.02163  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │   0.274762  │    0.409304 │     0.524177 │  1.24162  │  0.896379 │   1.11428  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nThe result shows that RNNs, especially with L2 regularization, perform best with the lowest error metrics, suggesting higher accuracy and generalization. In contrast, GRU and LSTM models, particularly with regularization, exhibit higher errors, indicating less effective predictions and potential overfitting, especially in complex models where they fail to capture underlying patterns as efficiently.\n\n\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │   0.0227048 │    0.118012 │     0.150681 │  0.091456 │  0.2621   │   0.302417 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │   0.0181818 │    0.104524 │     0.13484  │  0.087305 │  0.259603 │   0.295474 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │   0.0229274 │    0.11412  │     0.151418 │  0.130364 │  0.328039 │   0.361059 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │   0.02249   │    0.112564 │     0.149967 │  0.293959 │  0.498804 │   0.542179 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │   0.0233633 │    0.11192  │     0.152851 │  0.235821 │  0.444043 │   0.485614 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │   0.0231367 │    0.112708 │     0.152108 │  0.098414 │  0.282986 │   0.31371  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nThe results show that the L2 regularized RNN outperform other models, having the lowest training and validation MSE and MAE, indicating they generalize better on unseen data. Conversely, GRUs and LSTMs, especially with L2 regularization, show higher error rates in validation, suggesting potential overfitting or less effectiveness in capturing complex patterns. Particularly, regularized LSTMs and GRUs show considerable increases in validation errors compared to their training performance, highlighting challenges in model stability across different data subsets.\n\n\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │    0.156048 │    0.31658  │     0.395029 │  0.185671 │  0.359929 │   0.430895 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │    0.122194 │    0.265084 │     0.349563 │  0.136967 │  0.296626 │   0.370091 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │    0.191979 │    0.345011 │     0.438155 │  0.222296 │  0.372967 │   0.471482 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │    0.187826 │    0.334333 │     0.433389 │  0.228349 │  0.373415 │   0.477858 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │    0.193516 │    0.342503 │     0.439905 │  0.219581 │  0.374944 │   0.468595 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │    0.193618 │    0.347288 │     0.44002  │  0.227829 │  0.375522 │   0.477315 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nThe results reveals that the RNN with L2 regularization shows superior performance, as they have lowest training and validation errors, which suggests better generalization on unseen data. In contrast, the GRU and LSTM models, particularly with L2 regularization, shows higher error, indicating potential overfitting. The regularized RNN model emerges as the most effective in managing overfitting and achieving higher predictive accuracy.\n\n\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │  0.00535037 │   0.0551332 │    0.0731462 │  0.758507 │  0.640908 │   0.870923 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │  0.00453961 │   0.0536078 │    0.0673766 │  1.22632  │  0.964119 │   1.10739  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │  0.00592626 │   0.0584674 │    0.0769822 │  0.889458 │  0.766761 │   0.943111 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │  0.00766894 │   0.064497  │    0.0875725 │  2.70266  │  1.43477  │   1.64398  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │  0.00766894 │   0.064497  │    0.0875725 │  2.70266  │  1.43477  │   1.64398  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │  0.00640586 │   0.0598533 │    0.0800366 │  2.23633  │  1.30582  │   1.49544  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nThe RNN and GRU models without regularization outperform on the training set but show higher errors on validation data, suggesting overfitting. L2 Regularized RNN and GRU, and both LSTM models (with and without regularization) show less effective training and prediction capabilities with higher error rates across both sets, indicating potential issues in balancing bias and variance.\n\n\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │  0.00385925 │   0.0493704 │    0.0621229 │  0.241482 │ 0.252935  │   0.491409 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │  0.0019726  │   0.0355592 │    0.044414  │  0.012517 │ 0.0799382 │   0.111879 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │  0.00248111 │   0.0379035 │    0.0498107 │  0.224071 │ 0.260936  │   0.473361 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │  0.00274596 │   0.0389203 │    0.0524019 │  0.044396 │ 0.139761  │   0.210704 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │  0.0023674  │   0.0373848 │    0.0486559 │  0.344455 │ 0.312781  │   0.586903 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │  0.00209198 │   0.0346632 │    0.0457382 │  0.373875 │ 0.281131  │   0.611453 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nThe RNN and GRU models without regularization show lower training errors but higher validation errors, suggesting potential overfitting. In contrast, the L2 regularized models, especially RNN, significantly improve in validation, indicating effective mitigation against overfitting. The LSTM models, regardless of regularization, while maintaining competitive training errors, do not perform as well in validation, indicating less effective generalization.\n\n\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │    0.30333  │    0.348757 │     0.550754 │   1.51433 │  0.968769 │    1.23058 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │    0.246717 │    0.342717 │     0.496706 │   1.00403 │  0.774277 │    1.00201 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │    0.826495 │    0.474815 │     0.909118 │   1.32186 │  0.484335 │    1.14972 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │    0.767283 │    0.444922 │     0.875947 │   1.25035 │  0.440379 │    1.11819 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │    0.823936 │    0.479866 │     0.907709 │   1.30895 │  0.496887 │    1.14409 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │    0.724108 │    0.420223 │     0.850945 │   1.17441 │  0.513033 │    1.0837  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nAcross the models, the RNN and LSTM with L2 regularization show the best improvement in validation scores, indicating better generalization compared to their non-regularized counterparts. This suggests that regularization helps manage overfitting, thus enhancing the models’ predictive performance on new data. Notably, all models exhibit higher errors on validation data, which could suggest either overfitting on training data or underfitting to complex patterns not captured during training. This highlights the importance of model tuning and possibly more complex architectures or training procedures to better capture underlying data patterns.\n\n\n\nQuestion 2 What effect does including regularization have on your results?\nI used L2 regularization in all 3 ANN models because it prevents overfitting by adding a penalty proportional to the square of the model coefficients, promoting smaller weights. This technique helps improve the model’s generalization on unseen data particularly in scenarios where the model is complex or the training data is noisy, which is the most common challenges for natural gas dynamics and CO2 emissions datasets.\nIncluding regularization in the above models improved validation metrics. For instance, in the natural gas prices dataset, the RNN with L2 regularization shows improved validation MSE and RMSE compared to the standard RNN, indicating it handles overfitting more effectively. But if we can see a notable increase in RMSE on the validation set compared to the training set across all datasets, suggesting that overfitting remains an issue even though regularization tries to mitigate it.\nQuestion 3 How far into the future can the deep learning model accurately predict the future?\nDeep learning models can accurately predict the near future based on factors like the dataset’s nature, the model’s complexity, and the forecast duration. These models are particularly effective in making short to medium-term predictions. However, their accuracy diminishes over longer periods. For datasets with variables like natural gas prices, consumption, and emissions—which are subject to numerous external influences and inherent volatility—predictions remain reliably precise for a short span, typically from a few days to a couple of months. Beyond this range, the predictions become increasingly uncertain and require frequent updates and adjustments to maintain accuracy.\n\n\nANN vs. Traditional Univariate TS Models\n\nPricesConsumptionProductionImportsExportsCVX StocksCO2 Emissions\n\n\nAnn Models Error Summary\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │   0.0856424 │    0.215367 │     0.292647 │  0.150874 │  0.289132 │   0.388425 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │   0.0969527 │    0.198325 │     0.311372 │  0.147882 │  0.25337  │   0.384555 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │   0.0930746 │    0.212394 │     0.305081 │  0.183277 │  0.318754 │   0.428109 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │   0.0943274 │    0.213642 │     0.307128 │  0.183933 │  0.303652 │   0.428875 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │   0.0928673 │    0.215475 │     0.304741 │  0.206514 │  0.336715 │   0.454438 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │   0.0969785 │    0.215971 │     0.311414 │  0.186477 │  0.302549 │   0.43183  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nTraditional Univariate Models Error Summary\n\n\n╒═════════╤═════════════╤══════════╤══════════╤══════════╤═════════╤══════════╤═════════════╕\n│ Model   │          ME │     RMSE │      MAE │      MPE │    MAPE │     MASE │        ACF1 │\n╞═════════╪═════════════╪══════════╪══════════╪══════════╪═════════╪══════════╪═════════════╡\n│ ARIMA   │ -0.00927973 │ 0.758939 │ 0.512161 │ -2.63271 │ 11.3002 │ 0.362227 │  0.00500843 │\n├─────────┼─────────────┼──────────┼──────────┼──────────┼─────────┼──────────┼─────────────┤\n│ SARIMA  │ -0.00818136 │ 0.79182  │ 0.522379 │ -1.36221 │ 11.2339 │ 0.369454 │ -0.00177239 │\n╘═════════╧═════════════╧══════════╧══════════╧══════════╧═════════╧══════════╧═════════════╛\n\n\nRMSE Comparison\n\n\n╒══════════════╤══════════╕\n│ Model        │     RMSE │\n╞══════════════╪══════════╡\n│ RNN          │ 0.292647 │\n├──────────────┼──────────┤\n│ RNN(L2 Reg)  │ 0.311372 │\n├──────────────┼──────────┤\n│ GRU          │ 0.305081 │\n├──────────────┼──────────┤\n│ GRU(L2 Reg)  │ 0.307128 │\n├──────────────┼──────────┤\n│ LSTM         │ 0.304741 │\n├──────────────┼──────────┤\n│ LSTM(L2 Reg) │ 0.311414 │\n├──────────────┼──────────┤\n│ ARIMA        │ 0.758939 │\n├──────────────┼──────────┤\n│ SARIMA       │ 0.79182  │\n╘══════════════╧══════════╛\n\n\n\n\nAnn Models Error Summary\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │   0.123641  │    0.275929 │     0.351626 │  0.422054 │  0.522695 │   0.649657 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │   0.0878347 │    0.230794 │     0.296369 │  0.37951  │  0.478876 │   0.616043 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │   0.233557  │    0.368506 │     0.483277 │  0.592965 │  0.654887 │   0.770042 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │   0.299569  │    0.434126 │     0.547329 │  1.21079  │  0.85321  │   1.10036  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │   0.297096  │    0.437727 │     0.545065 │  1.04373  │  0.879395 │   1.02163  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │   0.274762  │    0.409304 │     0.524177 │  1.24162  │  0.896379 │   1.11428  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nTraditional Univariate Models Error Summary\n\n\n╒═════════╤═════════╤══════════╤══════════╤══════════╤═════════╤══════════╤═════════════╕\n│ Model   │      ME │     RMSE │      MAE │      MPE │    MAPE │     MASE │        ACF1 │\n╞═════════╪═════════╪══════════╪══════════╪══════════╪═════════╪══════════╪═════════════╡\n│ ARIMA   │ 26124.5 │ 233048   │ 179130   │ 0.199796 │ 7.93188 │ 1.53233  │  0.00778866 │\n├─────────┼─────────┼──────────┼──────────┼──────────┼─────────┼──────────┼─────────────┤\n│ SARIMA  │ 10818.3 │  98342.5 │  73926.5 │ 0.348918 │ 3.23593 │ 0.632388 │ -0.00785298 │\n╘═════════╧═════════╧══════════╧══════════╧══════════╧═════════╧══════════╧═════════════╛\n\n\nRMSE Comparison\n\n\n╒══════════════╤═══════════════╕\n│ Model        │          RMSE │\n╞══════════════╪═══════════════╡\n│ RNN          │      0.351626 │\n├──────────────┼───────────────┤\n│ RNN(L2 Reg)  │      0.296369 │\n├──────────────┼───────────────┤\n│ GRU          │      0.483277 │\n├──────────────┼───────────────┤\n│ GRU(L2 Reg)  │      0.547329 │\n├──────────────┼───────────────┤\n│ LSTM         │      0.545065 │\n├──────────────┼───────────────┤\n│ LSTM(L2 Reg) │      0.524177 │\n├──────────────┼───────────────┤\n│ ARIMA        │ 233048        │\n├──────────────┼───────────────┤\n│ SARIMA       │  98342.5      │\n╘══════════════╧═══════════════╛\n\n\n\n\nAnn Models Error Summary\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │   0.0227048 │    0.118012 │     0.150681 │  0.091456 │  0.2621   │   0.302417 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │   0.0181818 │    0.104524 │     0.13484  │  0.087305 │  0.259603 │   0.295474 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │   0.0229274 │    0.11412  │     0.151418 │  0.130364 │  0.328039 │   0.361059 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │   0.02249   │    0.112564 │     0.149967 │  0.293959 │  0.498804 │   0.542179 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │   0.0233633 │    0.11192  │     0.152851 │  0.235821 │  0.444043 │   0.485614 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │   0.0231367 │    0.112708 │     0.152108 │  0.098414 │  0.282986 │   0.31371  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nTraditional Univariate Models Error Summary\n\n\n╒═════════╤══════════╤═════════╤═════════╤══════════╤═════════╤══════════╤════════════╕\n│ Model   │       ME │    RMSE │     MAE │      MPE │    MAPE │     MASE │       ACF1 │\n╞═════════╪══════════╪═════════╪═════════╪══════════╪═════════╪══════════╪════════════╡\n│ ARIMA   │ 22028.8  │ 97945.8 │ 73544.6 │ 0.684651 │ 2.7679  │ 0.564422 │ -0.0177529 │\n├─────────┼──────────┼─────────┼─────────┼──────────┼─────────┼──────────┼────────────┤\n│ SARIMA  │  2289.61 │ 52995.2 │ 34932.6 │ 0.079716 │ 1.31955 │ 0.268092 │  0.0231852 │\n╘═════════╧══════════╧═════════╧═════════╧══════════╧═════════╧══════════╧════════════╛\n\n\nRMSE Comparison\n\n\n╒══════════════╤══════════════╕\n│ Model        │         RMSE │\n╞══════════════╪══════════════╡\n│ RNN          │     0.150681 │\n├──────────────┼──────────────┤\n│ RNN(L2 Reg)  │     0.13484  │\n├──────────────┼──────────────┤\n│ GRU          │     0.151418 │\n├──────────────┼──────────────┤\n│ GRU(L2 Reg)  │     0.149967 │\n├──────────────┼──────────────┤\n│ LSTM         │     0.152851 │\n├──────────────┼──────────────┤\n│ LSTM(L2 Reg) │     0.152108 │\n├──────────────┼──────────────┤\n│ ARIMA        │ 97945.8      │\n├──────────────┼──────────────┤\n│ SARIMA       │ 52995.2      │\n╘══════════════╧══════════════╛\n\n\n\n\nAnn Models Error Summary\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │    0.156048 │    0.31658  │     0.395029 │  0.185671 │  0.359929 │   0.430895 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │    0.122194 │    0.265084 │     0.349563 │  0.136967 │  0.296626 │   0.370091 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │    0.191979 │    0.345011 │     0.438155 │  0.222296 │  0.372967 │   0.471482 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │    0.187826 │    0.334333 │     0.433389 │  0.228349 │  0.373415 │   0.477858 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │    0.193516 │    0.342503 │     0.439905 │  0.219581 │  0.374944 │   0.468595 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │    0.193618 │    0.347288 │     0.44002  │  0.227829 │  0.375522 │   0.477315 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nTraditional Univariate Models Error Summary\n\n\n╒═════════╤═══════════╤═════════╤═════════╤═══════════╤═════════╤══════════╤═══════════╕\n│ Model   │        ME │    RMSE │     MAE │       MPE │    MAPE │     MASE │      ACF1 │\n╞═════════╪═══════════╪═════════╪═════════╪═══════════╪═════════╪══════════╪═══════════╡\n│ ARIMA   │ -129.508  │ 22945.8 │ 18087.9 │ -0.616726 │ 6.37179 │ 0.808696 │ 0.0337695 │\n├─────────┼───────────┼─────────┼─────────┼───────────┼─────────┼──────────┼───────────┤\n│ SARIMA  │   63.5282 │ 17693.6 │ 14186.8 │ -0.209191 │ 4.95986 │ 0.634282 │ 0.0273341 │\n╘═════════╧═══════════╧═════════╧═════════╧═══════════╧═════════╧══════════╧═══════════╛\n\n\nRMSE Comparison\n\n\n╒══════════════╤══════════════╕\n│ Model        │         RMSE │\n╞══════════════╪══════════════╡\n│ RNN          │     0.395029 │\n├──────────────┼──────────────┤\n│ RNN(L2 Reg)  │     0.349563 │\n├──────────────┼──────────────┤\n│ GRU          │     0.438155 │\n├──────────────┼──────────────┤\n│ GRU(L2 Reg)  │     0.433389 │\n├──────────────┼──────────────┤\n│ LSTM         │     0.439905 │\n├──────────────┼──────────────┤\n│ LSTM(L2 Reg) │     0.44002  │\n├──────────────┼──────────────┤\n│ ARIMA        │ 22945.8      │\n├──────────────┼──────────────┤\n│ SARIMA       │ 17693.6      │\n╘══════════════╧══════════════╛\n\n\n\n\nAnn Models Error Summary\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │  0.00535037 │   0.0551332 │    0.0731462 │  0.758507 │  0.640908 │   0.870923 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │  0.00453961 │   0.0536078 │    0.0673766 │  1.22632  │  0.964119 │   1.10739  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │  0.00592626 │   0.0584674 │    0.0769822 │  0.889458 │  0.766761 │   0.943111 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │  0.00766894 │   0.064497  │    0.0875725 │  2.70266  │  1.43477  │   1.64398  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │  0.00766894 │   0.064497  │    0.0875725 │  2.70266  │  1.43477  │   1.64398  │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │  0.00640586 │   0.0598533 │    0.0800366 │  2.23633  │  1.30582  │   1.49544  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nTraditional Univariate Models Error Summary\n\n\n╒═════════╤═══════════╤═════════╤══════════╤═══════════╤══════════╤══════════╤════════════╕\n│ Model   │        ME │    RMSE │      MAE │       MPE │     MAPE │     MASE │       ACF1 │\n╞═════════╪═══════════╪═════════╪══════════╪═══════════╪══════════╪══════════╪════════════╡\n│ ARIMA   │ 2304.26   │ 21024.2 │ 12413.9  │  0.373212 │ 10.4586  │ 0.450189 │ -0.010844  │\n├─────────┼───────────┼─────────┼──────────┼───────────┼──────────┼──────────┼────────────┤\n│ SARIMA  │   88.1159 │ 16309.7 │  9630.99 │ -0.372932 │  9.00983 │ 0.349268 │ -0.0809737 │\n╘═════════╧═══════════╧═════════╧══════════╧═══════════╧══════════╧══════════╧════════════╛\n\n\nRMSE Comparison\n\n\n╒══════════════╤═══════════════╕\n│ Model        │          RMSE │\n╞══════════════╪═══════════════╡\n│ RNN          │     0.0731462 │\n├──────────────┼───────────────┤\n│ RNN(L2 Reg)  │     0.0673766 │\n├──────────────┼───────────────┤\n│ GRU          │     0.0769822 │\n├──────────────┼───────────────┤\n│ GRU(L2 Reg)  │     0.0875725 │\n├──────────────┼───────────────┤\n│ LSTM         │     0.0875725 │\n├──────────────┼───────────────┤\n│ LSTM(L2 Reg) │     0.0800366 │\n├──────────────┼───────────────┤\n│ ARIMA        │ 21024.2       │\n├──────────────┼───────────────┤\n│ SARIMA       │ 16309.7       │\n╘══════════════╧═══════════════╛\n\n\n\n\nAnn Models Error Summary\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │  0.00385925 │   0.0493704 │    0.0621229 │  0.241482 │ 0.252935  │   0.491409 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │  0.0019726  │   0.0355592 │    0.044414  │  0.012517 │ 0.0799382 │   0.111879 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │  0.00248111 │   0.0379035 │    0.0498107 │  0.224071 │ 0.260936  │   0.473361 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │  0.00274596 │   0.0389203 │    0.0524019 │  0.044396 │ 0.139761  │   0.210704 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │  0.0023674  │   0.0373848 │    0.0486559 │  0.344455 │ 0.312781  │   0.586903 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │  0.00209198 │   0.0346632 │    0.0457382 │  0.373875 │ 0.281131  │   0.611453 │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nTraditional Univariate Models Error Summary\n\n\n╒═════════╤═══════════╤═════════╤══════════╤═══════════╤═════════╤═══════════╤════════════╕\n│ Model   │        ME │    RMSE │      MAE │       MPE │    MAPE │      MASE │       ACF1 │\n╞═════════╪═══════════╪═════════╪══════════╪═══════════╪═════════╪═══════════╪════════════╡\n│ ARIMA   │ 0.0269576 │ 1.15102 │ 0.686711 │ 0.0246547 │ 1.19127 │ 0.0519912 │ 0.00422394 │\n╘═════════╧═══════════╧═════════╧══════════╧═══════════╧═════════╧═══════════╧════════════╛\n\n\nRMSE Comparison\n\n\n╒══════════════╤═══════════╕\n│ Model        │      RMSE │\n╞══════════════╪═══════════╡\n│ RNN          │ 0.0621229 │\n├──────────────┼───────────┤\n│ RNN(L2 Reg)  │ 0.044414  │\n├──────────────┼───────────┤\n│ GRU          │ 0.0498107 │\n├──────────────┼───────────┤\n│ GRU(L2 Reg)  │ 0.0524019 │\n├──────────────┼───────────┤\n│ LSTM         │ 0.0486559 │\n├──────────────┼───────────┤\n│ LSTM(L2 Reg) │ 0.0457382 │\n├──────────────┼───────────┤\n│ ARIMA        │ 1.15102   │\n╘══════════════╧═══════════╛\n\n\n\n\nAnn Models Error Summary\n\n\n╒══════════════╤═════════════╤═════════════╤══════════════╤═══════════╤═══════════╤════════════╕\n│ Model        │   train_MSE │   train_MAE │   train_RMSE │   val_MSE │   val_MAE │   val_RMSE │\n╞══════════════╪═════════════╪═════════════╪══════════════╪═══════════╪═══════════╪════════════╡\n│ RNN          │    0.30333  │    0.348757 │     0.550754 │   1.51433 │  0.968769 │    1.23058 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ RNN(L2 Reg)  │    0.246717 │    0.342717 │     0.496706 │   1.00403 │  0.774277 │    1.00201 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU          │    0.826495 │    0.474815 │     0.909118 │   1.32186 │  0.484335 │    1.14972 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ GRU(L2 Reg)  │    0.767283 │    0.444922 │     0.875947 │   1.25035 │  0.440379 │    1.11819 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM         │    0.823936 │    0.479866 │     0.907709 │   1.30895 │  0.496887 │    1.14409 │\n├──────────────┼─────────────┼─────────────┼──────────────┼───────────┼───────────┼────────────┤\n│ LSTM(L2 Reg) │    0.724108 │    0.420223 │     0.850945 │   1.17441 │  0.513033 │    1.0837  │\n╘══════════════╧═════════════╧═════════════╧══════════════╧═══════════╧═══════════╧════════════╛\n\n\nTraditional Univariate Models Error Summary\n\n\n╒═════════╤══════════╤═════════╤═════════╤══════════╤═════════╤══════════╤════════════╕\n│ Model   │       ME │    RMSE │     MAE │      MPE │    MAPE │     MASE │       ACF1 │\n╞═════════╪══════════╪═════════╪═════════╪══════════╪═════════╪══════════╪════════════╡\n│ ARIMA   │ -2.50447 │ 240.128 │ 153.942 │ -54.1323 │ 110.854 │ 0.768645 │ -0.0062189 │\n╘═════════╧══════════╧═════════╧═════════╧══════════╧═════════╧══════════╧════════════╛\n\n\nRMSE Comparison\n\n\n╒══════════════╤════════════╕\n│ Model        │       RMSE │\n╞══════════════╪════════════╡\n│ RNN          │   0.550754 │\n├──────────────┼────────────┤\n│ RNN(L2 Reg)  │   0.496706 │\n├──────────────┼────────────┤\n│ GRU          │   0.909118 │\n├──────────────┼────────────┤\n│ GRU(L2 Reg)  │   0.875947 │\n├──────────────┼────────────┤\n│ LSTM         │   0.907709 │\n├──────────────┼────────────┤\n│ LSTM(L2 Reg) │   0.850945 │\n├──────────────┼────────────┤\n│ ARIMA        │ 240.128    │\n╘══════════════╧════════════╛\n\n\n\n\n\nQuestion 4 How does your deep learning modeling compare to the traditional single-variable time-series ARMA/ARIMA models from HW-3?\nThe results show that deep learning models often excel in capturing complex patterns but may require techniques like regularization to manage overfitting and ensure they generalize well to new, unseen datasets. In comparison, traditional time-series models like ARIMA might not capture complex patterns as effectively but can provide stable and consistent predictions across different scenarios.\nFor example, the RNN model in natural gas prices dataset has a training MSE of about 0.086 and a validation MSE of about 0.151. This shows that while the model predicts fairly accurately on the training data, its predictions become less reliable on unseen data. In contrast, the traditional ARIMA model has an RMSE of about 0.759, suggesting that while it may not capture complex patterns as well as the RNN, it maintains a moderate level of prediction error across different data. Moreover, the GRU model with L2 regularization in the natural gas consumption dataset shows a training RMSE of about 0.547 and a validation RMSE of approximately 1.100. This example indicates significant overfitting, as the model’s error nearly doubles when applied to new data. On the other hand, the SARIMA model has a much higher RMSE of about 98,342 on the training set, which is extraordinarily high and suggests that the model might not be well-suited for this dataset without adjustments or considering model inadequacies. Furthermore, in natural gas production dataset, the LSTM model with L2 regularization shows smaller training and validation RMSEs of about 0.152 and 0.313, respectively. This demonstrates that the regularization helps in managing overfitting effectively, keeping the model’s performance more consistent across new data. Conversely, the ARIMA model here has a significantly lower RMSE of 97,945, highlighting a potential good fit for simpler models in stable data environments.\nModel Comparison\nRMSE Comparison: Lookin at the RMSE values, deep learning models such as RNN, LSTM, and their L2 regularized versions tend to have lower RMSE on training datasets, indicating a strong fit to the data. For example, in the natural gas prices dataset, the LSTM model shows a training RMSE of 0.304 compared to its validation RMSE of 0.454, reflecting overfitting. In contrast, traditional models like ARIMA and SARIMA in the same dataset have higher RMSE values (0.759 and 0.792, respectively) on the training set, suggesting that they are less flexible in capturing the complexities of the data but may generalize better on unseen data. However, the deep learning models’ validation RMSEs are considerably closer to their training RMSEs than those of traditional models, indicating potential for better generalization with proper tuning.\nForecasting Comparison: Deep learning models, particularly LSTM and GRU, have shown capabilities in modeling complex temporal sequences effectively, which is evident from their performance on various datasets. For example, the LSTM model in the natural gas imports dataset achieves a lower validation RMSE of 0.468 compared to the traditional ARIMA model’s RMSE of 22945.8, highlighting the superiority of LSTM in handling complex patterns. Similarly, in the natural gas production dataset, the LSTM model reduces the validation RMSE to 0.313, outperforming the traditional models. This demonstrates that deep learning models are particularly effective in scenarios where data has non-linear relationships and requires capturing long-term dependencies. Moreover, traditional models like ARIMA and SARIMAX still hold substantial value, particularly in scenarios where understanding the influence of external variables is critical."
  },
  {
    "objectID": "conclusions.html#economic-impact",
    "href": "conclusions.html#economic-impact",
    "title": "Conclusions",
    "section": "Economic Impact",
    "text": "Economic Impact\n\n\n\n\n\nHenry Hub prices are a critical benchmark for natural gas pricing across the United States, influencing everything from individual utility bills to the economics of major industries. These prices not only reflect current supply and demand conditions but also set expectations for future changes in the market.\n\n\n\n\n\nIn 2008, Henry Hub natural gas prices spiked as seen from the above plot due to factors like financial market disruptions, showcasing their sensitivity to economic conditions and market changes.\nGenerally, economic growth often leads to increased energy demand, pushing up natural gas prices as industrial and consumer demand grows. Conversely, during economic downturns, demand decreases, resulting in lower prices. Unemployment impacts natural gas prices more indirectly, with higher unemployment leading to reduced economic activity and energy demand, thus affecting prices.\nIn this analysis, I explored the influence of GDP and unemployment on natural gas prices using several predictive models whose forecast are shown below.\n\n\n\n\n\nARIMA model forecast\n\n\n\n\n\n\nSARIMA model forecast\n\n\n\n\n\n\nARIMAX model forecast\n\n\n\n\nThe ARIMA Model, focusing on historical price data, predicted significant price fluctuations, indicating the volatility of natural gas prices and their susceptibility to a variety of influences beyond economic indicators alone. The SARIMA Model added seasonal trends to the analysis, shedding light on seasonal price variations crucial for planning in energy sectors. Meanwhile, the ARIMAX Model integrated GDP and unemployment data, providing a comprehensive view of how broader economic conditions correlate with natural gas prices.\nThe results clearly demonstrate while prices are inherently volatile, they respond predictably to changes in GDP growth and unemployment rates. The forecasts suggest a slight upward trend in natural gas prices from 2020, with increased variability towards 2025, indicative of a strengthening US economy characterized by GDP growth and decreasing unemployment. This trend towards stabilization in prices suggests fewer drastic fluctuations in the coming years, highlighting how economic recovery directly impacts energy demand and prices. Such insights are invaluable for energy companies, policymakers, and consumers in making informed decisions about energy use, pricing strategies, and policy formulation."
  },
  {
    "objectID": "conclusions.html#demand-supply-dynamics",
    "href": "conclusions.html#demand-supply-dynamics",
    "title": "Conclusions",
    "section": "Demand / Supply Dynamics",
    "text": "Demand / Supply Dynamics"
  },
  {
    "objectID": "conclusions.html#environmental-impact",
    "href": "conclusions.html#environmental-impact",
    "title": "Conclusions",
    "section": "Environmental Impact",
    "text": "Environmental Impact\n\n\n\n\n\nAs natural gas usage and production have increased, so have the emissions, highlighting the need for strategic plan energy. This analysis highlights the direct impact of natural gas consumption and production on CO2 emissions within the energy sector.\n Using the ARIMA model, which relies on historical data, we observed inherent emission patterns, while the ARIMAX model provided a more nuanced forecast by integrating production and consumption data, the forecast is shown in the plot above. The forecast in the above plot shows that increase in production and consumption correlates with higher emissions, underscoring the environmental cost of natural gas usage. However, discrepancies in forecast intensities between the model may arise from their different sensitivities to external factors and inherent data noise. Overall, this analysis underscores the importance of improving natural gas efficiency and adopting cleaner technologies like carbon capture and advanced combustion to mitigate the sector’s environmental impact."
  },
  {
    "objectID": "conclusions.html#market-performance",
    "href": "conclusions.html#market-performance",
    "title": "Conclusions",
    "section": "Market Performance",
    "text": "Market Performance\n\n\n\n\n\n\n\n\n\n\nCVX Stocks\n\n\n\n\n\n\nBP Stocks\n\n\n\n\nThe analysis of stocks of top natural gas producing companies namely Chevron Corporation and British Petroleum as seen from the plot above, it’s evident that stock market volatility in major oil/gas companies is heavily influenced by global events and crises. It’s evident that the movements in stock prices for these companies are not random but closely tied to significant global events. For instance, the Great Recession and the COVID-19 pandemic caused noticeable spikes in volatility, indicating that these companies are sensitive to economic downturns and global crises. Moreover, events specifically impacting the oil and energy sectors, like the 2010 Deepwater Horizon oil spill, can also lead to significant increases in stock volatility. This shows that sector-specific crises can be just as impactful as broader economic crises. Furthermore, investors should be aware that stocks in the oil and energy sector can be prone to sudden and sharp fluctuations in response to global and industry-specific events. This could affect investment decisions, especially for those seeking more stable investments. Lastly, companies and investors alike should focus on risk management strategies to prepare for and mitigate the effects of these high-volatility periods. This could involve diversifying investments, setting aside reserves for crisis periods, or hedging against potential losses."
  },
  {
    "objectID": "conclusions.html#trade-dynamics",
    "href": "conclusions.html#trade-dynamics",
    "title": "Conclusions",
    "section": "Trade Dynamics",
    "text": "Trade Dynamics\n\n\n\n\n\nFinally, I analyzed export dynamics, as seen from the significant increase in U.S. natural gas exports shown in the plot above especially from 2014 given imports continue to decline. Analysing exports dynamics became crutical because it highlighted the growing relevance of exports in shaping the country’s energy strategy and market dynamics in the natural gas industry.\nI examined factors such as U.S. consumption levels, exchange rate fluctuations, and the economic health of key import countries like the Netherlands, and gained crucial insights into how to strategically balance local demand with global market opportunities. Note: I focused on the Netherlands due to it being significant role in importing the U.S. natural gas. Source: eia\n\n\n\n\n\nARIMA model forecast\n\n\n\n\n\n\nSARIMA model forecast\n\n\n\n\n\n\nARIMAX model forecast\n\n\n\n\nThe ARIMA model, focused only on historical export data, identified general trends in exports. The SARIMA model extended this analysis by incorporating seasonal variations, highlighting how exports fluctuate throughout the year, which is critical for timely market strategies. The ARIMAX model, which integrated domestic consumption, exchange rates, and economic indicators from the Netherlands, provided a comprehensive forecast, showing how these variables interplay to shape export dynamics as shown from the lots above. It predicts that U.S. natural gas exports are likely to increase, influenced by domestic consumption patterns, international economic conditions, and currency valuation changes suggesting a strengthening U.S. dollar as natural gas exports rise, potentially benefiting sectors like manufacturing and energy in importing countries by providing a cheaper or more reliable energy source. For the U.S., increased exports might raise domestic natural gas prices unless production adequately covers both domestic and international demands. This scenario underscores the importance of strategic planning to balance local consumption with export opportunities, optimizing national and international economic benefits while maintaining stable supply and pricing. These insights are crucial for stakeholders to make informed decisions that align with both national interests and global market dynamics."
  },
  {
    "objectID": "conclusions.html#demand-dynamics",
    "href": "conclusions.html#demand-dynamics",
    "title": "Conclusions",
    "section": "Demand Dynamics",
    "text": "Demand Dynamics"
  },
  {
    "objectID": "conclusions.html#supply-dynamics",
    "href": "conclusions.html#supply-dynamics",
    "title": "Conclusions",
    "section": "Supply Dynamics",
    "text": "Supply Dynamics"
  },
  {
    "objectID": "conclusions.html#revisiting-the-big-picture",
    "href": "conclusions.html#revisiting-the-big-picture",
    "title": "Conclusions",
    "section": "Revisiting the Big Picture",
    "text": "Revisiting the Big Picture\n\n\n\n\n\nThis project explored various aspects of the natural gas industry in the United States, utilizing multiple datasets and employing a combination of univariate, multivariate, and financial time series models, alongside cutting-edge deep learning approaches. My initial efforts focused on data visualization and exploratory data analysis to discern patterns and trends within the market. Through univariate analysis, I gained a broader understanding of time series analysis related to natural gas prices, consumption, production, imports, exports, CO2 emissions, and CVX stock movements. This served as a foundational step before advancing to more complex multivariate and financial time series analyses. Building on this foundation with a thorough review of existing literature, I developed sophisticated multivariate time series models. These models were instrumental in examining the effects of GDP and unemployment on natural gas prices, the influences of natural gas production, storage, and temperature on consumption, and the impacts of production and consumption on CO2 emissions. I also explored how domestic consumption, exchange rates, and the GDP of importing countries affect U.S. natural gas exports and investigated the interdependencies among production, import, and natural gas prices. Furthermore, I analyzed and forecasted the stock prices of leading natural gas production companies using financial time series models to assess their responsiveness to changes in the U.S. economy. This comprehensive analysis not only reinforced the understanding of the natural gas sector’s complexities but also underscored the efficacy of time-series modeling in predicting its economic implications."
  },
  {
    "objectID": "conclusions.html#demandsupply-dynamics",
    "href": "conclusions.html#demandsupply-dynamics",
    "title": "Conclusions",
    "section": "Demand/Supply Dynamics",
    "text": "Demand/Supply Dynamics\nDemand Dynamics\nThe analysis of natural gas consumption demonstrates a direct relationship with production levels as seen from the plot below.\n\n\n\n\n\n\nIt also shows a strong sensitivity to seasonal temperature variations. For instance, consumption spikes during winter months due to increased heating needs, and dips in summer when demand wanes as shown by the plot below.\n Storage also plays a critical role in balancing supply, especially during unexpected surges in demand such as those experienced during the 2013-2014 Polar Vortex due to extremely cold temperatures across much of North America. This led to a spike in natural gas demand for heating as a result the storage levels were significantly drawn down. As shown in the plot below.\n\n\n\nSource: eia\n\n\nFor this analysis, I explored the influence of production, temperature and storage on natural gas consumption using several predictive models.\n The ARIMA model analyzed historical consumption data, revealing intrinsic patterns and market cycles. The SARIMA model added seasonal trends, enhancing understanding of periodic demand fluctuations crucial for energy sector planning. The SARIMAX model, incorporating production, temperature, and storage, also our best performing model whose forecast is shown in the plot above provided a comprehensive view, predicting how these factors influence consumption. The forecast suggests that natural gas consumption is expected to follow a similar pattern as observed in the past, with some periods of high usage which typically align with winter season. The forecast also indicates a potential increase in consumption beyond our last recorded data, suggesting that the demand for natural gas might grow. Such insights are essential for ensuring adequate supply, avoiding shortages, and stabilizing prices, aiding stakeholders in strategic planning and policy formulation.\nSupply Side Dynamics\nIn this analysis, I looked at intricate relationships between U.S. natural gas imports, production, and prices using a VAR model.\n The model and the plot above helps to see how changes in one variable can influence the others due to their interconnected nature. It shows that the amount of natural gas being imported is starting to level off, which suggests the market is reaching a steady state which might indicate sufficient domestic production or a shift in energy policy affecting import dependence. Although prices are expected to remain stable for now, there’s a chance they might decline due to factors like market demand or political changes. We see that production is likely to decrease, which could be due to a variety of factors such as regulatory changes or natural resource limitations. In light of the analysis that natural gas imports are stabilizing and production is likely to decrease, stakeholders in the natural gas industry should focus on enhancing risk management, investing in innovative extraction technologies, optimizing supply chains, and engaging in policy advocacy. Additionally, continuous market analysis and a strong commitment to sustainability will be crucial. These strategies will help the industry navigate potential price declines and production challenges, ensuring stability and profitability amidst changing market dynamics and regulatory environments."
  },
  {
    "objectID": "index.html#analytical-angles",
    "href": "index.html#analytical-angles",
    "title": "Analysis of Natural Gas Industry in the US",
    "section": "Analytical Angles",
    "text": "Analytical Angles\nThis project aims to deepen understanding of the natural gas sector’s crucial role within the broader energy matrix by analyzing economic, environmental, demand and supply dynamics, stock market interactions, and trade impacts. It will highlight current market conditions and forecast future trends.\n\nEconomic Impact\n\n\n\nImage source: Shutterstock/FOTOGRIN\n\n\nThis comprehensive analysis embarks on a multifaceted journey to unravel the correlation between natural gas industry dynamics and key economic indicators such as GDP and unemployment rates. The analysis primarily focuses on how these economic factors impact natural gas prices, illuminating the industry’s influence on the broader economic health of the nation.\n\n\nDemand/Supply Dynamics\n\n\n\n\n\nThis analysis delves into the dynamics of demand and supply, exploring how factors such as natural gas prices, storage, and temperature influence consumption, as well as how supply-side dynamics, including imports and production, affect natural gas prices. A thorough understanding of these demand and supply behaviors will enrich the analysis.\n\n\nEnvironmental Impact\n\n\n\n\n\nA crucial facet of the exploration lies in understanding the Environmental Impact of natural gas consumption and production. By correlating natural gas usage and production with carbon emissions, the analysis contributes to the ongoing discourse on sustainable energy practices.\n\n\nMarket Performance\n\n\n\nimage by: Torsten Asmus/iStock via Getty Images\n\n\nExtending into the Financial Markets, the analysis scrutinizes the stock market performance of major energy companies involved in natural gas production. This facet provides insights into the industry’s financial resilience and responsiveness to market dynamics.\n\n\nTrade Dynamics\n\n\n\n\n\nTrade dynamics form a crucial part of this analysis, examining how natural gas trade relationships contribute to a broader understanding of the industry’s global economic influence. This analysis specifically explores how U.S. natural gas consumption, exchange rates, and the economic health of importing country affect natural gas exports.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#guiding-questions",
    "href": "index.html#guiding-questions",
    "title": "Analysis of Natural Gas Industry in the US",
    "section": "Guiding Questions",
    "text": "Guiding Questions\n\nHow can we anticipate changes in natural gas prices based on economic indicators like GDP and unemployment rates?\nHow can we optimize natural gas consumption in response to fluctuating production levels, and external factors like temperature and storage capacities?\nHow do domestic consumption, exchange rates, and the economic health of importing countries like the Netherlands affect US natural gas export volumes?\nHow can changes in natural gas consumption and production levels be expected to influence CO2 emissions?\nHow can understanding the relationship between supply dynamics, such as changes in imports and production, help optimize pricing strategies and inventory management for natural gas at Henry Hub?\nHow do stock market trends of major natural gas companies reflect the overall health of the natural gas industry?\nWhat are the future trends in natural gas prices based on historical price patterns?\nHow can we use historical consumption patterns to predict future demand for natural gas, and how might this influence US procurement and distribution strategies?\nCan we anticipate the future levels of natural gas imports and exports based on historical data?\nCan we predict future trends in CO2 emissions based solely on their historical patterns?Based on trends in CO2 emissions from natural gas usage, how can companies adjust their practices to better meet environmental standards?",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "datasrc.html#consumption-data",
    "href": "datasrc.html#consumption-data",
    "title": "Data Sources",
    "section": "Consumption Data",
    "text": "Consumption Data\n1) Yearly Data\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset provides information on natural gas yearly consumption in the United States.\nData for natural gas consumption covers the years 2005 to 2022, presented in trillion cubic feet.\n\n\n\n\n\n\nUS Natural Gas Yearly Consumption Dataset\n\n\nYear\nConsumption\n\n\n\n\n2005\n22.01\n\n\n2006\n21.70\n\n\n2007\n23.10\n\n\n2008\n23.28\n\n\n2009\n22.91\n\n\n2010\n24.09\n\n\n\n\n\nDownload yearly consumption data\n\n\n\n\n\n\n2) Monthly Data \n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset provides information on natural gas monthly consumption in the United States.\nData for natural gas consumption covers the years 2005 to 2022, presented in million cubic feet.\n\n\n\n\n\n\nUS Natural Gas Monthly Consumption Dataset\n\n\nMonth\nConsumption\n\n\n\n\n1/1/05\n2561858\n\n\n2/1/05\n2242986\n\n\n3/1/05\n2205787\n\n\n4/1/05\n1724877\n\n\n5/1/05\n1522613\n\n\n6/1/05\n1534122\n\n\n\n\n\nDownload monthly consumption data\n\n\n\n\n\n\n3) Consumption By Sector Data \n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset provides information on natural gas yearly consumption by sector in the United States.\nData for natural gas consumption covers the years 2005 to 2022, presented in billion cubic feet.\n\n\n\n\n\n\nUS Natural Gas Consumption by Sector Dataset\n\n\nYear\nconsumption\nsector\n\n\n\n\n2005\n5869\nElectric power\n\n\n2006\n6222\nElectric power\n\n\n2007\n6841\nElectric power\n\n\n2008\n6668\nElectric power\n\n\n2009\n6873\nElectric power\n\n\n2010\n7387\nElectric power\n\n\n\n\n\nDownload consumption by sector data",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "datasrc.html#production-data",
    "href": "datasrc.html#production-data",
    "title": "Data Sources",
    "section": "Production Data",
    "text": "Production Data\n1) Yearly Production \n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset provides information on natural gas yearly production in the United States.\nData for natural gas production spans from 2005 to 2022, presented in billion cubic feet.\n\n\n\n\n\n\nUS Natural Gas Yearly Production Dataset\n\n\nYear\nProduction\n\n\n\n\n2005\n489.4\n\n\n2006\n501.7\n\n\n2007\n521.9\n\n\n2008\n546.1\n\n\n2009\n557.6\n\n\n2010\n575.2\n\n\n\n\n\nDownload yearly production data\n\n\n\n\n\n\n2) Monthly Production \n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset provides information on natural gas monthly production in the United States.\nData for natural gas production covers the years 2005 to 2022, presented in million cubic feet.\n\n\n\n\n\n\nUS Natural Gas Monthly Production Dataset\n\n\nMonth\nProduction\n\n\n\n\n1/1/05\n2035036\n\n\n2/1/05\n1870546\n\n\n3/1/05\n2080504\n\n\n4/1/05\n1979474\n\n\n5/1/05\n2010625\n\n\n6/1/05\n1972975\n\n\n\n\n\nDownload monthly production data",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "datasrc.html#ghg-emissions",
    "href": "datasrc.html#ghg-emissions",
    "title": "Data Sources",
    "section": "GHG Emissions",
    "text": "GHG Emissions\n\n\n\n\n\nclick image for data source\n\n\n\n\nContains an extensive selection of Greenhouse gas emissions data from the energy sector for over 190 countries and regions.\nThe data included in this file with the following indicators:\n\nEnergy related greenhouse gas emissions\nFugitive greenhouse gas emissions\nExtended time series, starting in 1751 of CO2 emissions from fuel combustion.\n\n\n\n\n\n\n\nUS Natural Gas Yearly GHG Emissions Dataset\n\n\nYear\nTotal.GHG..emissions.from.fuel.combustion\nnatural.gas\nother\n\n\n\n\n1971\n4338.88\n1197.8\n3141.08\n\n\n1972\n4580.87\n1210.1\n3370.77\n\n\n1973\n4746.14\n1200.1\n3546.04\n\n\n1974\n4594.17\n1158.6\n3435.57\n\n\n1975\n4409.86\n1063.4\n3346.46\n\n\n1976\n4674.07\n1084.7\n3589.37\n\n\n\n\n\nDownload GHG emissions data"
  },
  {
    "objectID": "datasrc.html#import-data",
    "href": "datasrc.html#import-data",
    "title": "Data Sources",
    "section": "Import Data",
    "text": "Import Data\nYearly Volume Monthly Volume Yearly Prices"
  },
  {
    "objectID": "datasrc.html#export-data",
    "href": "datasrc.html#export-data",
    "title": "Data Sources",
    "section": "Export Data",
    "text": "Export Data\nYearly Volume Monthly Volume Yearly Prices"
  },
  {
    "objectID": "datasrc.html#co2-emissions",
    "href": "datasrc.html#co2-emissions",
    "title": "Data Sources",
    "section": "CO2 Emissions",
    "text": "CO2 Emissions\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset contains carbon dioxide emissions from energy consumption of natural gas in the USA.\nThe data spans from the year 2000 to 2023.\nThe unit of measurement is millions of metric tons of carbon dioxide.\n\n\n\n\n\n\nUS Natural Gas Monthly CO2 Emissions Dataset\n\n\n\nDate\nco2_value\n\n\n\n\n305\n6/1/23\n127.178\n\n\n306\n7/1/23\n143.918\n\n\n307\n8/1/23\n144.409\n\n\n308\n9/1/23\n127.841\n\n\n309\n10/1/23\n131.738\n\n\n310\n11/1/23\n152.661\n\n\n\n\n\nDownload Natural gas CO2 Emissions Data"
  },
  {
    "objectID": "datasrc.html#prices",
    "href": "datasrc.html#prices",
    "title": "Data Sources",
    "section": "Prices",
    "text": "Prices\n1) Henry Hub Prices\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset contains the monthly average henry hub natural gas spot price in the USA.\nThe data spans from the year 1973 to 2023.\nPrices are measured in Dollars per Million Btu.\n\n\n\n\n\n\nUS Average Henry Hub Natural Gas Spot Price Dataset\n\n\nMonth\nHenry.Hub.Natural.Gas.Spot.Price.Dollars.per.Million.Btu\n\n\n\n\n1/1/05\n6.15\n\n\n2/1/05\n6.14\n\n\n3/1/05\n6.96\n\n\n4/1/05\n7.16\n\n\n5/1/05\n6.47\n\n\n6/1/05\n7.18\n\n\n\n\n\nDownload Henry Hub Natural Gas Spot Price Data\n\n\n\n\n\n\n2) Prices By Sector\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset contains the yearly average price of natural gas delivered to residential and commercial consumers in the USA.\nThe data spans from the year 1997 to 2023.\nPrices are measured in dollars per thousand cubic feet.\n\n\n\n\n\n\nUS Average Natural Gas Price By Sector Dataset\n\n\nYear\nprices\nsector\n\n\n\n\n1997\n6.94\nResidential\n\n\n1998\n6.82\nResidential\n\n\n1999\n6.69\nResidential\n\n\n2000\n7.76\nResidential\n\n\n2001\n9.63\nResidential\n\n\n2002\n7.89\nResidential\n\n\n\n\n\nDownload Natural Gas Prices By Sector Data"
  },
  {
    "objectID": "datasrc.html#industry-employment",
    "href": "datasrc.html#industry-employment",
    "title": "Data Sources",
    "section": "Industry Employment",
    "text": "Industry Employment\n1) By Gender\n\n\n\n\n\nclick image for data source\n\n\n\n\nContains information of the workforce in the U.S. natural gas distribution industry by gender and age from 2014 to 2021\n\n\n\n\n\n\nWorkforce in US Natural Gas Distribution Industry By Gender And Age Dataset\n\n\nYear\nGender\nTotal.Population\nAge.Group\n\n\n\n\n2021\nFemale\n81\n16 to 19\n\n\n2021\nFemale\n1278\n20 to 24\n\n\n2021\nFemale\n2619\n25 to 29\n\n\n2021\nFemale\n3399\n30 to 34\n\n\n2021\nFemale\n2729\n35 to 39\n\n\n2021\nFemale\n2214\n40 to 44\n\n\n\n\n\nDownload employment by gender and age data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2) By Location\n\n\n\n\n\nclick image for data source\n\n\n\n\nContains information of the workforce in the U.S. natural gas distribution industry by location from 2014 to 2021\n\n\n\n\n\n\nWorkforce in US Natural Gas Distribution Industry By Location Dataset\n\n\nState\nYear\nTotal.Population\nCode\n\n\n\n\nAlabama\n2021\n2371\nAL\n\n\nAlabama\n2020\n2685\nAL\n\n\nAlabama\n2019\n2111\nAL\n\n\nAlabama\n2018\n2210\nAL\n\n\nAlabama\n2017\n2532\nAL\n\n\nAlabama\n2016\n2157\nAL\n\n\n\n\n\nDownload employment by location data\n\n\n\n\n\n3) By Wage\n\n\n\n\n\nclick image for data source\n\n\n\n\nContains information of the the U.S. natural gas distribution industry yearly wage from 2014 to 2021\n\n\n\n\n\n\nUS Natural Gas Distribution Industry Yearly Wage Dataset\n\n\nID.Year\nAverage.Wage\n\n\n\n\n2021\n86281.42\n\n\n2020\n81508.72\n\n\n2019\n79726.42\n\n\n2018\n77958.03\n\n\n2017\n75065.32\n\n\n2016\n72986.09\n\n\n\n\n\nDownload employment by wage data"
  },
  {
    "objectID": "datasrc.html#gdp",
    "href": "datasrc.html#gdp",
    "title": "Data Sources",
    "section": "GDP",
    "text": "GDP\n1) US GDP\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset presents Quarterly Gross domestic product (GDP) figures for the United States.\nThe data spans from the year 2005 to 2022.\nThe unit of analysis is Billions of Dollars, Seasonally Adjusted Annual Rate.\n\n\n\n\n\n\nUS GDP Dataset\n\n\nQuarter\nGDP\n\n\n\n\n2005-01-01\n12767.29\n\n\n2005-04-01\n12922.66\n\n\n2005-07-01\n13142.64\n\n\n2005-10-01\n13324.20\n\n\n2006-01-01\n13599.16\n\n\n2006-04-01\n13753.42\n\n\n\n\n\nDownload US GDP data\n\n\n\n\n\n\n2) Netherlands GDP\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset presents Quarterly Gross domestic product (GDP) figures for Netherlands.\nThe data spans from the year 2005 to 2022.\nThe unit of analysis is Millions of Euros,Seasonally Adjusted.\n\n\n\n\n\n\nNetherlands GDP Dataset\n\n\nDate\ngdp_int\n\n\n\n\n1/1/05\n134841.1\n\n\n4/1/05\n136587.9\n\n\n7/1/05\n138671.4\n\n\n10/1/05\n140509.7\n\n\n1/1/06\n143090.4\n\n\n4/1/06\n145172.7\n\n\n\n\n\nDownload Netherlands GDP data"
  },
  {
    "objectID": "datasrc.html#unemployment",
    "href": "datasrc.html#unemployment",
    "title": "Data Sources",
    "section": "Unemployment",
    "text": "Unemployment\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset presents monthly unemployment rate figures for the USA.\nThe data spans from the year 2005 to 2022.\n\n\n\n\n\n\nUS Monthly Unemployment Rate Dataset\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\n\n\n2005\n5.3\n5.4\n5.2\n5.2\n5.1\n5.0\n5.0\n4.9\n5.0\n5.0\n5.0\n4.9\n\n\n2006\n4.7\n4.8\n4.7\n4.7\n4.6\n4.6\n4.7\n4.7\n4.5\n4.4\n4.5\n4.4\n\n\n2007\n4.6\n4.5\n4.4\n4.5\n4.4\n4.6\n4.7\n4.6\n4.7\n4.7\n4.7\n5.0\n\n\n2008\n5.0\n4.9\n5.1\n5.0\n5.4\n5.6\n5.8\n6.1\n6.1\n6.5\n6.8\n7.3\n\n\n2009\n7.8\n8.3\n8.7\n9.0\n9.4\n9.5\n9.5\n9.6\n9.8\n10.0\n9.9\n9.9\n\n\n2010\n9.8\n9.8\n9.9\n9.9\n9.6\n9.4\n9.4\n9.5\n9.5\n9.4\n9.8\n9.3\n\n\n\n\n\nDownload Unemployment data"
  },
  {
    "objectID": "datasrc.html#exchange-rates",
    "href": "datasrc.html#exchange-rates",
    "title": "Data Sources",
    "section": "Exchange Rates",
    "text": "Exchange Rates\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset presents quaterly average exchange rate for converting US Dollars to Netherlands’ national currency (USD to EUR) figures.\nThe data spans from the year 2005 to 2022.\nThe unit of analysis is Euro, Not Seasonally Adjusted.\n\n\n\n\n\n\nExchange Rate for Netherlands Dataset\n\n\nDate\nexchange_rate\n\n\n\n\n2005-01-01\n105.2033\n\n\n2005-04-01\n103.9500\n\n\n2005-07-01\n102.4767\n\n\n2005-10-01\n101.5567\n\n\n2006-01-01\n100.7100\n\n\n2006-04-01\n102.1833\n\n\n\n\n\nDownload Exchange Rate data"
  },
  {
    "objectID": "datasrc.html#storage",
    "href": "datasrc.html#storage",
    "title": "Data Sources",
    "section": "Storage",
    "text": "Storage\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset contains the monthly underground storage volume of natural gas in the USA.\nThe data spans from the year 2005 to 2022\nThe unit of analysis is Million Cubic Feet.\n\n\n\n\n\n\nNatural Gas Storage in the US Dataset\n\n\n\n\n\n\nMonth\nU.S..Natural.Gas.Underground.Storage.Volume..Million.Cubic.Feet\n\n\n\n\n1/1/05\n6199291\n\n\n2/1/05\n5768939\n\n\n3/1/05\n5484332\n\n\n4/1/05\n5699060\n\n\n5/1/05\n6075521\n\n\n6/1/05\n6398738\n\n\n\n\n\nDownload Storage Data"
  },
  {
    "objectID": "datasrc.html#temperature",
    "href": "datasrc.html#temperature",
    "title": "Data Sources",
    "section": "Temperature",
    "text": "Temperature\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset contains the monthly average temperature figures across the USA.\nThe data spans from the year 2005 to 2022.\nThe temperature is measured in fahrenheit.\n\n\n\n\n\n\nAverage Temperature in the US Dataset\n\n\nmoonth\navg_temp\n\n\n\n\n1/1/05\n33.4\n\n\n2/1/05\n37.9\n\n\n3/1/05\n42.3\n\n\n4/1/05\n52.3\n\n\n5/1/05\n59.6\n\n\n6/1/05\n69.2\n\n\n\n\n\nDownload Temperature Data"
  },
  {
    "objectID": "datasrc.html#imports-data",
    "href": "datasrc.html#imports-data",
    "title": "Data Sources",
    "section": "Imports Data",
    "text": "Imports Data\n1) Yearly Volume\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset provides information on the imports of natural gas worldwide, categorized by country.\nThe data covers the years 1997 to 2022.\nThe unit of analysis is million standard cubic meters (M standard cu m).\n\n\n\n\n\n\nUS Natural Gas Yearly Imports Dataset\n\n\nYear\nImports\n\n\n\n\n1997\n86090\n\n\n1998\n90150\n\n\n1999\n101710\n\n\n2000\n108970\n\n\n2001\n113320\n\n\n2002\n113650\n\n\n\n\n\nDownload Imports data\n\n\n\n\n\n\n2) Monthly Volume\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset provides information on natural gas monthly imports in the United States.\nData for natural gas imports covers the years 1997 to 2022, presented in million cubic feet.\n\n\n\n\n\n\nUS Natural Gas Monthly Imports Dataset\n\n\nMonth\nImports\n\n\n\n\n1997-01-01\n278288\n\n\n1997-02-01\n240545\n\n\n1997-03-01\n256985\n\n\n1997-04-01\n238178\n\n\n1997-05-01\n241732\n\n\n1997-06-01\n232118\n\n\n\n\n\nDownload monthly imports data\n\n\n\n\n\n\n3) Yearly Prices\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset provides information on natural gas average annual imports price in the United States.\nData for natural gas imports covers the years 1997 to 2022, presented in dollars per thousand cubic feet.\n\n\n\n\n\n\nUS Natural Gas Yearly Average Imports Price Dataset\n\n\n\nYear\nImports_price\n\n\n\n\n21\n2017\n2.60\n\n\n22\n2018\n2.69\n\n\n23\n2019\n2.55\n\n\n24\n2020\n2.07\n\n\n25\n2021\n3.78\n\n\n26\n2022\n6.33\n\n\n\n\n\nDownload yearly imports price data",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "datasrc.html#exports-data",
    "href": "datasrc.html#exports-data",
    "title": "Data Sources",
    "section": "Exports Data",
    "text": "Exports Data\n1) Yearly Volume\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset provides information on the exports of natural gas worldwide, categorized by country.\nThe data covers the years 1997 to 2022.\nThe unit of analysis is million standard cubic meters (M standard cu m).\n\n\n\n\n\n\nUS Natural Gas Yearly Exports Dataset\n\n\nYear\nExports\n\n\n\n\n1997\n4430\n\n\n1998\n4480\n\n\n1999\n4560\n\n\n2000\n6820\n\n\n2001\n10590\n\n\n2002\n14540\n\n\n\n\n\nDownload Imports data\n\n\n\n\n\n\n2) Monthly Volume\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset provides information on natural gas monthly exports in the United States.\nData for natural gas exports covers the years 1997 to 2022, presented in million cubic feet.\n\n\n\n\n\n\nUS Natural Gas Monthly Exports Dataset\n\n\nMonth\nExports\n\n\n\n\n1997-01-01\n12028\n\n\n1997-02-01\n12443\n\n\n1997-03-01\n16276\n\n\n1997-04-01\n13872\n\n\n1997-05-01\n10097\n\n\n1997-06-01\n9527\n\n\n\n\n\nDownload monthly exports data\n\n\n\n\n\n\n3) Yearly Prices\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset provides information on natural gas average annual exports price in the United States.\nData for natural gas production covers the years 1997 to 2022, presented in dollars per thousand cubic feet.\n\n\n\n\n\n\nUS Natural Gas Yearly Average Exports Price Dataset\n\n\n\nYear\nExports_price\n\n\n\n\n21\n2017\n3.54\n\n\n22\n2018\n3.89\n\n\n23\n2019\n3.64\n\n\n24\n2020\n3.70\n\n\n25\n2021\n6.38\n\n\n26\n2022\n9.64\n\n\n\n\n\nDownload yearly imports price data",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "datasrc.html#co2-emissions-data",
    "href": "datasrc.html#co2-emissions-data",
    "title": "Data Sources",
    "section": "CO2 Emissions Data",
    "text": "CO2 Emissions Data\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset contains carbon dioxide emissions from energy consumption of natural gas in the USA.\nThe data spans from the year 2000 to 2023.\nThe unit of measurement is millions of metric tons of carbon dioxide.\n\n\n\n\n\n\nUS Natural Gas Monthly CO2 Emissions Dataset\n\n\n\nDate\nco2_value\n\n\n\n\n305\n6/1/23\n127.178\n\n\n306\n7/1/23\n143.918\n\n\n307\n8/1/23\n144.409\n\n\n308\n9/1/23\n127.841\n\n\n309\n10/1/23\n131.738\n\n\n310\n11/1/23\n152.661\n\n\n\n\n\nDownload Natural gas CO2 Emissions Data",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "datasrc.html#ghg-emissions-data",
    "href": "datasrc.html#ghg-emissions-data",
    "title": "Data Sources",
    "section": "GHG Emissions Data",
    "text": "GHG Emissions Data\n\n\n\n\n\nclick image for data source\n\n\n\n\nContains an extensive selection of Greenhouse gas emissions data from the energy sector for over 190 countries and regions.\nThe data included in this file with the following indicators:\n\nEnergy related greenhouse gas emissions\nFugitive greenhouse gas emissions\nExtended time series, starting in 1751 of CO2 emissions from fuel combustion.\n\n\n\n\n\n\n\nUS Natural Gas Yearly GHG Emissions Dataset\n\n\nYear\nTotal.GHG..emissions.from.fuel.combustion\nnatural.gas\nother\n\n\n\n\n1971\n4338.88\n1197.8\n3141.08\n\n\n1972\n4580.87\n1210.1\n3370.77\n\n\n1973\n4746.14\n1200.1\n3546.04\n\n\n1974\n4594.17\n1158.6\n3435.57\n\n\n1975\n4409.86\n1063.4\n3346.46\n\n\n1976\n4674.07\n1084.7\n3589.37\n\n\n\n\n\nDownload GHG emissions data",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "datasrc.html#prices-data",
    "href": "datasrc.html#prices-data",
    "title": "Data Sources",
    "section": "Prices Data",
    "text": "Prices Data\n1) Henry Hub Prices\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset contains the monthly average henry hub natural gas spot price in the USA.\nThe data spans from the year 1973 to 2023.\nPrices are measured in Dollars per Million Btu.\n\n\n\n\n\n\nUS Average Henry Hub Natural Gas Spot Price Dataset\n\n\nMonth\nHenry.Hub.Natural.Gas.Spot.Price.Dollars.per.Million.Btu\n\n\n\n\n1/1/05\n6.15\n\n\n2/1/05\n6.14\n\n\n3/1/05\n6.96\n\n\n4/1/05\n7.16\n\n\n5/1/05\n6.47\n\n\n6/1/05\n7.18\n\n\n\n\n\nDownload Henry Hub Natural Gas Spot Price Data\n\n\n\n\n\n\n2) Prices By Sector\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset contains the yearly average price of natural gas delivered to residential and commercial consumers in the USA.\nThe data spans from the year 1997 to 2023.\nPrices are measured in dollars per thousand cubic feet.\n\n\n\n\n\n\nUS Average Natural Gas Price By Sector Dataset\n\n\nYear\nprices\nsector\n\n\n\n\n1997\n6.94\nResidential\n\n\n1998\n6.82\nResidential\n\n\n1999\n6.69\nResidential\n\n\n2000\n7.76\nResidential\n\n\n2001\n9.63\nResidential\n\n\n2002\n7.89\nResidential\n\n\n\n\n\nDownload Natural Gas Prices By Sector Data",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "datasrc.html#industry-employment-data",
    "href": "datasrc.html#industry-employment-data",
    "title": "Data Sources",
    "section": "Industry Employment Data",
    "text": "Industry Employment Data\n1) By Gender\n\n\n\n\n\nclick image for data source\n\n\n\n\nContains information of the workforce in the U.S. natural gas distribution industry by gender and age from 2014 to 2021\n\n\n\n\n\n\nWorkforce in US Natural Gas Distribution Industry By Gender And Age Dataset\n\n\nYear\nGender\nTotal.Population\nAge.Group\n\n\n\n\n2021\nFemale\n81\n16 to 19\n\n\n2021\nFemale\n1278\n20 to 24\n\n\n2021\nFemale\n2619\n25 to 29\n\n\n2021\nFemale\n3399\n30 to 34\n\n\n2021\nFemale\n2729\n35 to 39\n\n\n2021\nFemale\n2214\n40 to 44\n\n\n\n\n\nDownload employment by gender and age data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2) By Location\n\n\n\n\n\nclick image for data source\n\n\n\n\nContains information of the workforce in the U.S. natural gas distribution industry by location from 2014 to 2021\n\n\n\n\n\n\nWorkforce in US Natural Gas Distribution Industry By Location Dataset\n\n\nState\nYear\nTotal.Population\nCode\n\n\n\n\nAlabama\n2021\n2371\nAL\n\n\nAlabama\n2020\n2685\nAL\n\n\nAlabama\n2019\n2111\nAL\n\n\nAlabama\n2018\n2210\nAL\n\n\nAlabama\n2017\n2532\nAL\n\n\nAlabama\n2016\n2157\nAL\n\n\n\n\n\nDownload employment by location data\n\n\n\n\n\n\n\n\n\n3) By Wage\n\n\n\n\n\nclick image for data source\n\n\n\n\nContains information of the the U.S. natural gas distribution industry yearly wage from 2014 to 2021\n\n\n\n\n\n\nUS Natural Gas Distribution Industry Yearly Wage Dataset\n\n\nID.Year\nAverage.Wage\n\n\n\n\n2021\n86281.42\n\n\n2020\n81508.72\n\n\n2019\n79726.42\n\n\n2018\n77958.03\n\n\n2017\n75065.32\n\n\n2016\n72986.09\n\n\n\n\n\nDownload employment by wage data",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "datasrc.html#gdp-data",
    "href": "datasrc.html#gdp-data",
    "title": "Data Sources",
    "section": "GDP Data",
    "text": "GDP Data\n1) US GDP\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset presents Quarterly Gross domestic product (GDP) figures for the United States.\nThe data spans from the year 2005 to 2022.\nThe unit of analysis is Billions of Dollars, Seasonally Adjusted Annual Rate.\n\n\n\n\n\n\nUS GDP Dataset\n\n\nQuarter\nGDP\n\n\n\n\n2005-01-01\n12767.29\n\n\n2005-04-01\n12922.66\n\n\n2005-07-01\n13142.64\n\n\n2005-10-01\n13324.20\n\n\n2006-01-01\n13599.16\n\n\n2006-04-01\n13753.42\n\n\n\n\n\nDownload US GDP data\n\n\n\n\n\n\n2) Netherlands GDP\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset presents Quarterly Gross domestic product (GDP) figures for Netherlands.\nThe data spans from the year 2005 to 2022.\nThe unit of analysis is Millions of Euros,Seasonally Adjusted.\n\n\n\n\n\n\nNetherlands GDP Dataset\n\n\nDate\ngdp_int\n\n\n\n\n1/1/05\n134841.1\n\n\n4/1/05\n136587.9\n\n\n7/1/05\n138671.4\n\n\n10/1/05\n140509.7\n\n\n1/1/06\n143090.4\n\n\n4/1/06\n145172.7\n\n\n\n\n\nDownload Netherlands GDP data",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "datasrc.html#unemployment-data",
    "href": "datasrc.html#unemployment-data",
    "title": "Data Sources",
    "section": "Unemployment Data",
    "text": "Unemployment Data\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset presents monthly unemployment rate figures for the USA.\nThe data spans from the year 2005 to 2022.\n\n\n\n\n\n\nUS Monthly Unemployment Rate Dataset\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\n\n\n2005\n5.3\n5.4\n5.2\n5.2\n5.1\n5.0\n5.0\n4.9\n5.0\n5.0\n5.0\n4.9\n\n\n2006\n4.7\n4.8\n4.7\n4.7\n4.6\n4.6\n4.7\n4.7\n4.5\n4.4\n4.5\n4.4\n\n\n2007\n4.6\n4.5\n4.4\n4.5\n4.4\n4.6\n4.7\n4.6\n4.7\n4.7\n4.7\n5.0\n\n\n2008\n5.0\n4.9\n5.1\n5.0\n5.4\n5.6\n5.8\n6.1\n6.1\n6.5\n6.8\n7.3\n\n\n2009\n7.8\n8.3\n8.7\n9.0\n9.4\n9.5\n9.5\n9.6\n9.8\n10.0\n9.9\n9.9\n\n\n2010\n9.8\n9.8\n9.9\n9.9\n9.6\n9.4\n9.4\n9.5\n9.5\n9.4\n9.8\n9.3\n\n\n\n\n\nDownload Unemployment data",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "datasrc.html#stocks-data",
    "href": "datasrc.html#stocks-data",
    "title": "Data Sources",
    "section": "Stocks Data",
    "text": "Stocks Data\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis data represents daily opening price, highest price, lowest price, closing price, adjusted closing price, and trading volume for Chevron Corp (NYSE:CVX) on different dates from 2000 to 2022.\nIt will be used in financial analysis to track the performance and volatility of the stock prices for top natural gas producing companies over time.\n\n\n\n\n\n\nCVX Stocks Dataset\n\n\n\n\n\n\n\n\n\n\n\nCVX.Adjusted\nCVX.Open\nCVX.Close\nCVX.High\nCVX.Low\nCVX.Volume\nDate\n\n\n\n\n17.13982\n42.93750\n41.81250\n42.93750\n41.28125\n4387600\n2000-01-03\n\n\n17.13982\n41.46875\n41.81250\n42.06250\n41.25000\n3702400\n2000-01-04\n\n\n17.44725\n41.53125\n42.56250\n43.28125\n41.53125\n5567600\n2000-01-05\n\n\n18.19024\n42.65625\n44.37500\n44.59375\n42.65625\n4353400\n2000-01-06\n\n\n18.51048\n45.00000\n45.15625\n45.43750\n44.50000\n4487400\n2000-01-07\n\n\n18.01089\n45.03125\n43.93750\n45.03125\n43.84375\n2104800\n2000-01-10\n\n\n\n\n\nDownload Chevron Corporation (CVX) stocks data",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "datasrc.html#exchange-rates-data",
    "href": "datasrc.html#exchange-rates-data",
    "title": "Data Sources",
    "section": "Exchange Rates Data",
    "text": "Exchange Rates Data\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset presents quaterly average exchange rate for converting US Dollars to Netherlands’ national currency (USD to EUR) figures.\nThe data spans from the year 2005 to 2022.\nThe unit of analysis is Euro, Not Seasonally Adjusted.\n\n\n\n\n\n\nExchange Rate for Netherlands Dataset\n\n\nDate\nexchange_rate\n\n\n\n\n2005-01-01\n105.2033\n\n\n2005-04-01\n103.9500\n\n\n2005-07-01\n102.4767\n\n\n2005-10-01\n101.5567\n\n\n2006-01-01\n100.7100\n\n\n2006-04-01\n102.1833\n\n\n\n\n\nDownload Exchange Rate data",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "datasrc.html#storage-data",
    "href": "datasrc.html#storage-data",
    "title": "Data Sources",
    "section": "Storage Data",
    "text": "Storage Data\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset contains the monthly underground storage volume of natural gas in the USA.\nThe data spans from the year 2005 to 2022\nThe unit of analysis is Million Cubic Feet.\n\n\n\n\n\n\nNatural Gas Storage in the US Dataset\n\n\n\n\n\n\nMonth\nU.S..Natural.Gas.Underground.Storage.Volume..Million.Cubic.Feet\n\n\n\n\n1/1/05\n6199291\n\n\n2/1/05\n5768939\n\n\n3/1/05\n5484332\n\n\n4/1/05\n5699060\n\n\n5/1/05\n6075521\n\n\n6/1/05\n6398738\n\n\n\n\n\nDownload Storage Data",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "datasrc.html#temperature-data",
    "href": "datasrc.html#temperature-data",
    "title": "Data Sources",
    "section": "Temperature Data",
    "text": "Temperature Data\n\n\n\n\n\nclick image for data source\n\n\n\n\nThis dataset contains the monthly average temperature figures across the USA.\nThe data spans from the year 2005 to 2022.\nThe temperature is measured in fahrenheit.\n\n\n\n\n\n\nAverage Temperature in the US Dataset\n\n\nmoonth\navg_temp\n\n\n\n\n1/1/05\n33.4\n\n\n2/1/05\n37.9\n\n\n3/1/05\n42.3\n\n\n4/1/05\n52.3\n\n\n5/1/05\n59.6\n\n\n6/1/05\n69.2\n\n\n\n\n\nDownload Temperature Data",
    "crumbs": [
      "Data Sources"
    ]
  },
  {
    "objectID": "conclusions.html#final-remarks",
    "href": "conclusions.html#final-remarks",
    "title": "Conclusions",
    "section": "Final Remarks",
    "text": "Final Remarks\nThis detailed study shows how changes in the economy, environmental issues, and market trends affect the U.S. natural gas industry and significantly impact the U.S. economy. By using data and forecasts effectively, industry leaders can make smart choices that meet both U.S. needs and global market demands. In simple terms, this strategy helps the economy grow sustainably while keeping the energy sector strong and ready to handle future challenges and opportunities."
  },
  {
    "objectID": "conclusions.html#final-remark",
    "href": "conclusions.html#final-remark",
    "title": "Conclusions",
    "section": "Final Remark",
    "text": "Final Remark\nThis detailed analysis showed how changes in the economy, environmental issues, and market trends affected the U.S. natural gas industry and significantly impacted the U.S. economy. By using data and forecasts effectively, industry leaders can make smart choices that meet both U.S. needs and global market demands. In simple terms, this strategy helps the economy grow sustainably while keeping the energy sector strong and ready to handle future challenges and opportunities."
  }
]